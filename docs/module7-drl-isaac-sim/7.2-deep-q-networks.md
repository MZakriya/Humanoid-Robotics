---
id: module7-drl-isaac-sim-7.2-deep-q-networks
title: "Deep Q-Networks"
slug: /module7-drl-isaac-sim-7.2-deep-q-networks
---

# Deep Q-Networks (DQN)

Deep Q-Networks (DQN) represent a breakthrough in reinforcement learning, combining Q-learning with deep neural networks to handle high-dimensional state spaces. Introduced by DeepMind in 2015, DQN demonstrated the ability to achieve human-level performance on Atari games and has since become foundational for many robotic applications.

## Background and Motivation

Traditional Q-learning struggles with high-dimensional state spaces because it requires a table of Q-values for each state-action pair. For problems with continuous or large discrete state spaces, this becomes intractable. DQN addresses this by using a deep neural network to approximate the Q-function $Q(s,a;θ)$, where $θ$ represents the network parameters.

## Core Components of DQN

### Experience Replay

Experience replay stores past experiences `(s sub t, a sub t, r sub t, s sub {t+1})` in a replay buffer and samples random batches for training. This breaks the correlation between consecutive experiences and allows for more efficient use of past experiences.

### Target Network

DQN uses a separate target network $Q(s,a;θ^-)$ to compute target values, with parameters $θ^-$ that are periodically updated from the main network parameters $θ$. This stabilizes training by providing consistent targets during learning.

### Algorithm Overview

The DQN algorithm can be summarized as:

1. Initialize main network $Q(s,a;θ)$ and target network $Q(s,a;θ^-)$
2. Initialize replay buffer D
3. For each episode:
   - Initialize sequence $s_1 = \{x_1\}$
   - For each time step:
     - With probability $ε$, select random action $a_t$
     - Otherwise, select $a_t = \arg\max_a Q(s_t, a; θ)$
     - Execute action `a sub t`, observe reward `r sub t` and next state `s sub {t+1}`
     - Store transition `(s sub t, a sub t, r sub t, s sub {t+1})` in D
     - Sample random mini-batch of transitions from D
     - For each transition, compute target:
       `y sub j = r sub j + gamma max_{a prime} Q(s sub {j+1}, a prime; theta^-)` (if not terminal)
       $y_j = r_j$ (if terminal)
     - Update network parameters `theta` using gradient descent on `(y sub j - Q(s sub j, a sub j; theta))^2`
     - Every C steps, update target network: `theta^-` to `theta`

## DQN Implementation for Robotics

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
from collections import deque
import gym
from typing import List, Tuple, Optional
import matplotlib.pyplot as plt

class DQN(nn.Module):
    """
    Deep Q-Network implementation
    """
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [64, 64]):
        super(DQN, self).__init__()

        layers = []
        input_dim = state_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim

        layers.append(nn.Linear(input_dim, action_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

class DQNAgent:
    """
    DQN Agent with experience replay and target network
    """
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 1e-3,
                 gamma: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 target_update_freq: int = 1000,
                 batch_size: int = 32,
                 buffer_size: int = 100000):

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.target_update_freq = target_update_freq
        self.batch_size = batch_size

        # Neural networks
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)

        # Initialize target network with same weights
        self.target_network.load_state_dict(self.q_network.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # Replay buffer
        self.memory = deque(maxlen=buffer_size)

        # Training statistics
        self.step_count = 0

    def remember(self, state: np.ndarray, action: int,
                 reward: float, next_state: np.ndarray, done: bool):
        """
        Store experience in replay buffer
        """
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state: np.ndarray, training: bool = True) -> int:
        """
        Choose action using epsilon-greedy policy
        """
        if training and np.random.random() <= self.epsilon:
            return random.randrange(self.action_dim)

        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.q_network(state_tensor)
        return np.argmax(q_values.cpu().data.numpy())

    def replay(self):
        """
        Train on batch of experiences from replay buffer
        """
        if len(self.memory) < self.batch_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.BoolTensor([e[4] for e in batch])

        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

        # Next Q values from target network
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        # Loss and optimization
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Update epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target_network(self):
        """
        Update target network with current network weights
        """
        self.target_network.load_state_dict(self.q_network.state_dict())

    def save_model(self, filepath: str):
        """
        Save model weights
        """
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, filepath)

    def load_model(self, filepath: str):
        """
        Load model weights
        """
        checkpoint = torch.load(filepath)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# Enhanced DQN with additional improvements
class DoubleDQN(DQNAgent):
    """
    Double DQN implementation to reduce overestimation bias
    """
    def replay(self):
        """
        Train using Double DQN update rule
        """
        if len(self.memory) < self.batch_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.BoolTensor([e[4] for e in batch])

        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

        # Double DQN: use main network to select actions, target network to evaluate
        next_actions = self.q_network(next_states).max(1)[1]
        next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        # Loss and optimization
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()

        # Update epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

class DuelingDQN(nn.Module):
    """
    Dueling DQN architecture: separate value and advantage streams
    """
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [64, 64]):
        super(DuelingDQN, self).__init__()

        # Shared layers
        shared_layers = []
        input_dim = state_dim
        for hidden_dim in hidden_dims[:-1]:  # All but last for shared layers
            shared_layers.append(nn.Linear(input_dim, hidden_dim))
            shared_layers.append(nn.ReLU())
            input_dim = hidden_dim
        self.shared_layers = nn.Sequential(*shared_layers)

        # Value stream
        self.value_stream = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[-1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[-1], 1)
        )

        # Advantage stream
        self.advantage_stream = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[-1]),
            nn.ReLU(),
            nn.Linear(hidden_dims[-1], action_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        shared_features = F.relu(self.shared_layers(x))

        value = self.value_stream(shared_features)
        advantage = self.advantage_stream(shared_features)

        # $Q(s,a) = V(s) + A(s,a) - \text{mean}(A(s,a'))$
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        return q_values

class RainbowDQN(DQNAgent):
    """
    Rainbow DQN: combination of multiple DQN improvements
    """
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 1e-3,
                 gamma: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 target_update_freq: int = 1000,
                 batch_size: int = 32,
                 buffer_size: int = 100000,
                 use_dueling: bool = True):

        self.use_dueling = use_dueling
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.target_update_freq = target_update_freq
        self.batch_size = batch_size

        # Neural networks - use dueling architecture if specified
        if use_dueling:
            self.q_network = DuelingDQN(state_dim, action_dim)
            self.target_network = DuelingDQN(state_dim, action_dim)
        else:
            self.q_network = DQN(state_dim, action_dim)
            self.target_network = DQN(state_dim, action_dim)

        # Initialize target network with same weights
        self.target_network.load_state_dict(self.q_network.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # Replay buffer
        self.memory = deque(maxlen=buffer_size)

        # Training statistics
        self.step_count = 0

    def replay(self):
        """
        Train using Rainbow improvements (Double DQN + Dueling DQN)
        """
        if len(self.memory) < self.batch_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.BoolTensor([e[4] for e in batch])

        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

        # Double DQN: use main network to select actions, target network to evaluate
        next_actions = self.q_network(next_states).max(1)[1]
        next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        # Loss and optimization
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()

        # Update epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Prioritized Experience Replay (PER) implementation
class PERMemory:
    """
    Prioritized Experience Replay with SumTree for efficient sampling
    """
    def __init__(self, capacity: int, alpha: float = 0.6, beta_start: float = 0.4):
        self.capacity = capacity
        self.alpha = alpha  # How much prioritization is used
        self.beta = beta_start  # Importance sampling weight
        self.beta_increment = 0.001
        self.absolute_error_upper = 1.0

        # Tree structure for efficient sum calculation and sampling
        self.tree = SumTree(capacity)

    def add(self, experience: Tuple, priority: float):
        """Add experience with priority to the memory"""
        self.tree.add(priority, experience)

    def sample(self, batch_size: int) -> Tuple[np.ndarray, list, np.ndarray]:
        """Sample batch with priorities and indices"""
        batch = []
        idxs = []
        priorities = []
        segment = self.tree.total() / batch_size

        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)

            s = random.uniform(a, b)
            idx, p, data = self.tree.get_leaf(s)
            priorities.append(p)
            batch.append(data)
            idxs.append(idx)

        sampling_probabilities = priorities / self.tree.total()
        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)
        self.beta = np.min([1., self.beta + self.beta_increment])  # Anneal beta
        is_weight /= is_weight.max()

        return batch, idxs, is_weight

    def update(self, idx: int, priority: float):
        """Update priority of a sample"""
        priority = min(priority, self.absolute_error_upper)
        self.tree.update(idx, priority)

class SumTree:
    """
    SumTree data structure for efficient priority-based sampling
    """
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)  # Internal nodes
        self.data = np.zeros(capacity, dtype=object)  # Leaf nodes (experience storage)
        self.write = 0
        self.n_entries = 0

    def _propagate(self, idx: int, change: float):
        """Propagate priority changes up the tree"""
        parent = (idx - 1) // 2
        self.tree[parent] += change

        if parent != 0:
            self._propagate(parent, change)

    def _retrieve(self, idx: int, s: float) -> int:
        """Retrieve sample based on priority"""
        left = 2 * idx + 1
        right = left + 1

        if left >= len(self.tree):
            return idx

        if s <= self.tree[left]:
            return self._retrieve(left, s)
        else:
            return self._retrieve(right, s - self.tree[left])

    def total(self) -> float:
        """Get total priority"""
        return self.tree[0]

    def add(self, p: float, data: object):
        """Add data with priority p"""
        idx = self.write + self.capacity - 1

        self.data[self.write] = data
        self.update(idx, p)

        self.write += 1
        if self.write >= self.capacity:
            self.write = 0

        if self.n_entries < self.capacity:
            self.n_entries += 1

    def update(self, idx: int, p: float):
        """Update priority at index idx"""
        change = p - self.tree[idx]
        self.tree[idx] = p
        self._propagate(idx, change)

    def get_leaf(self, s: float) -> Tuple[int, float, object]:
        """Get leaf node based on value s"""
        idx = self._retrieve(0, s)
        dataIdx = idx - self.capacity + 1
        return idx, self.tree[idx], self.data[dataIdx]

class PERDQNAgent:
    """
    DQN with Prioritized Experience Replay
    """
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 1e-3,
                 gamma: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 target_update_freq: int = 1000,
                 batch_size: int = 32,
                 per_capacity: int = 100000):

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.target_update_freq = target_update_freq
        self.batch_size = batch_size

        # Neural networks
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)

        # Initialize target network with same weights
        self.target_network.load_state_dict(self.q_network.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # Prioritized experience replay
        self.memory = PERMemory(per_capacity)

        # Training statistics
        self.step_count = 0
        self.priority_epsilon = 1e-6  # Small positive value to prevent priority of 0

    def remember(self, state: np.ndarray, action: int,
                 reward: float, next_state: np.ndarray, done: bool):
        """Store experience with initial priority"""
        # Initial priority is based on maximum possible TD error
        max_priority = 1.0
        self.memory.add((state, action, reward, next_state, done), max_priority)

    def act(self, state: np.ndarray, training: bool = True) -> int:
        """Choose action using epsilon-greedy policy"""
        if training and np.random.random() <= self.epsilon:
            return random.randrange(self.action_dim)

        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.q_network(state_tensor)
        return np.argmax(q_values.cpu().data.numpy())

    def replay(self):
        """Train on batch of experiences with prioritized sampling"""
        if self.memory.tree.n_entries < self.batch_size:
            return

        # Sample batch with priorities
        batch, idxs, is_weights = self.memory.sample(self.batch_size)
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.BoolTensor([e[4] for e in batch])

        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

        # Double DQN: use main network to select actions, target network to evaluate
        next_actions = self.q_network(next_states).max(1)[1]
        next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        # Calculate TD errors for priority updates
        td_errors = torch.abs(current_q_values.squeeze() - target_q_values).cpu().data.numpy()

        # Loss with importance sampling weights
        loss = torch.mean(torch.FloatTensor(is_weights).to(current_q_values.device) *
                         F.mse_loss(current_q_values.squeeze(), target_q_values, reduction='none'))

        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()

        # Update priorities in PER
        priorities = (td_errors + self.priority_epsilon) ** self.memory.alpha
        for idx, priority in zip(idxs, priorities):
            self.memory.update(idx, priority)

        # Update epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Example usage and training loop
def train_dqn_robot_example():
    """
    Example training loop for a robotic task using DQN
    """
    # For this example, we'll use CartPole which is a classic RL benchmark
    # In a real robotics application, this would be replaced with a robot simulator
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    # Create agent
    agent = RainbowDQN(state_dim, action_dim, use_dueling=True)

    # Training parameters
    episodes = 1000
    max_steps = 200
    scores = []
    solved_threshold = 195.0  # CartPole is considered solved if avg score > 195

    print("Starting DQN training...")

    for episode in range(episodes):
        state = env.reset()
        if isinstance(state, tuple):  # Handle new gym versions
            state = state[0]

        total_reward = 0

        for step in range(max_steps):
            # Select action
            action = agent.act(state, training=True)

            # Take action
            result = env.step(action)

            # Handle new gym versions
            if len(result) == 4:
                next_state, reward, done, info = result
            else:
                next_state, reward, terminated, truncated, info = result
                done = terminated or truncated

            # Store experience
            agent.remember(state, action, reward, next_state, done)

            # Train agent
            agent.replay()

            state = next_state
            total_reward += reward

            if done:
                break

        scores.append(total_reward)

        # Update target network periodically
        if episode % agent.target_update_freq == 0:
            agent.update_target_network()

        # Print progress
        if episode % 100 == 0:
            avg_score = np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores)
            print(f"Episode {episode}, Average Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.3f}")

            # Check if solved
            if avg_score >= solved_threshold:
                print(f"Solved in {episode} episodes! Average score: {avg_score:.2f}")
                break

    env.close()
    return agent, scores

def evaluate_agent(agent: DQNAgent, env_name: str, num_episodes: int = 10):
    """
    Evaluate trained agent
    """
    env = gym.make(env_name)
    scores = []

    for episode in range(num_episodes):
        state = env.reset()
        if isinstance(state, tuple):
            state = state[0]

        total_reward = 0
        done = False

        while not done:
            action = agent.act(state, training=False)  # No exploration during evaluation
            result = env.step(action)

            if len(result) == 4:
                state, reward, done, info = result
            else:
                state, reward, terminated, truncated, info = result
                done = terminated or truncated

            total_reward += reward

        scores.append(total_reward)

    env.close()
    return scores

# Visualization function
def plot_training_progress(scores: List[float], window: int = 100):
    """
    Plot training progress with moving average
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Plot raw scores
    ax1.plot(scores, alpha=0.3, label='Raw Scores', color='lightblue')

    # Calculate and plot moving average
    if len(scores) >= window:
        moving_avg = [np.mean(scores[i:i+window]) for i in range(len(scores)-window+1)]
        ax1.plot(range(window-1, len(scores)), moving_avg,
                label=f'Moving Average ({window} episodes)', color='blue')

    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Score')
    ax1.set_title('Training Progress')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Plot score histogram
    ax2.hist(scores[-100:], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    ax2.set_xlabel('Score')
    ax2.set_ylabel('Frequency')
    ax2.set_title('Score Distribution (Last 100 Episodes)')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    # Train the agent
    agent, scores = train_dqn_robot_example()

    # Plot training progress
    plot_training_progress(scores)

    # Evaluate the trained agent
    eval_env = 'CartPole-v1'
    eval_scores = evaluate_agent(agent, eval_env, num_episodes=10)
    print(f"\nEvaluation Results:")
    print(f"Average Score: {np.mean(eval_scores):.2f} ± {np.std(eval_scores):.2f}")
    print(f"Min Score: {np.min(eval_scores)}, Max Score: {np.max(eval_scores)}")
