---
id: module7-drl-isaac-sim-7.6-advanced-techniques
title: "Advanced Techniques"
slug: /module7-drl-isaac-sim-7.6-advanced-techniques
---

# Advanced Deep Reinforcement Learning Techniques for Robotics

This chapter explores cutting-edge techniques in deep reinforcement learning that push the boundaries of what's possible in robotic applications. These advanced methods address complex challenges in real-world deployments and enable more sophisticated robot behaviors.

## Hierarchical Reinforcement Learning (HRL)

Hierarchical Reinforcement Learning decomposes complex tasks into simpler subtasks organized in a hierarchy, enabling learning of abstract behaviors and long-horizon tasks.

### Option-Critic Architecture

The Option-Critic framework jointly learns options (temporally extended actions) and their policies:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Tuple, Optional
import random

class OptionCriticNetwork(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, num_options: int, hidden_dim: int = 256):
        super(OptionCriticNetwork, self).__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.num_options = num_options

        # Shared feature extractor
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Option-value function
        self.option_value = nn.Linear(hidden_dim, num_options)

        # Intra-option policies (one for each option)
        self.intra_option_policies = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            ) for _ in range(num_options)
        ])

        # Termination function
        self.termination = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_options),
            nn.Sigmoid()  # Probability of termination for each option
        )

        # Option-critic components
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, state: torch.Tensor, option: int = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through the network
        Returns: (action_probs, option_values, termination_probs)
        """
        features = self.feature_extractor(state)

        # Option values (for option selection)
        option_values = self.option_value(features)

        # Termination probabilities
        termination_probs = self.termination(features)

        # Action probabilities (for selected option)
        if option is not None:
            action_logits = self.intra_option_policies[option](features)
            action_probs = torch.softmax(action_logits, dim=-1)
        else:
            # Return average over all options
            action_logits = torch.stack([
                self.intra_option_policies[opt](features) for opt in range(self.num_options)
            ], dim=1)
            action_probs = torch.softmax(action_logits, dim=-2)

        return action_probs, option_values, termination_probs

    def get_option_value(self, state: torch.Tensor, option: int) -> torch.Tensor:
        """Get value of specific option"""
        features = self.feature_extractor(state)
        option_values = self.option_value(features)
        return option_values[:, option]

    def sample_option(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:
        """Sample an option based on option values"""
        features = self.feature_extractor(state)
        option_values = self.option_value(features)
        option_probs = torch.softmax(option_values, dim=-1)

        # Sample option
        dist = torch.distributions.Categorical(option_probs)
        option = dist.sample()

        return option.item(), option_probs

class OptionCriticAgent:
    def __init__(self, state_dim: int, action_dim: int, num_options: int,
                 lr: float = 3e-4, gamma: float = 0.99, epsilon: float = 0.1):
        self.gamma = gamma
        self.epsilon = epsilon

        # Networks
        self.network = OptionCriticNetwork(state_dim, action_dim, num_options)
        self.target_network = OptionCriticNetwork(state_dim, action_dim, num_options)
        self.target_network.load_state_dict(self.network.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

        # Current option and state
        self.current_option = 0
        self.option_termination = False

        # Training statistics
        self.update_count = 0

    def select_action(self, state: torch.Tensor) -> Tuple[int, int]:
        """
        Select action and determine if option should terminate
        Returns: (action, new_option) if termination occurs, else (action, current_option)
        """
        state_tensor = state.unsqueeze(0) if state.dim() == 1 else state

        # Get termination probability for current option
        _, _, termination_probs = self.network(state_tensor)
        termination_prob = termination_probs[0, self.current_option].item()

        # Decide whether to continue current option or switch
        if np.random.random() < termination_prob or self.option_termination:
            # Sample new option
            new_option, _ = self.network.sample_option(state_tensor)
            self.current_option = new_option
            self.option_termination = False

        # Select action according to current option's policy
        action_probs, _, _ = self.network(state_tensor, self.current_option)
        dist = torch.distributions.Categorical(action_probs[0])
        action = dist.sample()

        return action.item(), self.current_option

    def update(self, state: torch.Tensor, action: int, reward: float,
               next_state: torch.Tensor, done: bool, option: int):
        """
        Update using option-critic algorithm
        """
        state_tensor = state.unsqueeze(0) if state.dim() == 1 else state
        next_state_tensor = next_state.unsqueeze(0) if next_state.dim() == 1 else next_state

        # Get termination probability
        _, _, termination_probs = self.network(state_tensor)
        beta = termination_probs[0, option].item()

        # Compute target value
        with torch.no_grad():
            if done:
                target = reward
            else:
                next_option_values = self.target_network.option_value(
                    self.target_network.feature_extractor(next_state_tensor)
                )
                next_option_probs = torch.softmax(next_option_values, dim=-1)

                # Expected value under next state's option distribution
                target = reward + self.gamma * torch.sum(
                    next_option_probs * next_option_values, dim=-1
                )

        # Compute current values
        features = self.network.feature_extractor(state_tensor)
        option_values = self.network.option_value(features)
        current_option_value = option_values[0, option]

        # Compute advantage for the selected option
        advantage = target - current_option_value.item()

        # Policy loss (actor)
        action_logits = self.network.intra_option_policies[option](features)
        action_probs = torch.softmax(action_logits, dim=-1)
        action_log_prob = torch.log(action_probs[0, action] + 1e-8)
        policy_loss = -action_log_prob * advantage

        # Option-value loss (critic)
        option_value_loss = nn.MSELoss()(
            current_option_value.unsqueeze(0),
            torch.FloatTensor([target])
        )

        # Termination loss
        termination_loss = beta * advantage  # Encourage termination when advantage is negative

        # Total loss
        total_loss = policy_loss + 0.5 * option_value_loss + termination_loss

        # Update
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)
        self.optimizer.step()

        # Update target network periodically
        if self.update_count % 100 == 0:
            self.target_network.load_state_dict(self.network.state_dict())

        self.update_count += 1

# Example usage for robotic manipulation
def create_hierarchical_manipulation_agent():
    """
    Create a hierarchical agent for robotic manipulation tasks
    """
    # Define manipulation task with options
    manipulation_options = [
        'reach_to_object',
        'grasp_object',
        'lift_object',
        'transport_object',
        'place_object'
    ]

    # Initialize agent
    agent = OptionCriticAgent(
        state_dim=24,  # Example: joint positions, velocities, object positions
        action_dim=8,  # Example: joint torque commands
        num_options=len(manipulation_options),
        lr=1e-4,
        gamma=0.99
    )

    print("Hierarchical manipulation agent created with options:")
    for i, option in enumerate(manipulation_options):
        print(f"  {i}: {option}")

    return agent, manipulation_options
```

### Feudal Networks

Feudal networks implement a manager-worker hierarchy where high-level managers set goals and low-level workers achieve them:

```python
class FeudalNetwork(nn.Module):
    """
    Feudal Network implementation with manager-worker hierarchy
    """
    def __init__(self, state_dim: int, action_dim: int, goal_dim: int = 8,
                 hidden_dim: int = 256, manager_horizon: int = 10):
        super(FeudalNetwork, self).__init__()

        self.goal_dim = goal_dim
        self.manager_horizon = manager_horizon

        # Manager network - sets high-level goals
        self.manager_lstm = nn.LSTM(state_dim, hidden_dim, batch_first=True)
        self.manager_goal_generator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, goal_dim)
        )

        # Worker network - achieves goals set by manager
        self.worker_lstm = nn.LSTM(state_dim + goal_dim, hidden_dim, batch_first=True)
        self.worker_policy = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

        # Worker value function
        self.worker_value = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # Goal-value function
        self.goal_value = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, state_seq: torch.Tensor, manager_state: Optional[Tuple] = None,
                worker_state: Optional[Tuple] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through feudal network
        Args:
            state_seq: Sequence of states [batch_size, seq_len, state_dim]
            manager_state: LSTM state for manager
            worker_state: LSTM state for worker
        Returns:
            (actions, goals, values)
        """
        batch_size, seq_len, _ = state_seq.shape

        # Process through manager LSTM
        manager_out, manager_state_out = self.manager_lstm(state_seq, manager_state)

        # Generate goals
        goals = self.manager_goal_generator(manager_out)

        # Process through worker LSTM (concatenate state and goal)
        state_goal_seq = torch.cat([state_seq, goals], dim=-1)
        worker_out, worker_state_out = self.worker_lstm(state_goal_seq, worker_state)

        # Generate actions and values
        actions = torch.softmax(self.worker_policy(worker_out), dim=-1)
        worker_values = self.worker_value(worker_out)
        goal_values = self.goal_value(manager_out)

        return actions, goals, worker_values, goal_values

class FeudalAgent:
    def __init__(self, state_dim: int, action_dim: int, goal_dim: int = 8,
                 lr_manager: float = 1e-4, lr_worker: float = 3e-4,
                 gamma: float = 0.99):
        self.gamma = gamma

        # Networks
        self.feudal_net = FeudalNetwork(state_dim, action_dim, goal_dim)
        self.target_feudal_net = FeudalNetwork(state_dim, action_dim, goal_dim)
        self.target_feudal_net.load_state_dict(self.feudal_net.state_dict())

        # Separate optimizers for manager and worker
        manager_params = list(self.feudal_net.manager_lstm.parameters()) + \
                        list(self.feudal_net.manager_goal_generator.parameters()) + \
                        list(self.feudal_net.goal_value.parameters())
        self.manager_optimizer = optim.Adam(manager_params, lr=lr_manager)

        worker_params = list(self.feudal_net.worker_lstm.parameters()) + \
                       list(self.feudal_net.worker_policy.parameters()) + \
                       list(self.feudal_net.worker_value.parameters())
        self.worker_optimizer = optim.Adam(worker_params, lr=lr_worker)

        # LSTM states
        self.manager_hidden = None
        self.worker_hidden = None

        # Manager update interval
        self.manager_update_freq = 10

    def select_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:
        """
        Select action using feudal hierarchy
        """
        state_tensor = state.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dims

        with torch.no_grad():
            actions, goals, worker_values, goal_values = self.feudal_net(
                state_tensor, self.manager_hidden, self.worker_hidden
            )

            # Update hidden states
            # Note: In practice, this would require more sophisticated hidden state management
            self.manager_hidden = None  # Would be updated based on actual LSTM outputs
            self.worker_hidden = None

            # Sample action
            action_probs = actions[0, -1]  # Get last action in sequence
            dist = torch.distributions.Categorical(action_probs)
            action = dist.sample()

            return action.item(), goals[0, -1]  # Return action and goal

    def update(self, state_seq: torch.Tensor, action_seq: torch.Tensor,
               reward_seq: torch.Tensor, goal_seq: torch.Tensor, done_seq: torch.Tensor):
        """
        Update feudal network using manager and worker updates
        """
        batch_size, seq_len, _ = state_seq.shape

        # Forward pass
        actions, goals, worker_values, goal_values = self.feudal_net(
            state_seq, self.manager_hidden, self.worker_hidden
        )

        # Worker updates (frequent)
        for t in range(seq_len):
            # Worker advantage
            if t == seq_len - 1:
                next_worker_value = 0 if done_seq[t] else self.target_feudal_net.worker_value(
                    self.target_feudal_net.worker_lstm(state_seq[:, t:t+1])[0]
                ).squeeze()
            else:
                next_worker_value = worker_values[:, t+1].squeeze()

            worker_advantage = reward_seq[t] + self.gamma * next_worker_value - worker_values[:, t].squeeze()

            # Worker policy loss
            action_log_probs = torch.log(actions[:, t, action_seq[t]] + 1e-8)
            worker_policy_loss = -torch.mean(action_log_probs * worker_advantage.detach())

            # Worker value loss
            target_worker_value = reward_seq[t] + self.gamma * next_worker_value
            worker_value_loss = nn.MSELoss()(
                worker_values[:, t].squeeze(),
                target_worker_value
            )

            # Update worker
            self.worker_optimizer.zero_grad()
            (worker_policy_loss + 0.5 * worker_value_loss).backward()
            torch.nn.utils.clip_grad_norm_(self.feudal_net.worker_lstm.parameters(), 1.0)
            self.worker_optimizer.step()

        # Manager updates (less frequent)
        if self.update_count % self.manager_update_freq == 0:
            # Calculate intrinsic reward for manager (based on worker success)
            intrinsic_rewards = self._calculate_intrinsic_rewards(state_seq, goal_seq)

            for t in range(0, seq_len, self.manager_horizon):
                if t + self.manager_horizon <= seq_len:
                    # Manager advantage over goal horizon
                    manager_return = 0
                    gamma_power = 1.0
                    for k in range(self.manager_horizon):
                        idx = min(t + k, seq_len - 1)
                        manager_return += gamma_power * intrinsic_rewards[idx]
                        gamma_power *= self.gamma

                    next_goal_value = 0 if t + self.manager_horizon >= seq_len else \
                        goal_values[:, t + self.manager_horizon].squeeze()

                    manager_advantage = manager_return + \
                        (self.gamma ** self.manager_horizon) * next_goal_value - \
                        goal_values[:, t].squeeze()

                    # Manager policy loss
                    goal_error = torch.mean((goals[:, t] - goal_seq[:, t]) ** 2)
                    manager_policy_loss = torch.mean(goal_error * manager_advantage.detach())

                    # Manager value loss
                    target_goal_value = manager_return + \
                        (self.gamma ** self.manager_horizon) * next_goal_value
                    manager_value_loss = nn.MSELoss()(
                        goal_values[:, t].squeeze(),
                        target_goal_value
                    )

                    # Update manager
                    self.manager_optimizer.zero_grad()
                    (manager_policy_loss + 0.5 * manager_value_loss).backward()
                    torch.nn.utils.clip_grad_norm_(self.feudal_net.manager_lstm.parameters(), 1.0)
                    self.manager_optimizer.step()

    def _calculate_intrinsic_rewards(self, state_seq: torch.Tensor,
                                   goal_seq: torch.Tensor) -> torch.Tensor:
        """
        Calculate intrinsic rewards for manager based on goal achievement
        """
        # Simple example: reward based on distance to goal
        intrinsic_rewards = torch.zeros(state_seq.shape[0])

        for i in range(state_seq.shape[0]):
            # This would depend on specific task and how to measure goal achievement
            # For example, if goal is to reach a position, measure distance to goal
            state_pos = state_seq[i, :2]  # First 2 dimensions as position
            goal_pos = goal_seq[i, :2]    # First 2 dimensions as goal position
            dist_to_goal = torch.norm(state_pos - goal_pos)

            # Inverse relationship: closer to goal = higher reward
            intrinsic_rewards[i] = 1.0 / (1.0 + dist_to_goal)

        return intrinsic_rewards

# Example usage
def example_feudal_network():
    """
    Example of feudal network for robotic task
    """
    # Create feudal agent for a manipulation task
    agent = FeudalAgent(
        state_dim=24,   # Example state dimension
        action_dim=8,   # Example action dimension
        goal_dim=4,     # Goal dimension (e.g., position goals)
        lr_manager=1e-4,
        lr_worker=3e-4,
        gamma=0.99
    )

    print("Feudal network agent created with manager-worker hierarchy")
    print("- Manager sets high-level goals every 10 steps")
    print("- Worker achieves goals with primitive actions")
    print("- Intrinsic rewards drive manager learning")

    return agent
```

## Multi-Task Learning

Multi-task learning enables robots to learn multiple related tasks simultaneously, sharing representations and knowledge:

```python
class MultiTaskNetwork(nn.Module):
    """
    Network for multi-task learning with shared and task-specific components
    """
    def __init__(self, state_dim: int, action_dims: List[int], num_tasks: int,
                 shared_hidden_dims: List[int] = [256, 256],
                 task_specific_dims: List[int] = [128, 128]):
        super(MultiTaskNetwork, self).__init__()

        self.num_tasks = num_tasks
        self.action_dims = action_dims

        # Shared feature extractor
        shared_layers = []
        prev_dim = state_dim
        for hidden_dim in shared_hidden_dims:
            shared_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU()
            ])
            prev_dim = hidden_dim
        self.shared_features = nn.Sequential(*shared_layers)

        # Task-specific heads
        self.task_heads = nn.ModuleList()
        for i in range(num_tasks):
            task_head = []
            task_prev_dim = prev_dim
            for task_hidden_dim in task_specific_dims:
                task_head.extend([
                    nn.Linear(task_prev_dim, task_hidden_dim),
                    nn.ReLU()
                ])
                task_prev_dim = task_hidden_dim

            # Final output layer for this task's actions
            task_head.append(nn.Linear(task_prev_dim, action_dims[i]))
            self.task_heads.append(nn.Sequential(*task_head))

        # Task embedding (optional - to provide task context)
        self.task_embedding = nn.Embedding(num_tasks, 16)

        # Task discriminator (for multi-task training stability)
        self.task_discriminator = nn.Sequential(
            nn.Linear(prev_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_tasks)
        )

    def forward(self, state: torch.Tensor, task_id: int) -> torch.Tensor:
        """
        Forward pass for specific task
        """
        # Extract shared features
        shared_features = self.shared_features(state)

        # Get task-specific output
        task_output = self.task_heads[task_id](shared_features)

        return task_output

    def forward_all_tasks(self, state: torch.Tensor) -> List[torch.Tensor]:
        """
        Forward pass for all tasks
        """
        shared_features = self.shared_features(state)
        outputs = []

        for task_head in self.task_heads:
            output = task_head(shared_features)
            outputs.append(output)

        return outputs

    def get_shared_features(self, state: torch.Tensor) -> torch.Tensor:
        """
        Extract shared features only
        """
        return self.shared_features(state)

class MultiTaskAgent:
    def __init__(self, state_dim: int, action_dims: List[int], num_tasks: int,
                 lr: float = 3e-4, gamma: float = 0.99):
        self.gamma = gamma
        self.num_tasks = num_tasks

        # Network
        self.network = MultiTaskNetwork(state_dim, action_dims, num_tasks)
        self.target_network = MultiTaskNetwork(state_dim, action_dims, num_tasks)
        self.target_network.load_state_dict(self.network.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)

        # Task weights for loss balancing
        self.task_weights = torch.ones(num_tasks)

        # Current task
        self.current_task = 0

    def select_action(self, state: torch.Tensor, task_id: int = None) -> int:
        """
        Select action for specific task
        """
        if task_id is None:
            task_id = self.current_task

        state_tensor = state.unsqueeze(0) if state.dim() == 1 else state
        action_logits = self.network(state_tensor, task_id)
        action_probs = torch.softmax(action_logits, dim=-1)

        # Sample action
        dist = torch.distributions.Categorical(action_probs[0])
        action = dist.sample()

        return action.item()

    def update(self, state: torch.Tensor, action: int, reward: float,
               next_state: torch.Tensor, done: bool, task_id: int):
        """
        Update for specific task
        """
        state_tensor = state.unsqueeze(0) if state.dim() == 1 else state
        next_state_tensor = next_state.unsqueeze(0) if next_state.dim() == 1 else next_state

        # Compute target value
        with torch.no_grad():
            if done:
                target = reward
            else:
                next_action_logits = self.target_network(next_state_tensor, task_id)
                next_action_probs = torch.softmax(next_action_logits, dim=-1)
                next_action_dist = torch.distributions.Categorical(next_action_probs[0])

                # Compute expected value
                next_q_values = self.target_network(next_state_tensor, task_id)
                target = reward + self.gamma * torch.sum(
                    next_action_probs * next_q_values, dim=-1
                )

        # Current Q value
        current_q_values = self.network(state_tensor, task_id)
        current_q_value = current_q_values[0, action]

        # Task-specific loss
        task_loss = nn.MSELoss()(
            current_q_value.unsqueeze(0),
            torch.FloatTensor([target])
        )

        # Apply task weighting
        weighted_loss = self.task_weights[task_id] * task_loss

        # Update network
        self.optimizer.zero_grad()
        weighted_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)
        self.optimizer.step()

        # Update target network periodically
        if self.update_count % 1000 == 0:
            self.target_network.load_state_dict(self.network.state_dict())

    def balance_task_learning(self, task_losses: List[float]):
        """
        Dynamically balance task weights based on learning progress
        """
        # Normalize losses to compute inverse weights
        losses_tensor = torch.FloatTensor(task_losses)
        normalized_losses = losses_tensor / (losses_tensor.sum() + 1e-8)

        # Update task weights inversely proportional to performance
        # Tasks that are learning poorly get higher weights
        self.task_weights = 1.0 / (normalized_losses + 1e-8)
        self.task_weights = self.task_weights / self.task_weights.sum()  # Normalize

    def train_multitask(self, envs: List, total_timesteps: int = 1000000):
        """
        Train on multiple tasks simultaneously
        """
        episode_rewards = {i: [] for i in range(self.num_tasks)}
        task_selector = 0

        state = envs[task_selector].reset()
        total_reward = 0

        for t in range(total_timesteps):
            # Select action for current task
            action = self.select_action(state, task_selector)

            # Take action in current task environment
            next_state, reward, done, info = envs[task_selector].step(action)

            # Store and update
            self.update(state, action, reward, next_state, done, task_selector)

            # Update counters
            total_reward += reward
            state = next_state

            if done:
                episode_rewards[task_selector].append(total_reward)
                total_reward = 0

                # Switch to next task periodically
                if t % 1000 == 0:  # Change task every 1000 timesteps
                    task_selector = (task_selector + 1) % self.num_tasks
                    state = envs[task_selector].reset()

        return episode_rewards

# Example multi-task scenario for humanoid robot
def example_multitask_humanoid():
    """
    Example multi-task learning for humanoid robot
    """
    # Define tasks and their action dimensions
    tasks = [
        'walking_forward',    # Action dim: 12 (leg joints)
        'walking_backward',   # Action dim: 12 (leg joints)
        'turning_left',       # Action dim: 12 (leg joints)
        'balancing',          # Action dim: 12 (all joints for balance)
        'stepping_over_obstacle'  # Action dim: 12 (leg joints)
    ]

    action_dimensions = [12, 12, 12, 12, 12]  # All tasks use same action dimension

    # Create multi-task agent
    agent = MultiTaskAgent(
        state_dim=60,  # Example: 30 joint positions + 30 joint velocities
        action_dims=action_dimensions,
        num_tasks=len(tasks),
        lr=1e-4,
        gamma=0.99
    )

    print("Multi-task humanoid agent created with tasks:")
    for i, task in enumerate(tasks):
        print(f"  {i}: {task} (action_dim: {action_dimensions[i]})")

    # In practice, you would create multiple environments for each task
    # envs = [create_task_environment(task) for task in tasks]

    return agent, tasks
```

## Meta-Learning for Rapid Adaptation

Meta-learning enables robots to rapidly adapt to new tasks or environments by learning a good initialization or learning algorithm:

```python
class MAML(nn.Module):
    """
    Model-Agnostic Meta-Learning implementation for rapid adaptation
    """
    def __init__(self, policy_network: nn.Module, meta_lr: float = 1e-4,
                 adaptation_lr: float = 1e-3, num_adaptation_steps: int = 5):
        super(MAML, self).__init__()

        self.policy_network = policy_network
        self.meta_lr = meta_lr
        self.adaptation_lr = adaptation_lr
        self.num_adaptation_steps = num_adaptation_steps

        # Meta-optimizer
        self.meta_optimizer = optim.Adam(policy_network.parameters(), lr=meta_lr)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.policy_network(state)

    def adapt(self, support_data: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
              create_copy: bool = True) -> nn.Module:
        """
        Adapt the policy to a new task using support data
        Args:
            support_data: List of (state, action, reward) tuples
            create_copy: Whether to create a copy of the network or adapt in-place
        Returns:
            Adapted network
        """
        if create_copy:
            adapted_network = copy.deepcopy(self.policy_network)
        else:
            adapted_network = self.policy_network

        # Perform adaptation steps
        for _ in range(self.num_adaptation_steps):
            total_loss = 0

            for state, action, reward in support_data:
                # Forward pass
                pred_action = adapted_network(state)
                loss = nn.MSELoss()(pred_action, action.unsqueeze(0))
                total_loss += loss

            total_loss /= len(support_data)

            # Compute gradients and update
            gradients = torch.autograd.grad(total_loss, adapted_network.parameters(),
                                          create_graph=True, retain_graph=True)

            # Update parameters using gradients
            for param, grad in zip(adapted_network.parameters(), gradients):
                param.data = param.data - self.adaptation_lr * grad

        return adapted_network

    def meta_update(self, query_data: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
                   support_data: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]):
        """
        Update meta-learner using query data after adaptation
        """
        # Adapt to the task
        adapted_network = self.adapt(support_data, create_copy=True)

        # Evaluate on query data
        query_loss = 0
        for state, action, reward in query_data:
            pred_action = adapted_network(state)
            loss = nn.MSELoss()(pred_action, action.unsqueeze(0))
            query_loss += loss

        query_loss /= len(query_data)

        # Update meta-parameters
        self.meta_optimizer.zero_grad()
        query_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), 1.0)
        self.meta_optimizer.step()

        return query_loss.item()

class MetaLearningTrainer:
    def __init__(self, maml_agent: MAML, task_sampler, meta_batch_size: int = 10):
        self.maml_agent = maml_agent
        self.task_sampler = task_sampler
        self.meta_batch_size = meta_batch_size

    def train_meta_learning(self, num_iterations: int = 1000) -> List[float]:
        """
        Train using meta-learning approach
        """
        meta_losses = []

        for iteration in range(num_iterations):
            iteration_loss = 0

            # Sample multiple tasks for this iteration
            for _ in range(self.meta_batch_size):
                # Sample a task
                task = self.task_sampler.sample_task()

                # Generate support and query data for this task
                support_data = self._generate_task_data(task, num_samples=10)
                query_data = self._generate_task_data(task, num_samples=5)

                # Perform meta-update
                loss = self.maml_agent.meta_update(query_data, support_data)
                iteration_loss += loss

            iteration_loss /= self.meta_batch_size
            meta_losses.append(iteration_loss)

            if iteration % 100 == 0:
                print(f"Meta Iteration {iteration}: Average Loss = {iteration_loss:.4f}")

        return meta_losses

    def _generate_task_data(self, task, num_samples: int):
        """
        Generate data for a specific task (placeholder implementation)
        """
        # This would involve running the policy in the task environment
        # to collect state-action-reward tuples
        states = torch.randn(num_samples, 24)  # Example state dimension
        actions = torch.randn(num_samples, 8)  # Example action dimension
        rewards = torch.randn(num_samples)    # Example rewards

        return [(states[i], actions[i], rewards[i]) for i in range(num_samples)]

    def adapt_to_new_task(self, new_task, adaptation_steps: int = 5):
        """
        Adapt to a new task using meta-learned initialization
        """
        # Generate support data for the new task
        support_data = self._generate_task_data(new_task, num_samples=20)

        # Adapt the policy
        adapted_policy = self.maml_agent.adapt(support_data, create_copy=False)

        return adapted_policy

# Example meta-learning usage
def example_meta_learning():
    """
    Example of meta-learning for robotic tasks
    """
    # Create a simple policy network
    policy_net = nn.Sequential(
        nn.Linear(24, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 8)
    )

    # Initialize MAML
    maml = MAML(
        policy_network=policy_net,
        meta_lr=1e-4,
        adaptation_lr=1e-3,
        num_adaptation_steps=5
    )

    # Create meta-learning trainer
    # task_sampler = RoboticTaskSampler()  # Actual task sampler would be implemented separately
    meta_trainer = MetaLearningTrainer(maml, None)  # Placeholder for task sampler

    # Train meta-learner
    # meta_losses = meta_trainer.train_meta_learning(num_iterations=2000)

    print("Meta-learning setup completed!")
    print("- Ready to adapt to new tasks with minimal data")
    print("- Uses gradient-based meta-learning for rapid adaptation")

    return meta_trainer
```

## Safe Reinforcement Learning

Safe RL ensures that agents satisfy safety constraints during learning and deployment:

```python
class ConstrainedPPOAgent:
    """
    PPO with safety constraints using Lagrangian relaxation
    """
    def __init__(self, state_dim: int, action_dim: int, constraint_dim: int,
                 lr_actor: float = 3e-4, lr_critic: float = 3e-4,
                 lr_cost: float = 3e-4, gamma: float = 0.99, cost_gamma: float = 0.99,
                 clip_epsilon: float = 0.2, safety_budget: float = 0.1):
        self.gamma = gamma
        self.cost_gamma = cost_gamma
        self.clip_epsilon = clip_epsilon
        self.safety_budget = safety_budget  # Maximum allowed constraint violation

        # Networks
        self.actor_critic = ActorCritic(state_dim, action_dim)

        # Cost critic (for constraint violations)
        self.cost_critic = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

        # Initialize Lagrange multipliers for constraints
        self.lagrange_multipliers = torch.zeros(constraint_dim, requires_grad=True)
        self.lagrange_optimizer = optim.Adam([self.lagrange_multipliers], lr=lr_cost)

        # Regular optimizers
        self.actor_optimizer = optim.Adam(self.actor_critic.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.actor_critic.critic.parameters(), lr=lr_critic)
        self.cost_optimizer = optim.Adam(self.cost_critic.parameters(), lr=lr_cost)

    def update(self, states: torch.Tensor, actions: torch.Tensor,
               rewards: torch.Tensor, costs: torch.Tensor,  # Constraint costs
               returns: torch.Tensor, advantages: torch.Tensor,
               cost_advantages: torch.Tensor, old_log_probs: torch.Tensor):
        """
        Update using constrained PPO
        """
        # Actor update with safety constraints
        new_actions, new_log_probs, new_values = self.actor_critic(states)
        new_costs = self.cost_critic(states)

        # Policy ratio
        ratio = torch.exp(new_log_probs - old_log_probs)

        # Standard PPO surrogate loss
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()

        # Cost-constrained loss: penalize high constraint violations
        cost_loss = (self.lagrange_multipliers * (costs - self.safety_budget)).mean()

        # Combined actor loss
        actor_loss = policy_loss + cost_loss

        # Critic update
        critic_loss = nn.MSELoss()(new_values.squeeze(), returns)

        # Cost critic update
        cost_target = self._compute_cost_targets(costs, states)
        cost_critic_loss = nn.MSELoss()(new_costs.squeeze(), cost_target)

        # Update networks
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor_critic.actor.parameters(), 1.0)
        self.actor_optimizer.step()

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor_critic.critic.parameters(), 1.0)
        self.critic_optimizer.step()

        self.cost_optimizer.zero_grad()
        cost_critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.cost_critic.parameters(), 1.0)
        self.cost_optimizer.step()

        # Update Lagrange multipliers
        constraint_violation = (costs - self.safety_budget).mean()
        lagrange_loss = -self.lagrange_multipliers * constraint_violation.detach()

        self.lagrange_optimizer.zero_grad()
        lagrange_loss.backward()
        self.lagrange_optimizer.step()

        # Ensure Lagrange multipliers remain positive
        self.lagrange_multipliers.data = torch.clamp(self.lagrange_multipliers.data, min=0.0)

    def _compute_cost_targets(self, costs: torch.Tensor, states: torch.Tensor) -> torch.Tensor:
        """
        Compute cost targets for cost critic training
        """
        with torch.no_grad():
            next_costs = self.cost_critic(states[1:]).squeeze()
            next_costs = torch.cat([next_costs, torch.zeros(1)])  # Last state has 0 cost
            cost_targets = costs + self.cost_gamma * next_costs
        return cost_targets

class LyapunovSafeRL:
    """
    Lyapunov-based safe RL for stability guarantees
    """
    def __init__(self, state_dim: int, action_dim: int,
                 lyapunov_network: nn.Module = None,
                 safety_threshold: float = 0.1):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.safety_threshold = safety_threshold

        # Lyapunov function approximator
        if lyapunov_network is None:
            self.lyapunov_net = nn.Sequential(
                nn.Linear(state_dim, 256),
                nn.ReLU(),
                nn.Linear(256, 256),
                nn.ReLU(),
                nn.Linear(256, 1),
                nn.Softplus()  # Ensures positive output
            )
        else:
            self.lyapunov_net = lyapunov_network

        # Safety critic
        self.safety_critic = nn.Sequential(
            nn.Linear(state_dim + action_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

        # Optimizers
        self.lyapunov_optimizer = optim.Adam(self.lyapunov_net.parameters(), lr=1e-4)
        self.safety_optimizer = optim.Adam(self.safety_critic.parameters(), lr=1e-4)

    def is_safe_action(self, state: torch.Tensor, action: torch.Tensor,
                      alpha: float = 0.1) -> bool:
        """
        Check if action satisfies Lyapunov safety condition
        V(x_{t+1}) - V(x_t) <= -alpha * ||x_t||^2
        """
        state.requires_grad_(True)

        # Current Lyapunov value
        V_current = self.lyapunov_net(state).squeeze()

        # Compute next state (this would involve environment dynamics)
        # For now, we'll use a simple approximation
        # In practice, this would be computed using the actual system dynamics
        next_state = state + 0.1 * action  # Simple Euler integration

        # Next Lyapunov value
        V_next = self.lyapunov_net(next_state).squeeze()

        # Lyapunov condition: V_next - V_current <= -alpha * ||state||^2
        lyapunov_condition = V_next - V_current + alpha * torch.norm(state)**2

        return lyapunov_condition <= self.safety_threshold

    def safe_action_selection(self, state: torch.Tensor, unsafe_action: torch.Tensor,
                            num_candidates: int = 10) -> torch.Tensor:
        """
        Select safe action by projecting unsafe action or sampling alternatives
        """
        state_tensor = state.unsqueeze(0) if state.dim() == 1 else state

        # Check if original action is safe
        if self.is_safe_action(state_tensor, unsafe_action):
            return unsafe_action

        # If not safe, try to find a safe action
        with torch.no_grad():
            # Sample candidate actions around the unsafe action
            for _ in range(num_candidates):
                # Add random perturbation to unsafe action
                perturbation = torch.randn_like(unsafe_action) * 0.1
                candidate_action = torch.clamp(unsafe_action + perturbation, -1.0, 1.0)

                if self.is_safe_action(state_tensor, candidate_action):
                    return candidate_action

            # If no safe action found by perturbation, project to safe set
            # This is a simplified projection - in practice would be more complex
            projected_action = self._project_to_safe_set(state_tensor, unsafe_action)

        return projected_action

    def _project_to_safe_set(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        Project action to safe set (simplified implementation)
        """
        # This would involve solving an optimization problem to find
        # the closest safe action to the unsafe one
        # For now, we'll return a conservative action
        safe_action = torch.sign(action) * 0.5 * torch.ones_like(action)  # Conservative action
        return safe_action

    def update_lyapunov(self, state_sequences: List[torch.Tensor]):
        """
        Update Lyapunov function to satisfy stability conditions
        """
        for state_seq in state_sequences:
            loss = 0
            for t in range(len(state_seq) - 1):
                V_curr = self.lyapunov_net(state_seq[t]).squeeze()
                V_next = self.lyapunov_net(state_seq[t+1]).squeeze()

                # Lyapunov decrease condition
                # We want V_next - V_curr <= -alpha * ||x_t||^2
                alpha = 0.1
                lyapunov_decrease = torch.relu(V_next - V_curr + alpha * torch.norm(state_seq[t])**2)
                loss += lyapunov_decrease**2

                # Positive definiteness (V(0) = 0)
                origin_state = torch.zeros_like(state_seq[0])
                V_origin = self.lyapunov_net(origin_state).squeeze()
                loss += V_origin**2

            # Gradient step on Lyapunov parameters
            self.lyapunov_optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.lyapunov_net.parameters(), 1.0)
            self.lyapunov_optimizer.step()

    def update_safety_critic(self, states: torch.Tensor, actions: torch.Tensor,
                           safety_labels: torch.Tensor):
        """
        Update safety critic to predict safety of state-action pairs
        """
        predicted_safety = self.safety_critic(torch.cat([states, actions], dim=-1))
        safety_loss = nn.BCEWithLogitsLoss()(predicted_safety.squeeze(), safety_labels)

        self.safety_optimizer.zero_grad()
        safety_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.safety_critic.parameters(), 1.0)
        self.safety_optimizer.step()

# Example safe RL usage
def example_safe_rl():
    """
    Example of safe reinforcement learning
    """
    # Initialize safe RL agent
    safe_agent = ConstrainedPPOAgent(
        state_dim=24,
        action_dim=8,
        constraint_dim=3,  # 3 safety constraints
        lr_actor=3e-4,
        lr_critic=3e-4,
        lr_cost=1e-4,
        gamma=0.99,
        cost_gamma=0.98,
        safety_budget=0.05  # Low safety budget for strict safety
    )

    print("Safe RL agent initialized with constraint handling")
    print("- Uses Lagrangian relaxation for constraint satisfaction")
    print("- Maintains safety budget for acceptable constraint violations")

    # Lyapunov-based safety checker
    lyapunov_safety = LyapunovSafeRL(
        state_dim=24,
        action_dim=8,
        safety_threshold=0.1
    )

    print("Lyapunov safety checker initialized")
    print("- Provides stability guarantees")
    print("- Projects actions to safe set when needed")

    return safe_agent, lyapunov_safety
```

## Imitation Learning and Learning from Demonstrations

Imitation learning leverages expert demonstrations to accelerate learning:

```python
class BehavioralCloning(nn.Module):
    """
    Behavioral Cloning - Direct imitation of expert actions
    """
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 256]):
        super(BehavioralCloning, self).__init__()

        layers = []
        prev_dim = state_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU()
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, action_dim))

        self.network = nn.Sequential(*layers)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.network(state)

class GAILDiscriminator(nn.Module):
    """
    Discriminator for Generative Adversarial Imitation Learning (GAIL)
    """
    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 256]):
        super(GAILDiscriminator, self).__init__()

        layers = []
        prev_dim = state_dim + action_dim  # Concatenate state and action

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)  # Add dropout for regularization
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))
        layers.append(nn.Sigmoid())  # Output probability of being expert

        self.network = nn.Sequential(*layers)

    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        sa = torch.cat([state, action], dim=-1)
        return self.network(sa)

class GAILAgent:
    """
    Generative Adversarial Imitation Learning agent
    """
    def __init__(self, state_dim: int, action_dim: int,
                 lr_actor: float = 3e-4, lr_disc: float = 3e-4,
                 gamma: float = 0.99, expert_demo_buffer = None):
        self.gamma = gamma

        # Actor network (policy)
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )

        # Discriminator
        self.discriminator = GAILDiscriminator(state_dim, action_dim)

        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.disc_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr_disc)

        # Expert demonstration buffer
        self.expert_buffer = expert_demo_buffer or []

        # Reward shaping parameters
        self.gail_reward_scale = 1.0

    def compute_gail_reward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        Compute GAIL reward based on discriminator output
        """
        with torch.no_grad():
            disc_output = self.discriminator(state, action)
            # GAIL reward is log(D) - log(1-D)
            # But we use the simpler form: log(D / (1-D)) = logit(D)
            gail_reward = torch.log(disc_output + 1e-8) - torch.log(1 - disc_output + 1e-8)
        return self.gail_reward_scale * gail_reward.squeeze()

    def update_discriminator(self, states_expert: torch.Tensor,
                           actions_expert: torch.Tensor,
                           states_agent: torch.Tensor,
                           actions_agent: torch.Tensor):
        """
        Update discriminator to distinguish expert vs agent trajectories
        """
        # Label expert demonstrations as 1, agent as 0
        expert_labels = torch.ones(states_expert.size(0))
        agent_labels = torch.zeros(states_agent.size(0))

        # Discriminator loss
        expert_outputs = self.discriminator(states_expert, actions_expert)
        agent_outputs = self.discriminator(states_agent, actions_agent)

        expert_loss = nn.BCELoss()(expert_outputs.squeeze(), expert_labels)
        agent_loss = nn.BCELoss()(agent_outputs.squeeze(), agent_labels)

        disc_loss = expert_loss + agent_loss

        # Update discriminator
        self.disc_optimizer.zero_grad()
        disc_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)
        self.disc_optimizer.step()

        return disc_loss.item()

    def update_actor(self, states: torch.Tensor, actions: torch.Tensor,
                     rewards: torch.Tensor):
        """
        Update actor using policy gradient with GAIL reward
        """
        # Compute action probabilities
        action_logits = self.actor(states)
        action_probs = torch.softmax(action_logits, dim=-1)
        dist = torch.distributions.Categorical(action_probs)

        # Policy gradient loss
        log_probs = dist.log_prob(actions)
        actor_loss = -(log_probs * rewards).mean()

        # Update actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_optimizer.step()

        return actor_loss.item()

class ImitationLearningTrainer:
    def __init__(self, agent, expert_demos, rl_agent=None):
        self.agent = agent
        self.expert_demos = expert_demos  # List of (state, action) tuples
        self.rl_agent = rl_agent  # Optional RL agent to combine with IL

    def behavioral_cloning_pretraining(self, epochs: int = 1000):
        """
        Pre-train using behavioral cloning before RL fine-tuning
        """
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.agent.parameters(), lr=1e-3)

        states = torch.FloatTensor([demo[0] for demo in self.expert_demos])
        actions = torch.FloatTensor([demo[1] for demo in self.expert_demos])

        for epoch in range(epochs):
            optimizer.zero_grad()

            predicted_actions = self.agent(states)
            loss = criterion(predicted_actions, actions)

            loss.backward()
            optimizer.step()

            if epoch % 100 == 0:
                print(f"BC Epoch {epoch}: Loss = {loss.item():.4f}")

    def gail_training(self, env, total_timesteps: int = 1000000):
        """
        Train using GAIL approach
        """
        # First, pre-train with behavioral cloning
        print("Pre-training with Behavioral Cloning...")
        self.behavioral_cloning_pretraining(epochs=500)

        # Then, train with GAIL
        print("Training with GAIL...")

        agent_buffer = []
        episode_count = 0
        state = env.reset()

        for t in range(total_timesteps):
            # Select action using current policy
            with torch.no_grad():
                action_probs = self.agent(torch.FloatTensor(state).unsqueeze(0))
                action_dist = torch.distributions.Categorical(torch.softmax(action_probs, dim=-1))
                action = action_dist.sample().item()

            # Take action in environment
            next_state, reward, done, info = env.step(action)

            # Store agent trajectory
            agent_buffer.append((state, action))

            # Compute GAIL reward
            gail_reward = self.agent.compute_gail_reward(
                torch.FloatTensor(state).unsqueeze(0),
                torch.FloatTensor([action])
            )

            # Update discriminator
            if len(self.expert_demos) > 0 and len(agent_buffer) > 100:
                # Sample from expert and agent buffers
                expert_batch = random.sample(self.expert_demos, min(64, len(self.expert_demos)))
                agent_batch = random.sample(agent_buffer, min(64, len(agent_buffer)))

                expert_states = torch.FloatTensor([demo[0] for demo in expert_batch])
                expert_actions = torch.FloatTensor([demo[1] for demo in expert_batch])
                agent_states = torch.FloatTensor([demo[0] for demo in agent_batch])
                agent_actions = torch.FloatTensor([demo[1] for demo in agent_batch])

                disc_loss = self.agent.update_discriminator(
                    expert_states, expert_actions,
                    agent_states, agent_actions
                )

            # Update actor with GAIL reward
            if len(agent_buffer) >= 32:
                batch = random.sample(agent_buffer, 32)
                batch_states = torch.FloatTensor([transition[0] for transition in batch])
                batch_actions = torch.LongTensor([transition[1] for transition in batch])

                # Compute GAIL rewards for batch
                gail_rewards = []
                for s, a in batch:
                    r = self.agent.compute_gail_reward(
                        torch.FloatTensor(s).unsqueeze(0),
                        torch.FloatTensor([a])
                    )
                    gail_rewards.append(r.item())

                gail_rewards = torch.FloatTensor(gail_rewards)
                actor_loss = self.agent.update_actor(batch_states, batch_actions, gail_rewards)

            # Update state
            state = next_state

            if done:
                state = env.reset()
                episode_count += 1

                if episode_count % 100 == 0:
                    print(f"GAIL Episode {episode_count}: Average reward optimization ongoing")

        return self.agent

# Example usage
def example_imitation_learning():
    """
    Example of imitation learning with expert demonstrations
    """
    # Generate synthetic expert demonstrations (in practice, these would come from human experts or pre-trained policies)
    expert_demos = []
    for _ in range(1000):
        # Synthetic expert behavior: move toward goal
        state = np.random.randn(24)  # Example state
        action = np.random.randn(8)  # Example expert action
        expert_demos.append((state, action))

    # Initialize GAIL agent
    gail_agent = GAILAgent(
        state_dim=24,
        action_dim=8,
        lr_actor=3e-4,
        lr_disc=3e-4,
        gamma=0.99
    )

    # Create trainer
    il_trainer = ImitationLearningTrainer(gail_agent, expert_demos)

    print("Imitation learning setup completed!")
    print("- Behavioral cloning for pre-training")
    print("- GAIL for adversarial learning from demonstrations")

    return il_trainer
```

## Advanced Exploration Strategies

### Curiosity-Driven Exploration

Curiosity-driven exploration uses prediction errors to encourage exploration of novel states:

```python
class ForwardDynamicsModel(nn.Module):
    """
    Forward dynamics model for curiosity computation
    """
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(ForwardDynamicsModel, self).__init__()

        # State and action encoders
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )

        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 2),
            nn.ReLU()
        )

        # Combined predictor
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, state_dim)
        )

    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        state_feat = self.state_encoder(state)
        action_feat = self.action_encoder(action)

        combined = torch.cat([state_feat, action_feat], dim=-1)
        next_state_pred = self.predictor(combined)

        return next_state_pred

class InverseDynamicsModel(nn.Module):
    """
    Inverse dynamics model for feature representation learning
    """
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(InverseDynamicsModel, self).__init__()

        # State encoder
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )

        # Combined predictor for action
        self.action_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )

    def forward(self, state: torch.Tensor, next_state: torch.Tensor) -> torch.Tensor:
        state_feat = self.state_encoder(state)
        next_state_feat = self.state_encoder(next_state)

        combined = torch.cat([state_feat, next_state_feat], dim=-1)
        action_pred = self.action_predictor(combined)

        return action_pred

class CuriosityDrivenAgent:
    """
    Agent with curiosity-driven exploration
    """
    def __init__(self, policy_network: nn.Module, state_dim: int, action_dim: int,
                 lr_policy: float = 3e-4, lr_forward: float = 1e-4,
                 lr_inverse: float = 1e-4, curiosity_scale: float = 0.1):
        self.curiosity_scale = curiosity_scale

        # Policy network
        self.policy = policy_network
        self.policy_optimizer = optim.Adam(policy_network.parameters(), lr=lr_policy)

        # Forward and inverse models for curiosity
        self.forward_model = ForwardDynamicsModel(state_dim, action_dim)
        self.inverse_model = InverseDynamicsModel(state_dim, action_dim)

        self.forward_optimizer = optim.Adam(self.forward_model.parameters(), lr=lr_forward)
        self.inverse_optimizer = optim.Adam(self.inverse_model.parameters(), lr=lr_inverse)

        # Feature encoder for state representation
        self.feature_encoder = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )

    def compute_intrinsic_reward(self, state: torch.Tensor, action: torch.Tensor,
                               next_state: torch.Tensor) -> torch.Tensor:
        """
        Compute intrinsic reward based on prediction error (curiosity)
        """
        with torch.no_grad():
            # Predict next state using forward model
            next_state_pred = self.forward_model(state, action)

            # Compute prediction error (intrinsic reward)
            prediction_error = torch.norm(next_state - next_state_pred, dim=-1)

            # Scale the intrinsic reward
            intrinsic_reward = self.curiosity_scale * prediction_error

        return intrinsic_reward

    def update_models(self, states: torch.Tensor, actions: torch.Tensor,
                     next_states: torch.Tensor):
        """
        Update forward and inverse models for curiosity computation
        """
        # Forward model loss (predict next state from state and action)
        next_state_pred = self.forward_model(states, actions)
        forward_loss = nn.MSELoss()(next_state_pred, next_states)

        # Inverse model loss (predict action from state and next state)
        action_pred = self.inverse_model(states, next_states)
        inverse_loss = nn.MSELoss()(action_pred, actions)

        # Update forward model
        self.forward_optimizer.zero_grad()
        forward_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.forward_model.parameters(), 1.0)
        self.forward_optimizer.step()

        # Update inverse model
        self.inverse_optimizer.zero_grad()
        inverse_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.inverse_model.parameters(), 1.0)
        self.inverse_optimizer.step()

        return forward_loss.item(), inverse_loss.item()

    def select_action_with_curiosity(self, state: torch.Tensor,
                                   exploration_coeff: float = 0.1) -> Tuple[int, float]:
        """
        Select action with curiosity-driven exploration bonus
        """
        state_tensor = state.unsqueeze(0) if state.dim() == 1 else state

        with torch.no_grad():
            # Get policy action
            action_probs = self.policy(state_tensor)
            action_dist = torch.distributions.Categorical(torch.softmax(action_probs, dim=-1))
            action = action_dist.sample()

            # Compute log probability for policy gradient methods
            log_prob = action_dist.log_prob(action)

        # Add exploration bonus based on action probability (less probable actions get higher bonus)
        exploration_bonus = exploration_coeff / (action_dist.probs[0, action] + 1e-8)

        return action.item(), exploration_bonus.item()

# Count-based Exploration
class CountBasedExplorer:
    """
    Count-based exploration using state visitation counts
    """
    def __init__(self, state_dim: int, count_scale: float = 1.0,
                 hash_resolution: int = 1000000):
        self.state_dim = state_dim
        self.count_scale = count_scale
        self.hash_resolution = hash_resolution

        # State visitation counts
        self.visit_counts = {}
        self.state_visit_threshold = 1  # States visited less than this get exploration bonus

    def get_state_hash(self, state: np.ndarray) -> int:
        """
        Convert continuous state to discrete hash for counting
        """
        # Quantize state to discrete bins
        quantized_state = np.round(state * self.hash_resolution).astype(int)
        state_hash = hash(tuple(quantized_state)) % self.hash_resolution
        return state_hash

    def update_counts(self, state: np.ndarray):
        """
        Update visitation count for state
        """
        state_hash = self.get_state_hash(state)
        self.visit_counts[state_hash] = self.visit_counts.get(state_hash, 0) + 1

    def compute_exploration_bonus(self, state: np.ndarray) -> float:
        """
        Compute exploration bonus based on visitation count (novelty)
        """
        state_hash = self.get_state_hash(state)
        visit_count = self.visit_counts.get(state_hash, 0)

        # Use inverse count for exploration bonus (less visited = higher bonus)
        # Add small epsilon to avoid division by zero
        exploration_bonus = self.count_scale / (visit_count + 1)

        return exploration_bonus

    def get_exploration_probability(self, state: np.ndarray) -> float:
        """
        Get exploration probability based on novelty
        """
        bonus = self.compute_exploration_bonus(state)
        # Convert bonus to probability (sigmoid transformation)
        prob = 1.0 / (1.0 + np.exp(-bonus))
        return min(prob, 0.9)  # Cap exploration probability

class AdvancedExplorationAgent:
    """
    Agent combining multiple exploration strategies
    """
    def __init__(self, base_agent, state_dim: int, action_dim: int):
        self.base_agent = base_agent
        self.state_dim = state_dim
        self.action_dim = action_dim

        # Curiosity-driven exploration
        self.curiosity_agent = CuriosityDrivenAgent(
            policy_network=base_agent.network if hasattr(base_agent, 'network') else None,
            state_dim=state_dim,
            action_dim=action_dim
        )

        # Count-based exploration
        self.count_explorer = CountBasedExplorer(state_dim)

        # Exploration parameters
        self.exploration_tradeoff = 0.5  # Balance between curiosity and count-based exploration

    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        Select action using combined exploration strategies
        """
        if not training:
            # For evaluation, use base policy without exploration
            return self.base_agent.select_action(state, evaluate=True)

        state_tensor = torch.FloatTensor(state)

        # Get base action from policy
        base_action, base_exploration_bonus = self.curiosity_agent.select_action_with_curiosity(
            state_tensor
        )

        # Compute exploration bonuses from different strategies
        count_bonus = self.count_explorer.compute_exploration_bonus(state)
        curiosity_bonus = base_exploration_bonus  # From curiosity agent

        # Combine exploration bonuses
        combined_bonus = (self.exploration_tradeoff * curiosity_bonus +
                         (1 - self.exploration_tradeoff) * count_bonus)

        # Update exploration strategies
        self.count_explorer.update_counts(state)

        # With probability based on combined exploration bonus, select random action
        exploration_prob = min(combined_bonus, 0.3)  # Cap exploration probability
        if np.random.random() < exploration_prob:
            # Select random action for exploration
            random_action = np.random.randint(0, self.action_dim)
            return random_action

        return base_action

    def update_exploration_models(self, states: torch.Tensor, actions: torch.Tensor,
                                next_states: torch.Tensor):
        """
        Update curiosity models
        """
        curiosity_forward_loss, curiosity_inverse_loss = self.curiosity_agent.update_models(
            states, actions, next_states
        )
        return curiosity_forward_loss, curiosity_inverse_loss

# Example usage
def example_advanced_exploration():
    """
    Example of advanced exploration strategies
    """
    # Create a base policy network
    base_policy = nn.Sequential(
        nn.Linear(24, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 8)
    )

    # Initialize advanced exploration agent
    explorer_agent = AdvancedExplorationAgent(
        base_agent=None,  # Placeholder - would be actual RL agent
        state_dim=24,
        action_dim=8
    )

    print("Advanced exploration setup completed!")
    print("- Curiosity-driven exploration using forward/inverse models")
    print("- Count-based exploration for novelty detection")
    print("- Combined exploration strategy")

    return explorer_agent
```

## Distributed Training

For large-scale robotic training, distributed approaches can significantly accelerate learning:

```python
import multiprocessing as mp
import threading
import queue
import time

class DistributedWorker:
    """
    Worker process for distributed training
    """
    def __init__(self, worker_id: int, env_creator, agent_class,
                 shared_network, update_queue, result_queue):
        self.worker_id = worker_id
        self.env = env_creator()
        self.agent = agent_class()
        self.shared_network = shared_network
        self.update_queue = update_queue
        self.result_queue = result_queue

    def run_episode(self) -> Tuple[List, List, float]:
        """
        Run a single episode and return experience
        """
        state = self.env.reset()
        states, actions, rewards = [], [], []
        total_reward = 0

        done = False
        while not done:
            action = self.agent.select_action(state)
            next_state, reward, done, info = self.env.step(action)

            states.append(state)
            actions.append(action)
            rewards.append(reward)
            total_reward += reward

            state = next_state

        return states, actions, total_reward

    def train_loop(self):
        """
        Main training loop for worker
        """
        while True:
            # Get latest network parameters from shared model
            try:
                # Sync with shared network
                self.agent.network.load_state_dict(self.shared_network.state_dict())
            except:
                pass  # Ignore sync errors

            # Collect experience
            states, actions, episode_reward = self.run_episode()

            # Compute gradients locally
            gradients = self.agent.compute_gradients(states, actions, rewards)

            # Send gradients and results to main process
            self.result_queue.put({
                'worker_id': self.worker_id,
                'gradients': gradients,
                'episode_reward': episode_reward,
                'episode_length': len(states)
            })

            # Wait for parameter update from main process
            try:
                updated_params = self.update_queue.get(timeout=10.0)
                if updated_params is not None:
                    self.agent.network.load_state_dict(updated_params)
            except queue.Empty:
                # Timeout - continue with current parameters
                continue

class DistributedTrainer:
    """
    Master process for distributed training
    """
    def __init__(self, num_workers: int, agent_class, env_creator,
                 learning_rate: float = 3e-4):
        self.num_workers = num_workers
        self.agent_class = agent_class
        self.env_creator = env_creator
        self.learning_rate = learning_rate

        # Shared network
        self.shared_network = None  # Will be initialized later
        self.global_optimizer = None

        # Queues for communication
        self.update_queues = [queue.Queue() for _ in range(num_workers)]
        self.result_queues = [queue.Queue() for _ in range(num_workers)]

        # Worker processes
        self.workers = []
        self.worker_processes = []

    def initialize_network(self, state_dim: int, action_dim: int):
        """
        Initialize shared network
        """
        self.shared_network = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
        self.global_optimizer = optim.Adam(self.shared_network.parameters(), lr=self.learning_rate)

    def start_workers(self):
        """
        Start worker processes
        """
        for i in range(self.num_workers):
            worker = DistributedWorker(
                worker_id=i,
                env_creator=self.env_creator,
                agent_class=self.agent_class,
                shared_network=self.shared_network,
                update_queue=self.update_queues[i],
                result_queue=self.result_queues[i]
            )

            process = mp.Process(target=worker.train_loop)
            process.start()

            self.workers.append(worker)
            self.worker_processes.append(process)

    def train_distributed(self, total_timesteps: int = 1000000):
        """
        Main distributed training loop
        """
        episode_count = 0
        start_time = time.time()

        for t in range(total_timesteps):
            # Collect results from all workers
            worker_results = []
            for i in range(self.num_workers):
                try:
                    result = self.result_queues[i].get_nowait()
                    worker_results.append(result)

                    # Send updated parameters back to worker
                    self.update_queues[i].put(self.shared_network.state_dict())
                except queue.Empty:
                    continue  # Worker hasn't finished yet

            # Aggregate gradients if we have results
            if worker_results:
                total_reward = 0
                aggregated_gradients = {}

                for result in worker_results:
                    total_reward += result['episode_reward']

                    # Accumulate gradients
                    for name, grad in result['gradients'].items():
                        if name not in aggregated_gradients:
                            aggregated_gradients[name] = grad
                        else:
                            aggregated_gradients[name] += grad

                # Average gradients
                for name in aggregated_gradients:
                    aggregated_gradients[name] /= len(worker_results)

                # Apply gradients to shared network
                for name, param in self.shared_network.named_parameters():
                    if name in aggregated_gradients:
                        param.grad = aggregated_gradients[name]

                self.global_optimizer.step()
                self.global_optimizer.zero_grad()

                # Log progress
                if episode_count % 100 == 0:
                    avg_reward = total_reward / len(worker_results) if worker_results else 0
                    elapsed_time = time.time() - start_time
                    print(f"Distributed Episode {episode_count}: Avg Reward = {avg_reward:.2f}, "
                          f"Time = {elapsed_time:.2f}s")

                episode_count += len(worker_results)

        # Stop all workers
        for process in self.worker_processes:
            process.terminate()

        return self.shared_network

# Example distributed training setup
def example_distributed_training():
    """
    Example of distributed training setup
    """
    # Define environment creator function
    def create_env():
        # This would be your actual Isaac Sim environment
        # For example: return IsaacSimHumanoidEnv()
        pass

    # Initialize distributed trainer
    dist_trainer = DistributedTrainer(
        num_workers=4,  # Use 4 worker processes
        agent_class=PPOAgent,  # Example agent class
        env_creator=create_env,
        learning_rate=3e-4
    )

    # Initialize network
    dist_trainer.initialize_network(state_dim=24, action_dim=8)

    # Start workers
    dist_trainer.start_workers()

    print("Distributed training setup completed!")
    print(f"- {dist_trainer.num_workers} worker processes")
    print("- Shared network with parameter synchronization")
    print("- Asynchronous gradient aggregation")

    return dist_trainer
```

## Practical Implementation Tips

### Performance Optimization

For efficient training in Isaac Sim:

1. **Vectorized Environments**: Use multiple parallel environments
2. **Mixed Precision Training**: Use FP16 to reduce memory and increase speed
3. **Gradient Compression**: Compress gradients for distributed training
4. **Experience Replay Optimization**: Efficient sampling and storage

### Debugging and Troubleshooting

Common issues and solutions:

1. **Exploding Gradients**: Use gradient clipping and proper weight initialization
2. **Poor Exploration**: Implement advanced exploration strategies
3. **Training Instability**: Reduce learning rate and use target networks
4. **Memory Issues**: Optimize batch sizes and use experience replay efficiently

## Conclusion

Advanced deep reinforcement learning techniques for robotics encompass a wide range of methodologies designed to address the challenges of real-world deployment. From hierarchical learning that enables complex behaviors to safe RL that ensures constraint satisfaction, these techniques push the boundaries of what's possible with autonomous robotic systems. The integration of imitation learning accelerates training by leveraging expert demonstrations, while distributed approaches make large-scale training feasible. Success in applying these advanced techniques requires careful consideration of the specific requirements of the robotic task, proper hyperparameter tuning, and effective monitoring of the training process.
