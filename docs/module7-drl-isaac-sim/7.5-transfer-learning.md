---
id: module7-drl-isaac-sim-7.5-transfer-learning
title: "Transfer Learning"
slug: /module7-drl-isaac-sim-7.5-transfer-learning
---

# Transfer Learning in Deep Reinforcement Learning

Transfer learning in deep reinforcement learning involves leveraging knowledge acquired from one task or environment to improve learning efficiency in a related task or environment. This is particularly important in robotics, where training from scratch in every new environment would be prohibitively expensive. Transfer learning can significantly reduce sample complexity and enable faster adaptation to new scenarios.

## Overview and Motivation

Traditional deep RL algorithms require extensive training in each specific environment, which is often infeasible for real-world robotic applications. Transfer learning addresses this by:

1. **Reducing sample complexity**: Leveraging pre-trained knowledge to learn faster
2. **Improving sim-to-real transfer**: Transferring policies from simulation to reality
3. **Cross-task generalization**: Applying learned behaviors to related tasks
4. **Domain adaptation**: Adapting to new environments with minimal retraining

The fundamental assumption in transfer learning is that there exists shared structure or knowledge between the source and target domains that can be exploited.

## Types of Transfer Learning

### 1. Policy Transfer

Policy transfer involves taking a pre-trained policy from a source task and adapting it to a target task:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import copy

class PolicyTransferNetwork(nn.Module):
    """
    Neural network designed for policy transfer between tasks
    """
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 hidden_dims: List[int] = [256, 256, 256],
                 transfer_layers: List[int] = [0, 1]):  # Layers to transfer
        super(PolicyTransferNetwork, self).__init__()

        self.transfer_layers = transfer_layers
        self.feature_extractor = nn.ModuleDict()
        self.task_specific_head = nn.ModuleDict()

        # Build shared feature extraction layers
        layers = []
        prev_dim = input_dim
        for i, hidden_dim in enumerate(hidden_dims):
            layer = nn.Linear(prev_dim, hidden_dim)
            layers.append(layer)
            layers.append(nn.ReLU())
            prev_dim = hidden_dim

            # Store transferable layers
            if i in transfer_layers:
                self.feature_extractor[f'layer_{i}'] = layer

        # Final shared layer
        self.shared_layers = nn.Sequential(*layers)

        # Task-specific output head
        self.task_specific_head = nn.Linear(prev_dim, output_dim)

        # Store layer dimensions for transfer operations
        self.layer_dims = [input_dim] + hidden_dims + [output_dim]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        features = self.shared_layers(x)
        output = self.task_specific_head(features)
        return output

    def freeze_transfer_layers(self):
        """Freeze the layers designated for transfer"""
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def unfreeze_transfer_layers(self):
        """Unfreeze the layers designated for transfer"""
        for param in self.feature_extractor.parameters():
            param.requires_grad = True

    def get_feature_extractor_state(self) -> Dict[str, torch.Tensor]:
        """Get state dict of transferable feature extractor layers"""
        return {k: v.state_dict() for k, v in self.feature_extractor.items()}

    def load_feature_extractor_state(self, state_dict: Dict[str, torch.Tensor]):
        """Load state dict into feature extractor layers"""
        for k, v in state_dict.items():
            self.feature_extractor[k].load_state_dict(v)

class PolicyTransferAgent:
    def __init__(self,
                 source_network: PolicyTransferNetwork,
                 target_network: PolicyTransferNetwork,
                 learning_rate: float = 1e-4,
                 fine_tune_lr: float = 1e-5):
        """
        Policy transfer agent that adapts source policy to target domain
        """
        self.source_network = source_network
        self.target_network = target_network
        self.fine_tune_lr = fine_tune_lr

        # Optimizers
        self.target_optimizer = optim.Adam(self.target_network.parameters(), lr=learning_rate)
        self.fine_tune_optimizer = optim.Adam(
            [p for p in self.target_network.parameters() if p.requires_grad],
            lr=fine_tune_lr
        )

        # Replay buffer for target domain
        self.replay_buffer = []
        self.buffer_size = 100000

    def transfer_policy(self,
                       source_task_name: str,
                       target_task_name: str,
                       transfer_strategy: str = 'fine_tune') -> None:
        """
        Transfer policy from source to target using specified strategy
        """
        if transfer_strategy == 'fine_tune':
            self._fine_tune_transfer(source_task_name, target_task_name)
        elif transfer_strategy == 'feature_extraction':
            self._feature_extraction_transfer(source_task_name, target_task_name)
        elif transfer_strategy == 'progressive_unfreezing':
            self._progressive_unfreezing_transfer(source_task_name, target_task_name)
        else:
            raise ValueError(f"Unknown transfer strategy: {transfer_strategy}")

    def _fine_tune_transfer(self, source_task: str, target_task: str):
        """
        Fine-tune target network using source network weights as initialization
        """
        print(f"Transferring policy from {source_task} to {target_task} using fine-tuning")

        # Copy feature extractor weights from source to target
        self.target_network.load_feature_extractor_state(
            self.source_network.get_feature_extractor_state()
        )

        # Freeze feature extractor initially, train only task-specific head
        self.target_network.freeze_transfer_layers()
        print("Feature extractor frozen, training task-specific head...")

    def _feature_extraction_transfer(self, source_task: str, target_task: str):
        """
        Use source network as feature extractor, train only new head
        """
        print(f"Transferring policy using feature extraction approach")

        # Load source feature extractor
        self.target_network.load_feature_extractor_state(
            self.source_network.get_feature_extractor_state()
        )

        # Freeze all transferred layers
        self.target_network.freeze_transfer_layers()

        # Only train the task-specific head
        for param in self.target_network.task_specific_head.parameters():
            param.requires_grad = True

    def _progressive_unfreezing_transfer(self, source_task: str, target_task: str):
        """
        Progressively unfreeze layers during fine-tuning
        """
        print(f"Transferring policy using progressive unfreezing")

        # Start with fully frozen feature extractor
        self.target_network.load_feature_extractor_state(
            self.source_network.get_feature_extractor_state()
        )
        self.target_network.freeze_transfer_layers()

        # Store which layers are frozen initially
        self.frozen_layers = set(self.target_network.transfer_layers)

    def fine_tune_on_target(self,
                           env,
                           num_episodes: int = 1000,
                           batch_size: int = 64,
                           update_frequency: int = 10) -> List[float]:
        """
        Fine-tune transferred policy on target environment
        """
        episode_rewards = []

        for episode in range(num_episodes):
            state = env.reset()
            total_reward = 0
            done = False

            while not done:
                # Select action using target network
                with torch.no_grad():
                    action = self.target_network(torch.FloatTensor(state)).argmax().item()

                # Take action in environment
                next_state, reward, done, info = env.step(action)

                # Store transition
                self.store_transition(state, action, reward, next_state, done)

                # Update network periodically
                if len(self.replay_buffer) >= batch_size and episode % update_frequency == 0:
                    self.train_on_batch(batch_size)

                state = next_state
                total_reward += reward

            episode_rewards.append(total_reward)

            # Print progress
            if episode % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode}: Average Reward = {avg_reward:.2f}")

        return episode_rewards

    def store_transition(self, state: np.ndarray, action: int,
                        reward: float, next_state: np.ndarray, done: bool):
        """Store transition in replay buffer"""
        self.replay_buffer.append((state, action, reward, next_state, done))

        if len(self.replay_buffer) > self.buffer_size:
            self.replay_buffer.pop(0)

    def train_on_batch(self, batch_size: int):
        """Train on a batch of transitions"""
        if len(self.replay_buffer) < batch_size:
            return

        # Sample batch
        batch_indices = np.random.choice(len(self.replay_buffer), batch_size, replace=False)
        batch = [self.replay_buffer[i] for i in batch_indices]

        states = torch.FloatTensor([transition[0] for transition in batch])
        actions = torch.LongTensor([transition[1] for transition in batch])
        rewards = torch.FloatTensor([transition[2] for transition in batch])
        next_states = torch.FloatTensor([transition[3] for transition in batch])
        dones = torch.BoolTensor([transition[4] for transition in batch])

        # Compute target values
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (0.99 * next_q_values * ~dones)

        # Current Q values
        current_q_values = self.target_network(states).gather(1, actions.unsqueeze(1))

        # Loss
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)

        # Update
        self.target_optimizer.zero_grad()
        loss.backward()
        self.target_optimizer.step()

# Example usage
def example_policy_transfer():
    """
    Example of policy transfer between different robotic tasks
    """
    # Define source and target network architectures
    source_net = PolicyTransferNetwork(
        input_dim=24,  # Example: 12 joint positions + 12 joint velocities
        output_dim=8,  # Example: 8 actuated joints
        hidden_dims=[256, 256, 128],
        transfer_layers=[0, 1]  # Transfer first two layers
    )

    target_net = PolicyTransferNetwork(
        input_dim=24,  # Same input dimension
        output_dim=8,  # Same output dimension (or different if needed)
        hidden_dims=[256, 256, 128],
        transfer_layers=[0, 1]
    )

    # Create transfer agent
    transfer_agent = PolicyTransferAgent(source_net, target_net)

    # Transfer policy using fine-tuning approach
    transfer_agent.transfer_policy(
        source_task_name="walker_forward",
        target_task_name="walker_backward",
        transfer_strategy="fine_tune"
    )

    # Fine-tune on target environment (mock environment for example)
    # target_env = WalkerBackwardEnvironment()  # Would be actual environment
    # rewards = transfer_agent.fine_tune_on_target(target_env, num_episodes=2000)

    print("Policy transfer completed!")
    return transfer_agent
```

### 2. Domain Adaptation

Domain adaptation focuses on transferring policies between simulation and reality or between different simulation environments:

```python
class DomainAdaptationNetwork(nn.Module):
    """
    Network with domain adaptation capabilities
    """
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 hidden_dims: List[int] = [256, 256],
                 num_domains: int = 2):  # Source and target domains
        super(DomainAdaptationNetwork, self).__init__()

        self.num_domains = num_domains

        # Shared feature extractor
        shared_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            shared_layers.append(nn.Linear(prev_dim, hidden_dim))
            shared_layers.append(nn.ReLU())
            prev_dim = hidden_dim
        self.shared_features = nn.Sequential(*shared_layers)

        # Domain classifier
        self.domain_classifier = nn.Sequential(
            nn.Linear(prev_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_domains)
        )

        # Task-specific heads for each domain
        self.task_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(prev_dim, 128),
                nn.ReLU(),
                nn.Linear(128, output_dim)
            ) for _ in range(num_domains)
        ])

        # Domain discriminator loss weight
        self.domain_loss_weight = 0.1

    def forward(self, x: torch.Tensor, domain_id: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass with domain-specific output
        Returns: (task_output, domain_logits)
        """
        features = self.shared_features(x)

        # Task output for specific domain
        task_output = self.task_heads[domain_id](features)

        # Domain classification (for adversarial training)
        domain_logits = self.domain_classifier(features)

        return task_output, domain_logits

    def extract_features(self, x: torch.Tensor) -> torch.Tensor:
        """Extract shared features only"""
        return self.shared_features(x)

class DomainAdversarialTransfer:
    """
    Domain adversarial transfer learning approach
    """
    def __init__(self,
                 source_env,
                 target_env,
                 network: DomainAdaptationNetwork,
                 learning_rate: float = 1e-4):
        self.source_env = source_env
        self.target_env = target_env
        self.network = network
        self.learning_rate = learning_rate

        # Optimizers
        self.task_optimizer = optim.Adam(network.parameters(), lr=learning_rate)
        self.domain_optimizer = optim.Adam(network.parameters(), lr=learning_rate)

        # Replay buffers for both domains
        self.source_buffer = []
        self.target_buffer = []
        self.buffer_size = 50000

    def train_adversarial(self,
                         num_episodes: int = 1000,
                         batch_size: int = 64,
                         domain_adv_steps: int = 5) -> Dict[str, List[float]]:
        """
        Train with domain adversarial loss to improve transfer
        """
        source_rewards = []
        target_rewards = []

        for episode in range(num_episodes):
            # Train on source domain
            if episode % 2 == 0:  # Alternate between domains
                domain_id = 0
                env = self.source_env
                buffer = self.source_buffer
            else:
                domain_id = 1
                env = self.target_env
                buffer = self.target_buffer

            # Collect experience in current domain
            state = env.reset()
            total_reward = 0
            done = False

            while not done:
                with torch.no_grad():
                    action, _ = self.network(torch.FloatTensor(state), domain_id)
                    action = action.argmax().item()

                next_state, reward, done, info = env.step(action)

                buffer.append((state, action, reward, next_state, done, domain_id))
                if len(buffer) > self.buffer_size:
                    buffer.pop(0)

                state = next_state
                total_reward += reward

            if domain_id == 0:
                source_rewards.append(total_reward)
            else:
                target_rewards.append(total_reward)

            # Domain adversarial training
            if len(self.source_buffer) >= batch_size and len(self.target_buffer) >= batch_size:
                for _ in range(domain_adv_steps):
                    self._train_domain_adversarial(batch_size)

            # Print progress
            if episode % 100 == 0:
                if source_rewards:
                    avg_source = np.mean(source_rewards[-100:])
                else:
                    avg_source = 0
                if target_rewards:
                    avg_target = np.mean(target_rewards[-100:])
                else:
                    avg_target = 0
                print(f"Episode {episode}: Source Avg = {avg_source:.2f}, Target Avg = {avg_target:.2f}")

        return {'source_rewards': source_rewards, 'target_rewards': target_rewards}

    def _train_domain_adversarial(self, batch_size: int):
        """
        Train domain adversarial loss
        """
        # Sample from both domains
        source_indices = np.random.choice(len(self.source_buffer), batch_size//2, replace=False)
        target_indices = np.random.choice(len(self.target_buffer), batch_size//2, replace=False)

        source_batch = [self.source_buffer[i] for i in source_indices]
        target_batch = [self.target_buffer[i] for i in target_indices]

        # Combine batches
        all_batch = source_batch + target_batch
        states = torch.FloatTensor([t[0] for t in all_batch])
        domain_labels = torch.LongTensor([t[5] for t in all_batch])  # domain_id is at index 5

        # Train domain classifier to distinguish domains
        _, domain_logits = self.network(states, 0)  # domain_id doesn't matter for features
        domain_loss = nn.CrossEntropyLoss()(domain_logits, domain_labels)

        # Minimize domain classification accuracy (maximize confusion)
        self.domain_optimizer.zero_grad()
        (-domain_loss).backward()  # Negative because we want to confuse the classifier
        self.domain_optimizer.step()

        # Train task networks to perform well while confusing domain classifier
        _, domain_logits = self.network(states, 0)
        task_domain_loss = nn.CrossEntropyLoss()(domain_logits, domain_labels)

        self.task_optimizer.zero_grad()
        # Task loss (depends on the specific task)
        task_loss = self._compute_task_loss(states, source_batch, target_batch)

        # Combined loss: task performance + domain confusion
        total_loss = task_loss - self.network.domain_loss_weight * task_domain_loss
        total_loss.backward()
        self.task_optimizer.step()

    def _compute_task_loss(self, states, source_batch, target_batch):
        """
        Compute task-specific loss (placeholder implementation)
        """
        # This would depend on the specific task
        # For this example, we'll return a dummy loss
        return torch.tensor(0.0, requires_grad=True)

# Example domain adaptation usage
def example_domain_adaptation():
    """
    Example of domain adaptation between sim and real
    """
    # Create domain adaptation network
    domain_net = DomainAdaptationNetwork(
        input_dim=24,
        output_dim=8,
        hidden_dims=[256, 256],
        num_domains=2  # sim and real
    )

    # Initialize with source (simulation) environment
    # source_env = IsaacSimWalkerEnvironment()  # Actual Isaac Sim env
    # target_env = RealRobotEnvironment()       # Actual robot env

    # Create domain adaptation trainer
    da_trainer = DomainAdversarialTransfer(
        source_env=None,  # Placeholder
        target_env=None,  # Placeholder
        network=domain_net
    )

    # Train with domain adversarial approach
    # results = da_trainer.train_adversarial(num_episodes=5000)

    print("Domain adaptation setup completed!")
    return da_trainer
```

### 3. Meta-Learning for Rapid Adaptation

Meta-learning enables rapid adaptation to new tasks by learning a good initialization or learning algorithm:

```python
class MAML(nn.Module):
    """
    Model-Agnostic Meta-Learning implementation
    """
    def __init__(self,
                 policy_network: nn.Module,
                 meta_lr: float = 1e-3,
                 adaptation_lr: float = 1e-2,
                 first_order: bool = True):
        super(MAML, self).__init__()

        self.policy_network = policy_network
        self.meta_lr = meta_lr
        self.adaptation_lr = adaptation_lr
        self.first_order = first_order

        # Meta-learner optimizer
        self.meta_optimizer = optim.Adam(policy_network.parameters(), lr=meta_lr)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.policy_network(x)

    def adapt(self,
              support_tasks: List[Tuple[List, List]],  # [(states, actions), ...]
              num_adaptation_steps: int = 1) -> nn.Module:
        """
        Adapt the policy to new tasks using support data
        """
        adapted_network = copy.deepcopy(self.policy_network)

        for _ in range(num_adaptation_steps):
            total_loss = 0.0

            for states, actions in support_tasks:
                # Forward pass on support data
                predictions = adapted_network(torch.FloatTensor(states))
                loss = nn.MSELoss()(predictions, torch.FloatTensor(actions))
                total_loss += loss

            total_loss /= len(support_tasks)

            # Compute gradients
            gradients = torch.autograd.grad(total_loss,
                                          adapted_network.parameters(),
                                          create_graph=not self.first_order,
                                          retain_graph=True)

            # Update adapted network parameters
            for param, grad in zip(adapted_network.parameters(), gradients):
                param.data = param.data - self.adaptation_lr * grad

        return adapted_network

    def meta_update(self,
                   query_tasks: List[Tuple[List, List]],  # [(states, actions), ...]
                   support_tasks: List[Tuple[List, List]]) -> float:
        """
        Update meta-learner using query tasks after adaptation
        """
        # Create adapted networks for each task
        adapted_networks = []
        for support_data in support_tasks:
            adapted_net = self.adapt([support_data])
            adapted_networks.append(adapted_net)

        # Compute loss on query tasks using adapted networks
        total_loss = 0.0
        for i, query_data in enumerate(query_tasks):
            states, actions = query_data
            adapted_net = adapted_networks[i]

            predictions = adapted_net(torch.FloatTensor(states))
            loss = nn.MSELoss()(predictions, torch.FloatTensor(actions))
            total_loss += loss

        total_loss /= len(query_tasks)

        # Update meta-parameters
        self.meta_optimizer.zero_grad()
        total_loss.backward()
        self.meta_optimizer.step()

        return total_loss.item()

class MetaLearningTrainer:
    """
    Trainer for meta-learning approaches
    """
    def __init__(self,
                 maml: MAML,
                 task_sampler,
                 num_meta_tasks: int = 20,
                 num_support_samples: int = 10,
                 num_query_samples: int = 5):
        self.maml = maml
        self.task_sampler = task_sampler
        self.num_meta_tasks = num_meta_tasks
        self.num_support_samples = num_support_samples
        self.num_query_samples = num_query_samples

    def train_meta_learning(self, num_iterations: int = 1000) -> List[float]:
        """
        Train using meta-learning approach
        """
        meta_losses = []

        for iteration in range(num_iterations):
            # Sample meta tasks
            support_tasks = []
            query_tasks = []

            for _ in range(self.num_meta_tasks):
                task = self.task_sampler.sample_task()

                # Generate support and query data for this task
                support_data = self._generate_task_data(task, self.num_support_samples)
                query_data = self._generate_task_data(task, self.num_query_samples)

                support_tasks.append(support_data)
                query_tasks.append(query_data)

            # Perform meta update
            meta_loss = self.maml.meta_update(query_tasks, support_tasks)
            meta_losses.append(meta_loss)

            if iteration % 100 == 0:
                avg_loss = np.mean(meta_losses[-100:])
                print(f"Meta Iteration {iteration}: Average Loss = {avg_loss:.4f}")

        return meta_losses

    def _generate_task_data(self, task, num_samples: int):
        """
        Generate data for a specific task (placeholder implementation)
        """
        # This would involve running the policy in the task environment
        # to collect state-action pairs
        states = np.random.randn(num_samples, 24)  # Example state dimension
        actions = np.random.randn(num_samples, 8)  # Example action dimension
        return states, actions

    def adapt_to_new_task(self, new_task, adaptation_steps: int = 5):
        """
        Adapt to a new task using meta-learned initialization
        """
        # Sample data from new task
        support_data = self._generate_task_data(new_task, self.num_support_samples)

        # Adapt the policy
        adapted_policy = self.maml.adapt([support_data], adaptation_steps)

        return adapted_policy

# Example meta-learning usage
def example_meta_learning():
    """
    Example of meta-learning for robotic tasks
    """
    # Create a simple policy network
    policy_net = nn.Sequential(
        nn.Linear(24, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 8)
    )

    # Initialize MAML
    maml = MAML(
        policy_network=policy_net,
        meta_lr=1e-4,
        adaptation_lr=1e-3,
        first_order=True
    )

    # Create meta-learning trainer
    # task_sampler = RoboticTaskSampler()  # Actual task sampler
    meta_trainer = MetaLearningTrainer(maml, None)  # Placeholder for task sampler

    # Train meta-learner
    # meta_losses = meta_trainer.train_meta_learning(num_iterations=2000)

    print("Meta-learning setup completed!")
    return meta_trainer
```

## Sim-to-Real Transfer Techniques

### Domain Randomization

Domain randomization helps improve sim-to-real transfer by training in randomized simulation environments:

```python
class DomainRandomizationTrainer:
    """
    Trainer using domain randomization for sim-to-real transfer
    """
    def __init__(self,
                 agent,
                 base_env_params: Dict[str, Tuple[float, float]],  # {param_name: (min_val, max_val)}
                 num_domains: int = 50):
        self.agent = agent
        self.base_env_params = base_env_params
        self.num_domains = num_domains

        # Store different environment configurations
        self.domain_configs = []
        self._generate_domain_configs()

    def _generate_domain_configs(self):
        """
        Generate different domain configurations by randomizing parameters
        """
        for _ in range(self.num_domains):
            config = {}
            for param_name, (min_val, max_val) in self.base_env_params.items():
                # Sample random value in range
                if isinstance(min_val, int) and isinstance(max_val, int):
                    val = np.random.randint(min_val, max_val + 1)
                else:
                    val = np.random.uniform(min_val, max_val)
                config[param_name] = val
            self.domain_configs.append(config)

    def train_with_domain_rand(self, total_timesteps: int = 1000000) -> List[float]:
        """
        Train with domain randomization
        """
        episode_rewards = []
        timesteps_per_domain = total_timesteps // self.num_domains

        for domain_idx in range(self.num_domains):
            # Set environment parameters for this domain
            domain_config = self.domain_configs[domain_idx]
            # self.agent.env.set_parameters(domain_config)  # Set environment parameters

            # Train on this domain
            domain_rewards = self._train_on_domain(timesteps_per_domain // self.num_domains)
            episode_rewards.extend(domain_rewards)

        return episode_rewards

    def _train_on_domain(self, timesteps: int) -> List[float]:
        """
        Train on a single domain configuration
        """
        rewards = []

        for t in range(timesteps):
            # Standard training loop
            # state = self.agent.env.reset()
            # action = self.agent.select_action(state)
            # next_state, reward, done, info = self.agent.env.step(action)
            # self.agent.store_transition(state, action, reward, next_state, done)
            # self.agent.train()

            # Placeholder reward
            reward = np.random.normal(0, 1)
            rewards.append(reward)

        return rewards

# Example domain randomization parameters
domain_rand_params = {
    'robot_mass_range': (0.8, 1.2),      # 80% to 120% of nominal mass
    'friction_range': (0.4, 1.6),       # Friction coefficient range
    'actuator_strength_range': (0.9, 1.1), # 90% to 110% of nominal strength
    'sensor_noise_std_range': (0.0, 0.05), # Sensor noise range
    'control_delay_range': (0.0, 0.02),    # 0-20ms control delay
    'gravity_range': (-10.0, -9.6),        # Gravity variation
    'ground_slope_range': (-0.1, 0.1)      # Ground slope variation
}

### System Identification and Domain Gap Reduction

```python
class SystemIDTrainer:
    """
    Trainer that performs system identification to reduce sim-to-real gap
    """
    def __init__(self, sim_env, real_env, identification_method='gp'):
        self.sim_env = sim_env
        self.real_env = real_env
        self.identification_method = identification_method

        # Models to capture the sim-to-real gap
        self.gap_models = {}

        # Data for system identification
        self.sim_data = []
        self.real_data = []

    def collect_identification_data(self, num_samples: int = 1000):
        """
        Collect data from both simulation and real environments
        """
        print("Collecting identification data from simulation...")
        for _ in range(num_samples):
            # Generate random actions
            action = np.random.uniform(-1, 1, size=self.sim_env.action_space.shape)

            # Apply action in simulation
            sim_state, sim_reward, sim_done, sim_info = self.sim_env.step(action)

            # Store simulation data
            self.sim_data.append({
                'state': sim_state.copy(),
                'action': action.copy(),
                'next_state': sim_state.copy(),  # Will be updated
                'reward': sim_reward
            })

        print("Collecting identification data from real environment...")
        for i in range(min(num_samples, len(self.real_env))):
            # Apply same actions to real environment if possible
            if i < len(self.real_data):
                continue  # Already have real data

            action = self.sim_data[i]['action']

            # Apply action in real environment
            real_state, real_reward, real_done, real_info = self.real_env.step(action)

            # Store real data
            self.real_data.append({
                'state': real_state.copy(),
                'action': action.copy(),
                'next_state': real_state.copy(),
                'reward': real_reward
            })

    def identify_system_gap(self):
        """
        Identify the gap between simulation and real system
        """
        if len(self.sim_data) != len(self.real_data):
            print("Warning: Different number of sim and real samples. Using minimum.")
            n_samples = min(len(self.sim_data), len(self.real_data))
        else:
            n_samples = len(self.sim_data)

        # Calculate state differences
        state_diffs = []
        for i in range(n_samples):
            sim_state = self.sim_data[i]['state']
            real_state = self.real_data[i]['state']

            state_diff = real_state - sim_state
            state_diffs.append(state_diff)

        # Fit model to predict real state from sim state
        if self.identification_method == 'gp':
            from sklearn.gaussian_process import GaussianProcessRegressor
            from sklearn.gaussian_process.kernels import RBF, ConstantKernel

            # Prepare training data: [sim_state, action] -> [real_state - sim_state]
            X_train = []
            y_train = []

            for i in range(n_samples):
                X_train.append(np.concatenate([
                    self.sim_data[i]['state'],
                    self.sim_data[i]['action']
                ]))
                y_train.append(state_diffs[i])

            X_train = np.array(X_train)
            y_train = np.array(y_train)

            # Train Gaussian Process models for each state dimension
            for dim in range(y_train.shape[1]):
                kernel = ConstantKernel(1.0) * RBF(1.0)
                gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)
                gp.fit(X_train, y_train[:, dim])
                self.gap_models[f'dim_{dim}'] = gp

    def adapt_simulation(self, sim_state: np.ndarray, action: np.ndarray) -> np.ndarray:
        """
        Adapt simulation state to better match real system
        """
        # Predict gap using trained models
        input_features = np.concatenate([sim_state, action])
        adapted_state = sim_state.copy()

        for dim, model in self.gap_models.items():
            gap_prediction = model.predict(input_features.reshape(1, -1))[0]
            adapted_state[int(dim.split('_')[1])] += gap_prediction

        return adapted_state

    def train_with_adapted_simulation(self, total_timesteps: int = 1000000):
        """
        Train using adapted simulation that accounts for sim-to-real gap
        """
        episode_rewards = []

        for t in range(total_timesteps):
            # Get state from simulation
            sim_state = self.sim_env.current_state

            # Adapt state to account for sim-to-real gap
            adapted_state = self.adapt_simulation(sim_state, np.zeros_like(sim_state))

            # Select action using adapted state
            action = self.agent.select_action(adapted_state)

            # Apply action in simulation
            next_sim_state, reward, done, info = self.sim_env.step(action)

            # Adapt next state as well
            adapted_next_state = self.adapt_simulation(next_sim_state, action)

            # Store transition with adapted states
            self.agent.store_transition(adapted_state, action, reward, adapted_next_state, done)
            self.agent.train()

            if done:
                episode_rewards.append(self.agent.current_episode_reward)
                self.agent.current_episode_reward = 0

        return episode_rewards
```

### Adaptive Control Integration

Adaptive control techniques can be integrated with RL for better transfer:

```python
class AdaptiveControlRLAgent:
    """
    RL agent integrated with adaptive control for robustness
    """
    def __init__(self,
                 rl_agent,
                 adaptive_controller_params: Dict[str, float] = None):
        self.rl_agent = rl_agent
        self.adaptive_controller_params = adaptive_controller_params or {
            'gamma': 0.1,      # Adaptation rate
            'delta': 0.01,     # Robustness parameter
            'theta_init': 0.1   # Initial parameter estimates
        }

        # Adaptive parameters for unknown dynamics
        self.adaptive_params = np.full(8, self.adaptive_controller_params['theta_init'])  # Example: 8 parameters

        # Parameter bounds
        self.param_bounds = (-10.0, 10.0)

    def compute_adaptive_control(self, state: np.ndarray,
                               rl_action: np.ndarray) -> np.ndarray:
        """
        Compute adaptive control correction to RL action
        """
        # Compute adaptive control term based on current state
        # This is a simplified example - in practice, this would be based on
        # system identification of unknown dynamics

        # Example: adaptive compensation for unmodeled dynamics
        adaptive_term = np.zeros_like(rl_action)

        # Use state-dependent regressor for adaptive law
        regressor = self._compute_regressor(state)

        # Adaptive control law
        adaptive_term = regressor @ self.adaptive_params

        # Apply adaptive correction to RL action
        corrected_action = rl_action + adaptive_term

        # Clip to valid range
        corrected_action = np.clip(corrected_action, -1.0, 1.0)

        return corrected_action

    def _compute_regressor(self, state: np.ndarray) -> np.ndarray:
        """
        Compute regressor vector for adaptive control
        """
        # This would be tailored to specific unknown dynamics
        # For example, if we're compensating for unmodeled friction:
        position = state[:4]  # Example: first 4 elements are positions
        velocity = state[4:8]  # Example: next 4 elements are velocities

        # Regressor based on position and velocity
        regressor = np.concatenate([
            np.abs(velocity),           # Absolute velocity (for Coulomb friction)
            velocity * np.abs(velocity), # Velocity * |velocity| (for viscous friction)
            np.sign(velocity) * 0.1     # Sign of velocity (for dry friction)
        ])

        return regressor[:len(self.adaptive_params)]  # Match parameter dimension

    def update_adaptive_params(self, state: np.ndarray,
                             action: np.ndarray,
                             next_state: np.ndarray):
        """
        Update adaptive parameters based on prediction error
        """
        # Predict next state using current model + adaptive compensation
        predicted_state = self._predict_state(state, action)

        # Calculate prediction error
        prediction_error = next_state - predicted_state

        # Regressor for adaptation law
        regressor = self._compute_regressor(state)

        # Adaptive law: θ̇ = -γ * φ * e (gradient descent)
        adaptation_rate = self.adaptive_controller_params['gamma']
        param_update = -adaptation_rate * regressor * prediction_error[:len(regressor)]

        # Apply robust modification to prevent drift
        robust_term = self.adaptive_controller_params['delta'] * np.tanh(
            self.adaptive_params / self.adaptive_controller_params['delta']
        )
        param_update -= robust_term

        # Update parameters
        self.adaptive_params += param_update

        # Apply parameter bounds
        self.adaptive_params = np.clip(self.adaptive_params,
                                     self.param_bounds[0],
                                     self.param_bounds[1])

    def _predict_state(self, state: np.ndarray, action: np.ndarray) -> np.ndarray:
        """
        Predict next state using model + adaptive compensation
        """
        # This would use the underlying system dynamics model
        # For now, return a simplified prediction
        return state + 0.1 * action  # Simplified integration

    def select_action(self, state: np.ndarray) -> np.ndarray:
        """
        Select action using RL policy with adaptive control correction
        """
        # Get RL action
        rl_action = self.rl_agent.select_action(state)

        # Apply adaptive control correction
        corrected_action = self.compute_adaptive_control(state, rl_action)

        return corrected_action

    def train(self, state: np.ndarray, action: np.ndarray,
              reward: float, next_state: np.ndarray, done: bool):
        """
        Train both RL agent and adaptive controller
        """
        # Update adaptive parameters
        self.update_adaptive_params(state, action, next_state)

        # Train RL agent with corrected action
        self.rl_agent.train(state, action, reward, next_state, done)

# Example usage
def example_adaptive_rl():
    """
    Example of adaptive RL for robust control
    """
    # Initialize RL agent (e.g., SAC, PPO, etc.)
    # rl_agent = SACAgent(state_dim=24, action_dim=8, max_action=1.0)

    # Create adaptive RL agent
    adaptive_agent = AdaptiveControlRLAgent(None)  # Placeholder for RL agent

    print("Adaptive RL agent initialized with adaptive control integration")
    return adaptive_agent
```

## Transfer Learning Evaluation Metrics

### Domain Adaptation Metrics

```python
class TransferEvaluator:
    """
    Evaluate transfer learning performance
    """
    def __init__(self):
        self.metrics = {
            'transfer_efficiency': [],
            'adaptation_speed': [],
            'asymptotic_performance': [],
            'negative_transfer': [],
            'positive_transfer': []
        }

    def evaluate_transfer(self,
                         source_performance: List[float],
                         target_performance: List[float],
                         baseline_performance: List[float] = None) -> Dict[str, float]:
        """
        Evaluate transfer performance using various metrics
        """
        if baseline_performance is None:
            baseline_performance = target_performance  # Use target as baseline if none provided

        # Transfer efficiency: ratio of transfer improvement to baseline improvement
        if len(baseline_performance) >= 10 and len(target_performance) >= 10:
            baseline_recent = np.mean(baseline_performance[-10:])
            target_recent = np.mean(target_performance[-10:])

            if baseline_recent != 0:
                transfer_efficiency = target_recent / baseline_recent
            else:
                transfer_efficiency = float('inf')
        else:
            transfer_efficiency = 0.0

        # Adaptation speed: how quickly target performance improves
        if len(target_performance) >= 20:
            early_perf = np.mean(target_performance[:10])
            late_perf = np.mean(target_performance[-10:])
            adaptation_speed = (late_perf - early_perf) / len(target_performance)
        else:
            adaptation_speed = 0.0

        # Asymptotic performance: final performance level
        asymptotic_performance = np.mean(target_performance[-50:]) if len(target_performance) >= 50 else np.mean(target_performance)

        # Check for negative/positive transfer
        if len(target_performance) >= 10 and len(source_performance) >= 10:
            initial_target = np.mean(target_performance[:10])
            source_baseline = np.mean(source_performance[-10:])

            if initial_target < source_baseline:
                negative_transfer = True
                positive_transfer = False
            else:
                negative_transfer = False
                positive_transfer = True
        else:
            negative_transfer = False
            positive_transfer = False

        metrics = {
            'transfer_efficiency': transfer_efficiency,
            'adaptation_speed': adaptation_speed,
            'asymptotic_performance': asymptotic_performance,
            'negative_transfer': negative_transfer,
            'positive_transfer': positive_transfer
        }

        return metrics

    def plot_transfer_analysis(self,
                              source_rewards: List[float],
                              target_rewards: List[float],
                              baseline_rewards: List[float] = None):
        """
        Plot transfer learning analysis
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Plot 1: Learning curves comparison
        axes[0, 0].plot(source_rewards, alpha=0.3, label='Source Task', color='blue')
        axes[0, 0].plot(target_rewards, alpha=0.3, label='Target Task (with transfer)', color='red')

        if baseline_rewards is not None:
            axes[0, 0].plot(baseline_rewards, alpha=0.3, label='Target Task (without transfer)', color='green')

        # Add moving averages
        if len(source_rewards) >= 10:
            sr_ma = [np.mean(source_rewards[max(0, i-9):i+1]) for i in range(len(source_rewards))]
            axes[0, 0].plot(sr_ma, label='Source MA (10)', color='blue', linewidth=2)

        if len(target_rewards) >= 10:
            tr_ma = [np.mean(target_rewards[max(0, i-9):i+1]) for i in range(len(target_rewards))]
            axes[0, 0].plot(tr_ma, label='Target MA (10)', color='red', linewidth=2)

        if baseline_rewards is not None and len(baseline_rewards) >= 10:
            br_ma = [np.mean(baseline_rewards[max(0, i-9):i+1]) for i in range(len(baseline_rewards))]
            axes[0, 0].plot(br_ma, label='Baseline MA (10)', color='green', linewidth=2)

        axes[0, 0].set_title('Learning Curves Comparison')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # Plot 2: Performance improvement
        if baseline_rewards is not None and len(baseline_rewards) == len(target_rewards):
            improvement = np.array(target_rewards) - np.array(baseline_rewards)
            axes[0, 1].plot(improvement, alpha=0.3, color='purple')
            if len(improvement) >= 10:
                imp_ma = [np.mean(improvement[max(0, i-9):i+1]) for i in range(len(improvement))]
                axes[0, 1].plot(imp_ma, color='purple', linewidth=2, label='Improvement MA (10)')
            axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            axes[0, 1].set_title('Performance Improvement over Baseline')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Reward Difference')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # Plot 3: Transfer efficiency over time
        window_size = 20
        if len(target_rewards) >= window_size and len(baseline_rewards) >= window_size:
            transfer_efficiencies = []
            for i in range(window_size, len(target_rewards)):
                target_window = np.mean(target_rewards[i-window_size:i])
                baseline_window = np.mean(baseline_rewards[i-window_size:i])

                if baseline_window != 0:
                    eff = target_window / baseline_window
                    transfer_efficiencies.append(eff)
                else:
                    transfer_efficiencies.append(float('inf'))

            axes[1, 0].plot(transfer_efficiencies, color='orange', linewidth=2)
            axes[1, 0].axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='No transfer effect')
            axes[1, 0].set_title('Transfer Efficiency Over Time')
            axes[1, 0].set_xlabel('Episode Window')
            axes[1, 0].set_ylabel('Transfer Efficiency')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # Plot 4: Final performance comparison
        final_target = np.mean(target_rewards[-50:]) if len(target_rewards) >= 50 else np.mean(target_rewards)
        final_baseline = np.mean(baseline_rewards[-50:]) if baseline_rewards is not None and len(baseline_rewards) >= 50 else (np.mean(baseline_rewards) if baseline_rewards else 0)

        bars = ['Target (Transfer)', 'Baseline (No Transfer)']
        values = [final_target, final_baseline]

        axes[1, 1].bar(bars, values, color=['red', 'green'], alpha=0.7)
        axes[1, 1].set_title('Final Performance Comparison')
        axes[1, 1].set_ylabel('Average Reward')
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

# Example evaluation
def example_transfer_evaluation():
    """
    Example of evaluating transfer learning performance
    """
    evaluator = TransferEvaluator()

    # Simulated performance data
    source_rewards = np.random.normal(10, 2, 1000).cumsum() / (np.arange(1000) + 1)  # Decreasing variance
    target_rewards = np.random.normal(8, 3, 1000).cumsum() / (np.arange(1000) + 1) + 2  # Higher baseline due to transfer
    baseline_rewards = np.random.normal(5, 4, 1000).cumsum() / (np.arange(1000) + 1)  # Lower baseline without transfer

    # Evaluate transfer
    metrics = evaluator.evaluate_transfer(list(source_rewards),
                                        list(target_rewards),
                                        list(baseline_rewards))

    print("Transfer Learning Evaluation Metrics:")
    for metric, value in metrics.items():
        print(f"{metric}: {value}")

    # Plot analysis
    evaluator.plot_transfer_analysis(list(source_rewards),
                                   list(target_rewards),
                                   list(baseline_rewards))

    return metrics
```

## Practical Considerations

### When to Apply Transfer Learning

Transfer learning is most beneficial when:

1. **Tasks are related**: Source and target tasks share underlying structure
2. **Limited data**: Target domain has limited training data
3. **Similar dynamics**: Physical properties are comparable between domains
4. **Computational constraints**: Need to reduce training time

### Challenges and Limitations

1. **Negative transfer**: When source knowledge hurts target performance
2. **Domain mismatch**: Significant differences between source and target domains
3. **Overfitting to source**: Model becomes too specialized to source task
4. **Catastrophic forgetting**: Forgetting source task knowledge during transfer

## Implementation Best Practices

### Architecture Design for Transfer

```python
class TransferableArchitecture(nn.Module):
    """
    Neural network architecture designed for easy transfer
    """
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 feature_dim: int = 256,
                 num_shared_layers: int = 3,
                 num_task_specific_layers: int = 2):
        super(TransferableArchitecture, self).__init__()

        # Shared feature extractor (transferable)
        self.shared_layers = nn.Sequential()
        prev_dim = input_dim
        for i in range(num_shared_layers):
            layer = nn.Linear(prev_dim, feature_dim)
            self.shared_layers.add_module(f'shared_fc_{i}', layer)
            self.shared_layers.add_module(f'shared_relu_{i}', nn.ReLU())
            prev_dim = feature_dim

        # Task-specific head (adaptable)
        self.task_head = nn.Sequential()
        for i in range(num_task_specific_layers):
            layer_dim = feature_dim if i < num_task_specific_layers - 1 else output_dim
            layer = nn.Linear(prev_dim, layer_dim)
            self.task_head.add_module(f'task_fc_{i}', layer)
            if i < num_task_specific_layers - 1:  # Don't add ReLU after final layer
                self.task_head.add_module(f'task_relu_{i}', nn.ReLU())
            prev_dim = layer_dim

        # Store architecture info for transfer
        self.feature_dim = feature_dim
        self.num_shared_layers = num_shared_layers
        self.num_task_specific_layers = num_task_specific_layers

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        features = self.shared_layers(x)
        output = self.task_head(features)
        return output

    def get_features(self, x: torch.Tensor) -> torch.Tensor:
        """Extract shared features only"""
        return self.shared_layers(x)

    def freeze_shared_layers(self):
        """Freeze shared layers during transfer"""
        for param in self.shared_layers.parameters():
            param.requires_grad = False

    def unfreeze_shared_layers(self):
        """Unfreeze shared layers"""
        for param in self.shared_layers.parameters():
            param.requires_grad = True

def create_transferable_agent(source_task_config: Dict, target_task_config: Dict):
    """
    Create an agent that can be easily transferred between tasks
    """
    # Create architecture suitable for both tasks
    max_input_dim = max(source_task_config['input_dim'], target_task_config['input_dim'])
    max_output_dim = max(source_task_config['output_dim'], target_task_config['output_dim'])

    architecture = TransferableArchitecture(
        input_dim=max_input_dim,
        output_dim=max_output_dim,
        feature_dim=256,
        num_shared_layers=3,
        num_task_specific_layers=2
    )

    # Initialize with source task
    source_agent = PPOAgent(architecture)

    # Prepare for transfer to target task
    target_architecture = TransferableArchitecture(
        input_dim=target_task_config['input_dim'],
        output_dim=target_task_config['output_dim'],
        feature_dim=256,
        num_shared_layers=3,
        num_task_specific_layers=2
    )

    # Transfer shared weights
    target_architecture.shared_layers.load_state_dict(
        source_agent.network.shared_layers.state_dict()
    )

    # Initialize target agent with transferred architecture
    target_agent = PPOAgent(target_architecture)

    return source_agent, target_agent
```

## Conclusion

Transfer learning in deep reinforcement learning provides a powerful approach to accelerate learning in robotics applications. By leveraging pre-trained knowledge from source tasks, environments, or simulations, robots can adapt more quickly to new scenarios with minimal additional training. The key to successful transfer lies in identifying the right components to transfer, using appropriate adaptation techniques, and carefully managing the balance between preserving useful knowledge and adapting to new requirements.

The techniques covered in this chapter—from policy transfer and domain adaptation to meta-learning and sim-to-real transfer—offer various approaches depending on the specific requirements of the robotic application. The choice of method depends on factors such as the similarity between source and target domains, computational resources, and the urgency of deployment. As robotic systems become more complex and deployment scenarios more diverse, transfer learning will continue to play an increasingly important role in making reinforcement learning practical for real-world robotic applications.
