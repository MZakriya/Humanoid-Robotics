---
id: module7-drl-isaac-sim-7.4-training-agents
title: "Training Agents"
slug: /module7-drl-isaac-sim-7.4-training-agents
---

# Training Agents in Isaac Sim

Training agents in Isaac Sim leverages the high-performance physics simulation and parallel environment capabilities to accelerate reinforcement learning. This chapter covers various training methodologies, from classical RL algorithms to advanced techniques like curriculum learning and domain randomization.

## Overview of Training Paradigms

Isaac Sim supports multiple training paradigms for different types of robotic tasks:

- **On-policy methods**: PPO, TRPO, REINFORCE
- **Off-policy methods**: DDPG, TD3, SAC, DQN
- **Multi-agent methods**: MADDPG, QMIX, MAPPO
- **Imitation learning**: Behavioral cloning, GAIL
- **Curriculum learning**: Progressive task complexity

## On-Policy Training (PPO Implementation)

Proximal Policy Optimization (PPO) is a popular on-policy algorithm that balances sample efficiency with stability:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.distributions import Normal, Categorical
import copy

class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, action_type='continuous'):
        super(ActorCritic, self).__init__()

        self.action_type = action_type

        # Shared layers
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU()
        )

        # Actor head
        if action_type == 'continuous':
            self.actor_mean = nn.Linear(256, action_dim)
            self.actor_logstd = nn.Parameter(torch.zeros(action_dim))
        else:  # discrete
            self.actor = nn.Linear(256, action_dim)

        # Critic head
        self.critic = nn.Linear(256, 1)

    def forward(self, state):
        shared_out = self.shared(state)

        if self.action_type == 'continuous':
            mean = self.actor_mean(shared_out)
            std = torch.exp(self.actor_logstd)
            value = self.critic(shared_out)
            return mean, std, value
        else:
            logits = self.actor(shared_out)
            value = self.critic(shared_out)
            return logits, value

    def get_action(self, state):
        if self.action_type == 'continuous':
            mean, std, value = self.forward(state)
            dist = Normal(mean, std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(axis=-1)
        else:
            logits, value = self.forward(state)
            probs = torch.softmax(logits, dim=-1)
            dist = Categorical(probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)

        return action, log_prob, value

class PPOBuffer:
    def __init__(self, state_dim, action_dim, max_size, gamma=0.99, lam=0.95):
        self.max_size = max_size
        self.ptr = 0
        self.size = 0

        self.states = torch.zeros((max_size, state_dim))
        self.actions = torch.zeros((max_size, action_dim) if action_dim > 1 else (max_size,))
        self.rewards = torch.zeros(max_size)
        self.values = torch.zeros(max_size)
        self.log_probs = torch.zeros(max_size)
        self.returns = torch.zeros(max_size)
        self.advantages = torch.zeros(max_size)

        self.gamma = gamma
        self.lam = lam

    def store(self, state, action, reward, value, log_prob):
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.values[self.ptr] = value
        self.log_probs[self.ptr] = log_prob

        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)

    def finish_path(self, last_val=0):
        # Calculate advantages and returns
        rewards_plus = np.append(self.rewards[:self.size].numpy(), last_val)
        values_plus = np.append(self.values[:self.size].numpy(), last_val)

        deltas = rewards_plus[:-1] + self.gamma * values_plus[1:] - values_plus[:-1]

        self.advantages[:self.size] = torch.FloatTensor(
            self.discount_cumsum(deltas, self.gamma * self.lam)
        )

        self.returns[:self.size] = self.advantages[:self.size] + self.values[:self.size]

    def discount_cumsum(self, x, discount):
        """Calculate discounted cumulative sum"""
        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]

    def get_batch(self, batch_size=None):
        """Get a batch of data for training"""
        if batch_size is None:
            batch_size = self.size

        # Shuffle indices
        indices = np.random.choice(self.size, size=batch_size, replace=False)

        return (
            self.states[indices],
            self.actions[indices],
            self.returns[indices],
            self.advantages[indices],
            self.log_probs[indices]
        )

class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, clip_epsilon=0.2,
                 epochs=10, minibatch_size=64, action_type='continuous'):
        self.gamma = gamma
        self.clip_epsilon = clip_epsilon
        self.epochs = epochs
        self.minibatch_size = minibatch_size

        # Networks
        self.actor_critic = ActorCritic(state_dim, action_dim, action_type)
        self.old_actor_critic = copy.deepcopy(self.actor_critic)

        # Optimizer
        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)

        # Update counter
        self.update_count = 0

    def update(self, buffer):
        """Update the policy using PPO"""
        for epoch in range(self.epochs):
            for start in range(0, buffer.size, self.minibatch_size):
                end = start + self.minibatch_size
                batch = buffer.get_batch(self.minibatch_size)

                states, actions, returns, advantages, old_log_probs = batch

                # Normalize advantages
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

                # Get new policy values
                if self.actor_critic.action_type == 'continuous':
                    new_means, new_stds, new_values = self.actor_critic(states)
                    dist = Normal(new_means, new_stds)
                    new_log_probs = dist.log_prob(actions).sum(axis=-1)

                    # Ratio
                    ratio = torch.exp(new_log_probs - old_log_probs)

                    # Surrogate losses
                    surr1 = ratio * advantages
                    surr2 = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon) * advantages

                    # Actor loss
                    actor_loss = -torch.min(surr1, surr2).mean()
                else:
                    new_logits, new_values = self.actor_critic(states)
                    probs = torch.softmax(new_logits, dim=-1)
                    dist = Categorical(probs)
                    new_log_probs = dist.log_prob(actions)

                    # Ratio
                    ratio = torch.exp(new_log_probs - old_log_probs)

                    # Surrogate losses
                    surr1 = ratio * advantages
                    surr2 = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon) * advantages

                    # Actor loss
                    actor_loss = -torch.min(surr1, surr2).mean()

                # Critic loss
                critic_loss = nn.MSELoss()(new_values.squeeze(), returns)

                # Total loss
                total_loss = actor_loss + 0.5 * critic_loss

                # Update
                self.optimizer.zero_grad()
                total_loss.backward()
                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)
                self.optimizer.step()

        # Update old policy
        self.old_actor_critic.load_state_dict(self.actor_critic.state_dict())
        self.update_count += 1

# Example training loop for Isaac Sim
def train_ppo_agent(isaac_env, total_timesteps=1000000):
    """
    Train a PPO agent in Isaac Sim environment
    """
    import scipy.signal  # Import here to avoid global import issues

    # Get environment dimensions
    state_dim = isaac_env.get_obs_size()
    action_dim = isaac_env.get_action_size()
    action_type = 'continuous' if action_dim > 1 else 'discrete'

    # Initialize agent
    agent = PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        action_type=action_type,
        lr=3e-4,
        gamma=0.99,
        clip_epsilon=0.2,
        epochs=10,
        minibatch_size=64
    )

    # Initialize buffer
    buffer_size = 2048  # Standard PPO rollout length
    buffer = PPOBuffer(
        state_dim=state_dim,
        action_dim=action_dim,
        max_size=buffer_size,
        gamma=0.99,
        lam=0.95
    )

    # Training variables
    state = isaac_env.reset()
    episode_reward = 0
    episode_count = 0

    for t in range(total_timesteps):
        # Get action from policy
        with torch.no_grad():
            action, log_prob, value = agent.actor_critic.get_action(torch.FloatTensor(state))
            action = action.cpu().numpy()
            log_prob = log_prob.cpu().numpy()
            value = value.cpu().numpy()

        # Take action in environment
        next_state, reward, done, info = isaac_env.step(action)

        # Store in buffer
        buffer.store(
            torch.FloatTensor(state),
            torch.FloatTensor(action),
            torch.FloatTensor(reward),
            torch.FloatTensor(value),
            torch.FloatTensor(log_prob)
        )

        # Update counters
        episode_reward += reward
        state = next_state

        # Check if episode is done
        if done or t % buffer_size == 0:
            with torch.no_grad():
                _, _, last_value = agent.actor_critic.get_action(torch.FloatTensor(state))
                last_value = last_value.cpu().numpy()

            # Finish path and update
            buffer.finish_path(last_value)
            agent.update(buffer)

            # Reset buffer
            buffer.ptr = 0
            buffer.size = 0

            # Log episode
            if done:
                print(f"Episode {episode_count}: Reward = {episode_reward:.2f}")
                episode_reward = 0
                episode_count += 1
                state = isaac_env.reset()

    return agent
```

## Off-Policy Training (SAC Implementation)

Soft Actor-Critic (SAC) is an off-policy algorithm that maximizes both expected return and entropy:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.distributions import Normal
import random
from collections import deque

class SACActor(nn.Module):
    def __init__(self, state_dim, action_dim, max_action):
        super(SACActor, self).__init__()

        self.l1 = nn.Linear(state_dim, 256)
        self.l2 = nn.Linear(256, 256)

        # Mean and standard deviation for action distribution
        self.mean_linear = nn.Linear(256, action_dim)
        self.log_std_linear = nn.Linear(256, action_dim)

        self.max_action = max_action
        self.min_log_std = -20
        self.max_log_std = 2

    def forward(self, state):
        a = torch.relu(self.l1(state))
        a = torch.relu(self.l2(a))

        mean = self.mean_linear(a)
        log_std = self.log_std_linear(a)
        log_std = torch.clamp(log_std, self.min_log_std, self.max_log_std)

        return mean, log_std

    def sample(self, state):
        mean, log_std = self.forward(state)
        std = log_std.exp()

        normal = Normal(mean, std)
        x_t = normal.rsample()  # Reparameterization trick
        y_t = torch.tanh(x_t)
        action = y_t * self.max_action

        # Calculate log probability
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log(self.max_action * (1 - y_t.pow(2)) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)

        return action, log_prob

class SACCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(SACCritic, self).__init__()

        # Q1 network
        self.l1 = nn.Linear(state_dim + action_dim, 256)
        self.l2 = nn.Linear(256, 256)
        self.l3 = nn.Linear(256, 1)

        # Q2 network
        self.l4 = nn.Linear(state_dim + action_dim, 256)
        self.l5 = nn.Linear(256, 256)
        self.l6 = nn.Linear(256, 1)

    def forward(self, state, action):
        sa = torch.cat([state, action], 1)

        q1 = torch.relu(self.l1(sa))
        q1 = torch.relu(self.l2(q1))
        q1 = self.l3(q1)

        q2 = torch.relu(self.l4(sa))
        q2 = torch.relu(self.l5(q2))
        q2 = self.l6(q2)

        return q1, q2

    def Q1(self, state, action):
        sa = torch.cat([state, action], 1)

        q1 = torch.relu(self.l1(sa))
        q1 = torch.relu(self.l2(q1))
        q1 = self.l3(q1)

        return q1

class SACAgent:
    def __init__(self, state_dim, action_dim, max_action,
                 lr_actor=3e-4, lr_critic=3e-4, lr_alpha=3e-4,
                 gamma=0.99, tau=0.005, alpha=0.2):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_action = max_action
        self.gamma = gamma
        self.tau = tau
        self.alpha = alpha

        # Initialize networks
        self.actor = SACActor(state_dim, action_dim, max_action)
        self.critic = SACCritic(state_dim, action_dim)
        self.critic_target = SACCritic(state_dim, action_dim)

        # Copy weights to target networks
        self.critic_target.load_state_dict(self.critic.state_dict())

        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)

        # Automatic entropy tuning
        self.target_entropy = -torch.prod(torch.Tensor([action_dim])).item()
        self.log_alpha = torch.zeros(1, requires_grad=True)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr_alpha)

        # Replay buffer
        self.replay_buffer = deque(maxlen=1000000)
        self.batch_size = 256

    def store_transition(self, state, action, reward, next_state, done):
        """Store transition in replay buffer"""
        self.replay_buffer.append((state, action, reward, next_state, done))

    def train(self):
        """Train the SAC agent"""
        if len(self.replay_buffer) < self.batch_size:
            return

        # Sample batch from replay buffer
        batch = random.sample(self.replay_buffer, self.batch_size)
        state, action, reward, next_state, done = map(torch.FloatTensor, zip(*batch))

        # Compute target Q values
        with torch.no_grad():
            next_action, next_log_prob = self.actor.sample(next_state)
            next_q1, next_q2 = self.critic_target(next_state, next_action)
            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_prob
            target_q = reward + (1 - done) * self.gamma * next_q

        # Critic loss
        current_q1, current_q2 = self.critic(state, action)
        critic_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)

        # Update critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor loss
        actions_pred, log_prob = self.actor.sample(state)
        q1_pred, q2_pred = self.critic(state, actions_pred)
        current_q = torch.min(q1_pred, q2_pred)
        actor_loss = (self.alpha * log_prob - current_q).mean()

        # Update actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Update alpha (temperature parameter) for entropy regularization
        alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        self.alpha = self.log_alpha.exp()

        # Soft update target networks
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

    def select_action(self, state, evaluate=False):
        """Select action using current policy"""
        state = torch.FloatTensor(state).unsqueeze(0)

        if evaluate:
            # For evaluation, take mean action (deterministic)
            mean, log_std = self.actor(state)
            action = torch.tanh(mean) * self.max_action
        else:
            # For training, sample from distribution (stochastic)
            action, _ = self.actor.sample(state)

        return action.cpu().data.numpy().flatten()

# Example SAC training loop for Isaac Sim
def train_sac_agent(isaac_env, total_timesteps=1000000):
    """
    Train a SAC agent in Isaac Sim environment
    """
    # Get environment dimensions
    state_dim = isaac_env.get_obs_size()
    action_dim = isaac_env.get_action_size()
    max_action = 1.0  # Adjust based on your environment

    # Initialize agent
    agent = SACAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        max_action=max_action,
        lr_actor=3e-4,
        lr_critic=3e-4,
        lr_alpha=3e-4,
        gamma=0.99,
        tau=0.005,
        alpha=0.2
    )

    # Training variables
    state = isaac_env.reset()
    episode_reward = 0
    episode_count = 0

    for t in range(total_timesteps):
        # Select action with exploration noise
        action = agent.select_action(state, evaluate=False)

        # Take action in environment
        next_state, reward, done, info = isaac_env.step(action)

        # Store transition in replay buffer
        agent.store_transition(state, action, reward, next_state, done)

        # Update agent
        agent.train()

        # Update counters
        episode_reward += reward
        state = next_state

        # Check if episode is done
        if done:
            print(f"Episode {episode_count}: Reward = {episode_reward:.2f}, Alpha = {agent.alpha:.3f}")
            episode_reward = 0
            episode_count += 1
            state = isaac_env.reset()

    return agent
```

## Multi-Agent Training (MAPPO Implementation)

Multi-Agent PPO (MAPPO) extends PPO to multi-agent scenarios:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.distributions import Categorical

class MultiAgentActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, num_agents, hidden_dim=256):
        super(MultiAgentActorCritic, self).__init__()

        self.num_agents = num_agents
        self.action_dim = action_dim

        # Shared observation encoder
        self.shared_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Individual actor-critic heads for each agent
        self.actors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            ) for _ in range(num_agents)
        ])

        self.critics = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
            ) for _ in range(num_agents)
        ])

    def forward(self, obs_batch):
        """
        Forward pass for all agents
        obs_batch: [batch_size, num_agents, state_dim]
        """
        batch_size, _, _ = obs_batch.shape

        # Encode observations
        encoded = self.shared_encoder(obs_batch.view(-1, obs_batch.shape[-1]))
        encoded = encoded.view(batch_size, self.num_agents, -1)

        # Get actions and values for all agents
        actions = []
        action_probs = []
        values = []

        for i in range(self.num_agents):
            # Actor
            logits = self.actors[i](encoded[:, i, :])
            probs = torch.softmax(logits, dim=-1)
            dist = Categorical(probs)
            action = dist.sample()

            actions.append(action)
            action_probs.append(dist.log_prob(action))

            # Critic
            value = self.critics[i](encoded[:, i, :])
            values.append(value.squeeze(-1))

        return torch.stack(actions, dim=1), torch.stack(action_probs, dim=1), torch.stack(values, dim=1)

class MAPPOAgent:
    def __init__(self, state_dim, action_dim, num_agents, lr=3e-4, gamma=0.99,
                 clip_epsilon=0.2, epochs=10, minibatch_size=64):
        self.gamma = gamma
        self.clip_epsilon = clip_epsilon
        self.epochs = epochs
        self.minibatch_size = minibatch_size

        # Networks
        self.actor_critic = MultiAgentActorCritic(state_dim, action_dim, num_agents)
        self.old_actor_critic = MultiAgentActorCritic(state_dim, action_dim, num_agents)

        # Copy weights
        self.old_actor_critic.load_state_dict(self.actor_critic.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)

        # Update counter
        self.update_count = 0

    def update(self, obs_batch, actions_batch, returns_batch, advantages_batch, old_log_probs_batch):
        """Update the multi-agent policy using MAPPO"""
        for epoch in range(self.epochs):
            indices = torch.randperm(obs_batch.size(0))

            for start_idx in range(0, obs_batch.size(0), self.minibatch_size):
                end_idx = min(start_idx + self.minibatch_size, obs_batch.size(0))
                idx = indices[start_idx:end_idx]

                obs_mb = obs_batch[idx]
                actions_mb = actions_batch[idx]
                returns_mb = returns_batch[idx]
                advantages_mb = advantages_batch[idx]
                old_log_probs_mb = old_log_probs_batch[idx]

                # Normalize advantages
                advantages_mb = (advantages_mb - advantages_mb.mean()) / (advantages_mb.std() + 1e-8)

                # Get new policy values
                new_actions, new_log_probs, new_values = self.actor_critic(obs_mb)

                # Ratio
                ratio = torch.exp(new_log_probs - old_log_probs_mb)

                # Surrogate losses
                surr1 = ratio * advantages_mb
                surr2 = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon) * advantages_mb

                # Actor loss (clipped surrogate objective)
                actor_loss = -torch.min(surr1, surr2).mean()

                # Critic loss
                critic_loss = nn.MSELoss()(new_values, returns_mb)

                # Total loss
                total_loss = actor_loss + 0.5 * critic_loss

                # Update
                self.optimizer.zero_grad()
                total_loss.backward()
                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)
                self.optimizer.step()

        # Update old policy
        self.old_actor_critic.load_state_dict(self.actor_critic.state_dict())
        self.update_count += 1

# Multi-agent environment wrapper
class MultiAgentIsaacEnv:
    def __init__(self, isaac_env, num_agents):
        self.isaac_env = isaac_env
        self.num_agents = num_agents

    def reset(self):
        """Reset environment and return observations for all agents"""
        obs = self.isaac_env.reset()
        # Reshape to [num_agents, obs_dim] if needed
        return obs

    def step(self, actions):
        """Take step with actions from all agents"""
        obs, reward, done, info = self.isaac_env.step(actions)
        return obs, reward, done, info

    def get_obs_size(self):
        """Get observation size for a single agent"""
        return self.isaac_env.get_obs_size()

    def get_action_size(self):
        """Get action size for a single agent"""
        return self.isaac_env.get_action_size()

def train_mappo_agents(multi_env, total_timesteps=1000000):
    """
    Train multi-agent PPO in Isaac Sim environment
    """
    # Get environment dimensions
    state_dim = multi_env.get_obs_size()
    action_dim = multi_env.get_action_size()
    num_agents = multi_env.num_agents

    # Initialize agent
    agent = MAPPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        num_agents=num_agents,
        lr=3e-4,
        gamma=0.99,
        clip_epsilon=0.2,
        epochs=10,
        minibatch_size=64
    )

    # Initialize buffer
    buffer_size = 2048
    obs_buffer = torch.zeros((buffer_size, num_agents, state_dim))
    actions_buffer = torch.zeros((buffer_size, num_agents))
    rewards_buffer = torch.zeros((buffer_size, num_agents))
    values_buffer = torch.zeros((buffer_size, num_agents))
    log_probs_buffer = torch.zeros((buffer_size, num_agents))
    returns_buffer = torch.zeros((buffer_size, num_agents))
    advantages_buffer = torch.zeros((buffer_size, num_agents))

    # Training variables
    obs = multi_env.reset()
    episode_reward = 0
    episode_count = 0
    step = 0

    for t in range(total_timesteps):
        # Get actions from policy
        with torch.no_grad():
            actions, log_probs, values = agent.actor_critic(
                torch.FloatTensor(obs).unsqueeze(0)
            )
            actions = actions.squeeze(0)
            log_probs = log_probs.squeeze(0)
            values = values.squeeze(0)

        # Store in buffer
        obs_buffer[step] = torch.FloatTensor(obs)
        actions_buffer[step] = actions
        values_buffer[step] = values
        log_probs_buffer[step] = log_probs

        # Take action in environment
        next_obs, rewards, dones, infos = multi_env.step(actions.numpy())

        rewards_buffer[step] = torch.FloatTensor(rewards)

        # Update counters
        episode_reward += rewards.mean()  # Average reward across agents
        obs = next_obs
        step += 1

        # Check if episode is done or buffer is full
        if dones.any() or step == buffer_size:
            # Calculate returns and advantages
            with torch.no_grad():
                _, _, last_values = agent.actor_critic(
                    torch.FloatTensor(obs).unsqueeze(0)
                )
                last_values = last_values.squeeze(0)

            # Calculate advantages and returns
            rewards_plus = torch.cat([rewards_buffer[:step], last_values.unsqueeze(0)])
            values_plus = torch.cat([values_buffer[:step], last_values.unsqueeze(0)])

            deltas = rewards_plus[:-1] + 0.99 * values_plus[1:] - values_plus[:-1]

            advantages = torch.zeros_like(deltas[:-1])
            gae = 0
            for i in reversed(range(deltas[:-1].size(0))):
                gae = deltas[i] + 0.99 * 0.95 * gae
                advantages[i] = gae

            returns = advantages + values_buffer[:step]

            # Update agent
            agent.update(
                obs_buffer[:step],
                actions_buffer[:step],
                returns,
                advantages,
                log_probs_buffer[:step]
            )

            # Reset buffer
            step = 0

            # Log episode
            if dones.any():
                print(f"Episode {episode_count}: Avg Reward = {episode_reward:.2f}")
                episode_reward = 0
                episode_count += 1
                obs = multi_env.reset()

    return agent
```

## Curriculum Learning

Curriculum learning progressively increases task difficulty to improve learning efficiency:

```python
class CurriculumManager:
    def __init__(self, stages, threshold=0.8):
        """
        Manage curriculum learning stages
        stages: List of dictionaries with environment parameters for each stage
        threshold: Performance threshold to advance to next stage
        """
        self.stages = stages
        self.threshold = threshold
        self.current_stage = 0
        self.performance_history = []
        self.stage_episodes = 0
        self.min_episodes_per_stage = 10

    def evaluate_performance(self, episode_rewards):
        """Evaluate if we should advance to next stage"""
        if len(episode_rewards) < self.min_episodes_per_stage:
            return False

        # Calculate recent performance
        recent_rewards = episode_rewards[-self.min_episodes_per_stage:]
        avg_performance = np.mean(recent_rewards)

        # Check if performance exceeds threshold
        if avg_performance >= self.threshold and self.stage_episodes >= self.min_episodes_per_stage:
            return True

        return False

    def update_stage(self, episode_reward):
        """Update curriculum stage based on performance"""
        self.performance_history.append(episode_reward)
        self.stage_episodes += 1

        if self.current_stage < len(self.stages) - 1:
            if self.evaluate_performance(self.performance_history):
                self.current_stage += 1
                self.stage_episodes = 0
                print(f"Advancing to stage {self.current_stage + 1}")
                return True  # Stage changed

        return False  # No stage change

    def get_current_stage_params(self):
        """Get parameters for current stage"""
        return self.stages[self.current_stage]

class CurriculumTrainer:
    def __init__(self, agent, env, curriculum_stages):
        self.agent = agent
        self.env = env
        self.curriculum = CurriculumManager(curriculum_stages)

        # Stage-specific parameters
        self.stage_params = {}

    def train_with_curriculum(self, total_timesteps=1000000):
        """Train with curriculum learning"""
        state = self.env.reset()
        episode_reward = 0
        episode_count = 0

        for t in range(total_timesteps):
            # Select action
            action = self.agent.select_action(state)

            # Take action in environment
            next_state, reward, done, info = self.env.step(action)

            # Store transition (implementation depends on agent type)
            self.agent.store_transition(state, action, reward, next_state, done)

            # Update agent
            self.agent.train()

            # Update counters
            episode_reward += reward
            state = next_state

            # Check if episode is done
            if done:
                # Update curriculum
                stage_changed = self.curriculum.update_stage(episode_reward)

                if stage_changed:
                    # Update environment with new stage parameters
                    stage_params = self.curriculum.get_current_stage_params()
                    self.env.update_params(stage_params)

                print(f"Episode {episode_count}: Reward = {episode_reward:.2f}, Stage = {self.curriculum.current_stage + 1}")
                episode_reward = 0
                episode_count += 1
                state = self.env.reset()

# Example curriculum stages for a locomotion task
locomotion_curriculum = [
    # Stage 1: Simple flat terrain
    {
        'terrain_complexity': 0.1,
        'obstacle_density': 0.0,
        'reward_scaling': 1.0
    },
    # Stage 2: Slightly uneven terrain
    {
        'terrain_complexity': 0.3,
        'obstacle_density': 0.05,
        'reward_scaling': 1.2
    },
    # Stage 3: Moderate obstacles
    {
        'terrain_complexity': 0.5,
        'obstacle_density': 0.1,
        'reward_scaling': 1.5
    },
    # Stage 4: Challenging terrain
    {
        'terrain_complexity': 0.8,
        'obstacle_density': 0.2,
        'reward_scaling': 2.0
    }
]
```

## Domain Randomization

Domain randomization helps improve sim-to-real transfer by varying environment parameters:

```python
class DomainRandomization:
    def __init__(self, randomization_params):
        """
        randomization_params: Dictionary with parameter ranges for randomization
        """
        self.params = randomization_params
        self.current_values = {}
        self.update_randomization_values()

    def update_randomization_values(self):
        """Update randomization values for current episode"""
        for param_name, param_range in self.params.items():
            if isinstance(param_range, dict):
                # Handle different randomization strategies
                if param_range['distribution'] == 'uniform':
                    self.current_values[param_name] = np.random.uniform(
                        param_range['range'][0], param_range['range'][1]
                    )
                elif param_range['distribution'] == 'gaussian':
                    self.current_values[param_name] = np.random.normal(
                        param_range['mean'], param_range['std']
                    )
            else:
                # Simple range specification
                self.current_values[param_name] = np.random.uniform(
                    param_range[0], param_range[1]
                )

    def apply_to_environment(self, env):
        """Apply randomization values to environment"""
        for param_name, value in self.current_values.items():
            setattr(env, param_name, value)

# Example domain randomization parameters
domain_rand_params = {
    'robot_mass_range': {'distribution': 'uniform', 'range': [0.8, 1.2]},  # 80% to 120% of nominal
    'friction_range': {'distribution': 'uniform', 'range': [0.5, 1.5]},
    'gravity_range': {'distribution': 'gaussian', 'mean': -9.81, 'std': 0.5},
    'actuator_strength_range': {'distribution': 'uniform', 'range': [0.9, 1.1]},
    'sensor_noise_std': {'distribution': 'uniform', 'range': [0.0, 0.05]},
    'delay_range': {'distribution': 'uniform', 'range': [0.0, 0.02]}  # 0-20ms delay
}

class DRTrainer:
    def __init__(self, agent, env, randomization_params):
        self.agent = agent
        self.env = env
        self.domain_rand = DomainRandomization(randomization_params)

    def train_with_domain_rand(self, total_timesteps=1000000):
        """Train with domain randomization"""
        episode_count = 0

        for t in range(total_timesteps):
            # Update domain randomization for new episode if needed
            if t % 1000 == 0:  # Update every 1000 timesteps
                self.domain_rand.update_randomization_values()
                self.domain_rand.apply_to_environment(self.env)

            # Reset environment with current randomization values
            if t % 1000 == 0:  # Reset every 1000 timesteps or when episode ends
                state = self.env.reset()
                episode_count += 1

            # Select action
            action = self.agent.select_action(state)

            # Take action in environment
            next_state, reward, done, info = self.env.step(action)

            # Store transition and update agent
            self.agent.store_transition(state, action, reward, next_state, done)
            self.agent.train()

            state = next_state

            if done:
                state = self.env.reset()
                episode_count += 1

# Example usage
def train_with_advanced_techniques(isaac_env):
    """
    Train with advanced techniques: curriculum learning and domain randomization
    """
    # Get environment dimensions
    state_dim = isaac_env.get_obs_size()
    action_dim = isaac_env.get_action_size()
    max_action = 1.0

    # Initialize agent (using SAC as example)
    agent = SACAgent(state_dim, action_dim, max_action)

    # Initialize domain randomization
    dr_trainer = DRTrainer(agent, isaac_env, domain_rand_params)

    # Initialize curriculum learning
    curriculum_trainer = CurriculumTrainer(agent, isaac_env, locomotion_curriculum)

    print("Starting training with advanced techniques...")
    print("- Domain randomization enabled")
    print("- Curriculum learning enabled")

    # Training with both techniques
    dr_trainer.train_with_domain_rand(total_timesteps=500000)
    curriculum_trainer.train_with_curriculum(total_timesteps=500000)

    return agent
```

## Training Monitoring and Evaluation

Effective monitoring is crucial for successful training:

```python
import matplotlib.pyplot as plt
from collections import deque
import json
import os

class TrainingMonitor:
    def __init__(self, save_dir="training_logs"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.losses = deque(maxlen=100)
        self.entropies = deque(maxlen=100)
        self.learning_rates = deque(maxlen=100)

        self.timesteps = []
        self.episode_count = 0

    def log_episode(self, reward, length, loss=None, entropy=None, lr=None):
        """Log episode statistics"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)

        if loss is not None:
            self.losses.append(loss)
        if entropy is not None:
            self.entropies.append(entropy)
        if lr is not None:
            self.learning_rates.append(lr)

        self.episode_count += 1

    def get_metrics(self):
        """Get current training metrics"""
        metrics = {
            'avg_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'avg_length': np.mean(self.episode_lengths) if self.episode_lengths else 0,
            'recent_rewards': list(self.episode_rewards),
            'episode_count': self.episode_count
        }

        if self.losses:
            metrics['avg_loss'] = np.mean(self.losses)
        if self.entropies:
            metrics['avg_entropy'] = np.mean(self.entropies)
        if self.learning_rates:
            metrics['current_lr'] = self.learning_rates[-1]

        return metrics

    def plot_training_progress(self):
        """Plot training progress"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Plot 1: Episode rewards
        axes[0, 0].plot(list(self.episode_rewards), alpha=0.3, label='Raw Rewards')
        if len(self.episode_rewards) >= 10:
            moving_avg = [np.mean(list(self.episode_rewards)[max(0, i-10):i+1])
                         for i in range(len(self.episode_rewards))]
            axes[0, 0].plot(moving_avg, label='Moving Average (10 eps)', linewidth=2)
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # Plot 2: Episode lengths
        axes[0, 1].plot(list(self.episode_lengths), alpha=0.3, label='Episode Lengths')
        if len(self.episode_lengths) >= 10:
            moving_avg_len = [np.mean(list(self.episode_lengths)[max(0, i-10):i+1])
                             for i in range(len(self.episode_lengths))]
            axes[0, 1].plot(moving_avg_len, label='Moving Average (10 eps)', linewidth=2)
        axes[0, 1].set_title('Episode Lengths')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Length')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # Plot 3: Losses (if available)
        if self.losses:
            axes[1, 0].plot(list(self.losses), alpha=0.3, label='Loss')
            if len(self.losses) >= 10:
                moving_avg_loss = [np.mean(list(self.losses)[max(0, i-10):i+1])
                                  for i in range(len(self.losses))]
                axes[1, 0].plot(moving_avg_loss, label='Moving Average (10 steps)', linewidth=2)
            axes[1, 0].set_title('Training Loss')
            axes[1, 0].set_xlabel('Update Step')
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # Plot 4: Entropy (if available)
        if self.entropies:
            axes[1, 1].plot(list(self.entropies), alpha=0.3, label='Entropy')
            if len(self.entropies) >= 10:
                moving_avg_entropy = [np.mean(list(self.entropies)[max(0, i-10):i+1])
                                     for i in range(len(self.entropies))]
                axes[1, 1].plot(moving_avg_entropy, label='Moving Average (10 steps)', linewidth=2)
            axes[1, 1].set_title('Policy Entropy')
            axes[1, 1].set_xlabel('Update Step')
            axes[1, 1].set_ylabel('Entropy')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, 'training_progress.png'))
        plt.show()

    def save_checkpoint(self, agent, episode):
        """Save model checkpoint"""
        checkpoint_path = os.path.join(self.save_dir, f'checkpoint_{episode}.pth')
        torch.save({
            'episode': episode,
            'model_state_dict': agent.state_dict(),
            'metrics': self.get_metrics()
        }, checkpoint_path)

    def save_training_config(self, config):
        """Save training configuration"""
        config_path = os.path.join(self.save_dir, 'training_config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)

# Complete training pipeline example
def complete_training_pipeline(isaac_env, algorithm='sac', total_timesteps=1000000):
    """
    Complete training pipeline with monitoring
    """
    # Initialize agent based on algorithm
    state_dim = isaac_env.get_obs_size()
    action_dim = isaac_env.get_action_size()
    max_action = 1.0

    if algorithm.lower() == 'ppo':
        agent = PPOAgent(state_dim, action_dim, action_type='continuous')
    elif algorithm.lower() == 'sac':
        agent = SACAgent(state_dim, action_dim, max_action)
    else:
        raise ValueError(f"Unsupported algorithm: {algorithm}")

    # Initialize training monitor
    monitor = TrainingMonitor()

    # Training configuration
    config = {
        'algorithm': algorithm,
        'state_dim': state_dim,
        'action_dim': action_dim,
        'total_timesteps': total_timesteps,
        'max_action': max_action
    }
    monitor.save_training_config(config)

    # Training loop
    state = isaac_env.reset()
    episode_reward = 0
    episode_length = 0
    episode_count = 0

    for t in range(total_timesteps):
        # Select action
        if algorithm.lower() == 'ppo':
            with torch.no_grad():
                action, log_prob, value = agent.actor_critic.get_action(torch.FloatTensor(state))
                action = action.cpu().numpy()
        else:  # SAC
            action = agent.select_action(state)

        # Take action in environment
        next_state, reward, done, info = isaac_env.step(action)

        # Store transition based on algorithm
        if algorithm.lower() == 'sac':
            agent.store_transition(state, action, reward, next_state, done)

        # Update agent
        agent.train()

        # Update counters
        episode_reward += reward
        episode_length += 1
        state = next_state

        # Check if episode is done
        if done or t == total_timesteps - 1:
            # Log episode
            monitor.log_episode(
                reward=episode_reward,
                length=episode_length,
                loss=getattr(agent, 'last_loss', None),
                entropy=getattr(agent, 'last_entropy', None)
            )

            # Print progress
            if episode_count % 10 == 0:
                metrics = monitor.get_metrics()
                print(f"Episode {episode_count}: Avg Reward = {metrics['avg_reward']:.2f}, "
                      f"Avg Length = {metrics['avg_length']:.1f}")

            # Save checkpoint periodically
            if episode_count % 100 == 0:
                monitor.save_checkpoint(agent, episode_count)

            # Reset for next episode
            episode_reward = 0
            episode_length = 0
            episode_count += 1
            state = isaac_env.reset()

    # Plot final training progress
    monitor.plot_training_progress()

    return agent, monitor

if __name__ == "__main__":
    # Example usage would go here
    # Note: Actual Isaac Sim environment would need to be properly initialized
    print("Training agents in Isaac Sim - complete implementation")
