---
id: module4-3d-simulation-1.3-unity-integration
title: "Unity Integration"
slug: /module4-3d-simulation-1.3-unity-integration
---

# 4.3 Unity Integration

## Overview

Unity has emerged as a powerful platform for robotics simulation, particularly with the introduction of specialized tools like the Unity Robotics Hub and NVIDIA Isaac Sim. Unlike traditional physics-based simulators, Unity offers photorealistic rendering capabilities, extensive asset libraries, and robust development tools that make it ideal for perception-heavy robotics applications and human-robot interaction scenarios.

## Unity Robotics Ecosystem

### Unity Robotics Hub

The Unity Robotics Hub serves as the central integration point between Unity and ROS/ROS2 systems:

- **ROS TCP Connector**: Enables communication between Unity and ROS/ROS2 networks
- **Robotics Library**: Pre-built components for common robotics patterns
- **Sample Environments**: Ready-to-use simulation environments
- **Documentation and Tutorials**: Comprehensive learning resources

### NVIDIA Isaac Sim

NVIDIA Isaac Sim represents the state-of-the-art in Unity-based robotics simulation:

- **GPU-Accelerated Physics**: PhysX engine optimized for robotics applications
- **Synthetic Data Generation**: High-quality training data for AI models
- **Photorealistic Rendering**: RTX-accelerated rendering for perception tasks
- **Large-Scale Environments**: Support for complex, multi-building scenarios

## Installation and Setup

### Unity Hub and Editor

```bash
# Download Unity Hub from Unity's website
# Install Unity Editor (recommended version 2021.3 LTS or newer)

# Install required modules:
# - Physics (PhysX)
# - XR (if needed for VR applications)
# - 2D/3D packages as needed
```

### Unity Robotics Package Installation

```csharp
// In Unity Package Manager, add the following packages:
// 1. Unity Robotics Package
// 2. Unity Simulation Package
// 3. Unity Perception Package

// Or via manifest.json:
{
  "dependencies": {
    "com.unity.robotics.ros-tcp-connector": "0.7.0",
    "com.unity.robotics.urdf-importer": "0.5.2",
    "com.unity.perception": "1.2.0"
  }
}
```

## ROS/ROS2 Integration

### ROS TCP Connector

The ROS TCP Connector enables real-time communication between Unity and ROS systems:

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;
using RosMessageTypes.Geometry;

public class RobotController : MonoBehaviour
{
    ROSConnection ros;
    string rosIP = "127.0.0.1"; // Default IP
    int rosPort = 10000; // Default port

    // Robot joint states
    float[] jointPositions = new float[6];
    float[] jointVelocities = new float[6];

    void Start()
    {
        // Get the ROS connection static instance
        ros = ROSConnection.GetOrCreateInstance();
        ros.RegisterPublisher<JointStateMsg>("joint_states");

        // Subscribe to ROS topics
        ros.Subscribe<JointStateMsg>("joint_commands", OnJointCommandsReceived);
    }

    void OnJointCommandsReceived(JointStateMsg msg)
    {
        // Process joint commands from ROS
        for (int i = 0; i < Mathf.Min(msg.position.Length, jointPositions.Length); i++)
        {
            jointPositions[i] = (float)msg.position[i];
        }
    }

    void Update()
    {
        // Publish joint states to ROS
        var jointState = new JointStateMsg();
        jointState.name = new string[] { "joint1", "joint2", "joint3", "joint4", "joint5", "joint6" };
        jointState.position = new double[] {
            jointPositions[0], jointPositions[1], jointPositions[2],
            jointPositions[3], jointPositions[4], jointPositions[5]
        };
        jointState.velocity = new double[] {
            jointVelocities[0], jointVelocities[1], jointVelocities[2],
            jointVelocities[3], jointVelocities[4], jointVelocities[5]
        };
        jointState.effort = new double[6]; // Effort values

        ros.Publish("joint_states", jointState);
    }
}
```

### URDF Importer Integration

Unity's URDF Importer allows direct import of ROS robot models:

```csharp
using Unity.Robotics.UrdfImporter;
using UnityEngine;

public class RobotSetup : MonoBehaviour
{
    [Header("URDF Import Settings")]
    public string urdfPath;
    public JointType jointType = JointType.revolute;
    public float jointDamping = 0.1f;
    public float jointFriction = 0.1f;

    public void ImportRobotFromURDF()
    {
        // Import URDF file and create Unity robot model
        var robot = UrdfRobotExtensions.CreateRobotFromUrdf(urdfPath, this.transform);

        // Configure joint properties
        ConfigureJoints(robot);
    }

    void ConfigureJoints(UrdfRobot robot)
    {
        foreach (var joint in robot.joints)
        {
            var jointComponent = joint.GetComponent<Unity.Robotics.UrdfImporter.Joint>();
            if (jointComponent != null)
            {
                jointComponent.jointDamping = jointDamping;
                jointComponent.jointFriction = jointFriction;
            }
        }
    }
}
```

## Physics Simulation in Unity

### PhysX Configuration for Robotics

```csharp
using UnityEngine;

public class PhysicsConfiguration : MonoBehaviour
{
    [Header("Physics Settings")]
    public float fixedTimestep = 0.01f; // 100 Hz physics update
    public float maxAngularVelocity = 50f;
    public float sleepThreshold = 0.005f;
    public int solverIterations = 8;
    public int solverVelocityIterations = 8;

    void Start()
    {
        // Configure Unity physics for robotics applications
        Time.fixedDeltaTime = fixedTimestep;
        Physics.defaultMaxAngularSpeed = maxAngularVelocity;
        Physics.sleepThreshold = sleepThreshold;
        Physics.defaultSolverIterations = solverIterations;
        Physics.defaultSolverVelocityIterations = solverVelocityIterations;

        // Enable continuous collision detection for fast-moving parts
        Physics.bounceThreshold = 2.0f;
        Physics.defaultContactOffset = 0.01f;
    }
}
```

### Custom Physics Materials

```csharp
using UnityEngine;

[CreateAssetMenu(fileName = "PhysicsMaterial", menuName = "Robotics/Physics Material")]
public class RobotPhysicsMaterial : ScriptableObject
{
    [Header("Friction Properties")]
    public float staticFriction = 0.5f;
    public float dynamicFriction = 0.3f;
    public PhysicMaterialCombine frictionCombine = PhysicMaterialCombine.Average;

    [Header("Bounce Properties")]
    public float bounce = 0.1f;
    public PhysicMaterialCombine bounceCombine = PhysicMaterialCombine.Average;

    [Header("Surface Type")]
    public string surfaceType = "default";
    public float gripFactor = 1.0f; // For locomotion simulation

    public PhysicMaterial CreatePhysicMaterial()
    {
        PhysicMaterial material = new PhysicMaterial();
        material.staticFriction = staticFriction;
        material.dynamicFriction = dynamicFriction;
        material.frictionCombine = frictionCombine;
        material.bounciness = bounce;
        material.bounceCombine = bounceCombine;

        return material;
    }
}
```

## Sensor Simulation

### Camera Sensor Implementation

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;
using System.Collections;

public class CameraSensor : MonoBehaviour
{
    [Header("Camera Settings")]
    public int imageWidth = 640;
    public int imageHeight = 480;
    public float fieldOfView = 60f;
    public string topicName = "camera/rgb/image_raw";

    private Camera cam;
    private RenderTexture renderTexture;
    private Texture2D texture2D;
    private ROSConnection ros;
    private float publishRate = 30f; // Hz

    void Start()
    {
        cam = GetComponent<Camera>();
        ros = ROSConnection.GetOrCreateInstance();

        SetupCamera();
        StartCoroutine(PublishImages());
    }

    void SetupCamera()
    {
        cam.fieldOfView = fieldOfView;

        // Create render texture for camera output
        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);
        cam.targetTexture = renderTexture;

        // Create 2D texture for image conversion
        texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);
    }

    IEnumerator PublishImages()
    {
        while (true)
        {
            yield return new WaitForSeconds(1f / publishRate);
            PublishCameraImage();
        }
    }

    void PublishCameraImage()
    {
        // Copy render texture to 2D texture
        RenderTexture.active = renderTexture;
        texture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);
        texture2D.Apply();

        // Convert to ROS message format
        byte[] imageData = texture2D.EncodeToPNG();

        // Create and publish ROS image message
        var imageMsg = new ImageMsg();
        imageMsg.header = new std_msgs.Header();
        imageMsg.header.stamp = new builtin_interfaces.Time(ROSConnection.GetTime());
        imageMsg.header.frame_id = this.name;
        imageMsg.height = (uint)imageHeight;
        imageMsg.width = (uint)imageWidth;
        imageMsg.encoding = "rgb8";
        imageMsg.is_bigendian = 0;
        imageMsg.step = (uint)(imageWidth * 3); // 3 bytes per pixel (RGB)
        imageMsg.data = imageData;

        ros.Publish(topicName, imageMsg);
    }
}
```

### LIDAR Sensor Simulation

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;
using System.Collections.Generic;

public class LidarSensor : MonoBehaviour
{
    [Header("LIDAR Configuration")]
    public int rayCount = 720;
    public float minAngle = -Mathf.PI / 2;
    public float maxAngle = Mathf.PI / 2;
    public float maxRange = 30.0f;
    public float updateRate = 10.0f;
    public string topicName = "scan";

    private ROSConnection ros;
    private RaycastHit[] raycastHits;

    void Start()
    {
        ros = ROSConnection.GetOrCreateInstance();
        raycastHits = new RaycastHit[rayCount];

        InvokeRepeating("PublishLidarData", 0, 1.0f / updateRate);
    }

    void PublishLidarData()
    {
        List<float> ranges = new List<float>();
        List<float> intensities = new List<float>();

        float angleIncrement = (maxAngle - minAngle) / rayCount;

        for (int i = 0; i < rayCount; i++)
        {
            float angle = minAngle + i * angleIncrement;

            // Calculate ray direction in world space
            Vector3 rayDirection = new Vector3(
                Mathf.Cos(angle),
                0,
                Mathf.Sin(angle)
            );
            rayDirection = transform.TransformDirection(rayDirection);

            // Perform raycast
            if (Physics.Raycast(transform.position, rayDirection, out RaycastHit hit, maxRange))
            {
                ranges.Add(hit.distance);
                intensities.Add(1.0f); // Simulated intensity
            }
            else
            {
                ranges.Add(maxRange);
                intensities.Add(0.0f);
            }
        }

        // Create and publish ROS LaserScan message
        var scanMsg = new LaserScanMsg();
        scanMsg.header = new std_msgs.Header();
        scanMsg.header.stamp = new builtin_interfaces.Time(ROSConnection.GetTime());
        scanMsg.header.frame_id = this.name;
        scanMsg.angle_min = minAngle;
        scanMsg.angle_max = maxAngle;
        scanMsg.angle_increment = angleIncrement;
        scanMsg.time_increment = 0; // Not used in simulation
        scanMsg.scan_time = 1.0f / updateRate;
        scanMsg.range_min = 0.1f;
        scanMsg.range_max = maxRange;
        scanMsg.ranges = ranges.ConvertAll(x => (float)x).ToArray();
        scanMsg.intensities = intensities.ToArray();

        ros.Publish(topicName, scanMsg);
    }
}
```

### IMU Sensor Simulation

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;

public class IMUSensor : MonoBehaviour
{
    [Header("IMU Configuration")]
    public string topicName = "imu/data";
    public float updateRate = 100.0f;
    public Vector3 noiseLinearAcceleration = new Vector3(0.01f, 0.01f, 0.01f);
    public Vector3 noiseAngularVelocity = new Vector3(0.001f, 0.001f, 0.001f);

    private ROSConnection ros;
    private Rigidbody attachedRigidbody;

    void Start()
    {
        ros = ROSConnection.GetOrCreateInstance();
        attachedRigidbody = GetComponentInParent<Rigidbody>();

        InvokeRepeating("PublishIMUData", 0, 1.0f / updateRate);
    }

    void PublishIMUData()
    {
        // Get linear acceleration from rigidbody
        Vector3 linearAcc = attachedRigidbody ? attachedRigidbody.velocity / Time.fixedDeltaTime : Vector3.zero;

        // Get angular velocity from rigidbody
        Vector3 angularVel = attachedRigidbody ? attachedRigidbody.angularVelocity : Vector3.zero;

        // Add noise to simulate real sensor behavior
        linearAcc += AddNoise(noiseLinearAcceleration);
        angularVel += AddNoise(noiseAngularVelocity);

        // Create and publish ROS IMU message
        var imuMsg = new ImuMsg();
        imuMsg.header = new std_msgs.Header();
        imuMsg.header.stamp = new builtin_interfaces.Time(ROSConnection.GetTime());
        imuMsg.header.frame_id = this.name;

        // Convert Unity coordinates to ROS coordinates (if needed)
        imuMsg.linear_acceleration.x = linearAcc.x;
        imuMsg.linear_acceleration.y = linearAcc.z;  // Unity Y -> ROS Z
        imuMsg.linear_acceleration.z = -linearAcc.y; // Unity Z -> ROS -Y

        imuMsg.angular_velocity.x = angularVel.x;
        imuMsg.angular_velocity.y = angularVel.z;
        imuMsg.angular_velocity.z = -angularVel.y;

        // Orientation (if available from parent transform)
        Quaternion orientation = transform.rotation;
        imuMsg.orientation.x = orientation.x;
        imuMsg.orientation.y = orientation.z;
        imuMsg.orientation.z = -orientation.y;
        imuMsg.orientation.w = orientation.w;

        // Set covariance matrices (diagonal values)
        for (int i = 0; i < 9; i++)
        {
            imuMsg.linear_acceleration_covariance[i] = i % 4 == 0 ? 0.01f : 0.0f; // diagonal
            imuMsg.angular_velocity_covariance[i] = i % 4 == 0 ? 0.01f : 0.0f; // diagonal
            imuMsg.orientation_covariance[i] = i % 4 == 0 ? 0.01f : 0.0f; // diagonal
        }

        ros.Publish(topicName, imuMsg);
    }

    Vector3 AddNoise(Vector3 noiseLevel)
    {
        return new Vector3(
            Random.Range(-noiseLevel.x, noiseLevel.x),
            Random.Range(-noiseLevel.y, noiseLevel.y),
            Random.Range(-noiseLevel.z, noiseLevel.z)
        );
    }
}
```

## Environment Design and Scene Management

### Procedural Environment Generation

```csharp
using UnityEngine;
using System.Collections.Generic;

public class ProceduralEnvironment : MonoBehaviour
{
    [Header("Environment Parameters")]
    public int gridSize = 10;
    public float cellSize = 5f;
    public GameObject[] obstaclePrefabs;
    public GameObject floorPrefab;
    public GameObject wallPrefab;

    [Header("Generation Settings")]
    public float obstacleDensity = 0.3f;
    public float minObstacleSize = 0.5f;
    public float maxObstacleSize = 2.0f;

    private List<GameObject> spawnedObjects = new List<GameObject>();

    public void GenerateEnvironment()
    {
        ClearEnvironment();

        // Generate floor
        GenerateFloor();

        // Generate walls
        GenerateBoundaryWalls();

        // Generate obstacles
        GenerateObstacles();
    }

    void GenerateFloor()
    {
        Vector3 floorSize = new Vector3(gridSize * cellSize, 0.1f, gridSize * cellSize);
        GameObject floor = Instantiate(floorPrefab, Vector3.zero, Quaternion.identity);
        floor.transform.localScale = new Vector3(floorSize.x, floorSize.y, floorSize.z);
        spawnedObjects.Add(floor);
    }

    void GenerateBoundaryWalls()
    {
        float halfGrid = gridSize * cellSize / 2f;

        // Create 4 walls around the boundary
        for (int i = 0; i < 4; i++)
        {
            GameObject wall = Instantiate(wallPrefab);
            Vector3 position = Vector3.zero;
            Vector3 scale = new Vector3(gridSize * cellSize, 2.5f, 0.2f);

            switch (i)
            {
                case 0: // North wall
                    position = new Vector3(0, 1.25f, halfGrid);
                    break;
                case 1: // South wall
                    position = new Vector3(0, 1.25f, -halfGrid);
                    break;
                case 2: // East wall
                    position = new Vector3(halfGrid, 1.25f, 0);
                    scale = new Vector3(0.2f, 2.5f, gridSize * cellSize);
                    break;
                case 3: // West wall
                    position = new Vector3(-halfGrid, 1.25f, 0);
                    scale = new Vector3(0.2f, 2.5f, gridSize * cellSize);
                    break;
            }

            wall.transform.position = position;
            wall.transform.localScale = scale;
            spawnedObjects.Add(wall);
        }
    }

    void GenerateObstacles()
    {
        int obstacleCount = Mathf.RoundToInt(gridSize * gridSize * obstacleDensity);

        for (int i = 0; i < obstacleCount; i++)
        {
            // Random position within grid
            float x = Random.Range(-gridSize * cellSize / 2f, gridSize * cellSize / 2f);
            float z = Random.Range(-gridSize * cellSize / 2f, gridSize * cellSize / 2f);
            Vector3 position = new Vector3(x, 0.5f, z);

            // Random obstacle selection
            GameObject obstaclePrefab = obstaclePrefabs[Random.Range(0, obstaclePrefabs.Length)];
            GameObject obstacle = Instantiate(obstaclePrefab, position, Quaternion.identity);

            // Random scaling
            float scale = Random.Range(minObstacleSize, maxObstacleSize);
            obstacle.transform.localScale = Vector3.one * scale;

            spawnedObjects.Add(obstacle);
        }
    }

    void ClearEnvironment()
    {
        foreach (GameObject obj in spawnedObjects)
        {
            if (obj != null)
                DestroyImmediate(obj);
        }
        spawnedObjects.Clear();
    }
}
```

### Lighting and Atmospheric Simulation

```csharp
using UnityEngine;

public class LightingSimulator : MonoBehaviour
{
    [Header("Lighting Configuration")]
    public Light sunLight;
    public AnimationCurve dayNightCycle;
    public Gradient skyColorGradient;
    public float cycleDuration = 120f; // Real-time seconds for full cycle

    [Header("Weather Simulation")]
    public bool enableWeather = false;
    public float cloudCover = 0.0f;
    public float precipitation = 0.0f;
    public float windSpeed = 0.0f;

    private float cycleTime = 0f;

    void Start()
    {
        if (sunLight == null)
            sunLight = FindObjectOfType<Light>(); // Find directional light
    }

    void Update()
    {
        UpdateDayNightCycle();
        UpdateWeatherEffects();
    }

    void UpdateDayNightCycle()
    {
        cycleTime += Time.deltaTime;
        float normalizedTime = (cycleTime % cycleDuration) / cycleDuration;
        float intensity = dayNightCycle.Evaluate(normalizedTime);

        // Update sun light
        sunLight.intensity = intensity;

        // Update sky color based on time of day
        RenderSettings.skybox.SetColor("_Tint", skyColorGradient.Evaluate(normalizedTime));

        // Rotate sun to simulate day progression
        float rotation = normalizedTime * 360f;
        sunLight.transform.rotation = Quaternion.Euler(rotation, 180f, 0f);
    }

    void UpdateWeatherEffects()
    {
        if (!enableWeather) return;

        // Update cloud cover (simplified)
        RenderSettings.skybox.SetFloat("_Clouds", cloudCover);

        // Update precipitation effects would require particle systems
        // Update wind effects would affect cloth simulation, etc.
    }
}
```

## Humanoid Robot Simulation

### Inverse Kinematics for Humanoid Models

```csharp
using UnityEngine;

public class HumanoidIKController : MonoBehaviour
{
    [Header("Humanoid IK Targets")]
    public Transform leftHandTarget;
    public Transform rightHandTarget;
    public Transform leftFootTarget;
    public Transform rightFootTarget;

    [Header("IK Settings")]
    public float handIKWeight = 1.0f;
    public float footIKWeight = 1.0f;
    public float bodyPositionWeight = 0.5f;
    public float bodyRotationWeight = 0.5f;

    private Animator animator;

    void Start()
    {
        animator = GetComponent<Animator>();
    }

    void OnAnimatorIK(int layerIndex)
    {
        // Hand IK
        if (leftHandTarget != null)
        {
            animator.SetIKPositionWeight(AvatarIKGoal.LeftHand, handIKWeight);
            animator.SetIKPosition(AvatarIKGoal.LeftHand, leftHandTarget.position);
            animator.SetIKRotationWeight(AvatarIKGoal.LeftHand, handIKWeight);
            animator.SetIKRotation(AvatarIKGoal.LeftHand, leftHandTarget.rotation);
        }

        if (rightHandTarget != null)
        {
            animator.SetIKPositionWeight(AvatarIKGoal.RightHand, handIKWeight);
            animator.SetIKPosition(AvatarIKGoal.RightHand, rightHandTarget.position);
            animator.SetIKRotationWeight(AvatarIKGoal.RightHand, handIKWeight);
            animator.SetIKRotation(AvatarIKGoal.RightHand, rightHandTarget.rotation);
        }

        // Foot IK
        if (leftFootTarget != null)
        {
            animator.SetIKPositionWeight(AvatarIKGoal.LeftFoot, footIKWeight);
            animator.SetIKPosition(AvatarIKGoal.LeftFoot, leftFootTarget.position);
            animator.SetIKRotationWeight(AvatarIKGoal.LeftFoot, footIKWeight);
            animator.SetIKRotation(AvatarIKGoal.LeftFoot, leftFootTarget.rotation);
        }

        if (rightFootTarget != null)
        {
            animator.SetIKPositionWeight(AvatarIKGoal.RightFoot, footIKWeight);
            animator.SetIKPosition(AvatarIKGoal.RightFoot, rightFootTarget.position);
            animator.SetIKRotationWeight(AvatarIKGoal.RightFoot, footIKWeight);
            animator.SetIKRotation(AvatarIKGoal.RightFoot, rightFootTarget.rotation);
        }
    }

    void OnAnimatorMove()
    {
        // Apply body positioning and rotation
        if (animator) {
            Vector3 bodyPosition = animator.bodyPosition;
            Quaternion bodyRotation = animator.bodyRotation;

            // Apply to transform if needed
            // This is where you might want to update the overall position/rotation
        }
    }
}
```

### Balance and Locomotion Simulation

```csharp
using UnityEngine;

public class BalanceController : MonoBehaviour
{
    [Header("Balance Parameters")]
    public Transform centerOfMass;
    public float balanceThreshold = 0.1f;
    public float recoverySpeed = 5.0f;
    public LayerMask groundLayer;

    [Header("Locomotion")]
    public float walkSpeed = 2.0f;
    public float runSpeed = 4.0f;
    public float turnSpeed = 180.0f;

    private Rigidbody rb;
    private Vector3 targetVelocity;
    private bool isGrounded;

    void Start()
    {
        rb = GetComponent<Rigidbody>();
        if (centerOfMass == null)
            centerOfMass = transform; // Fallback to transform
    }

    void Update()
    {
        CheckGrounded();
        HandleBalance();
    }

    void CheckGrounded()
    {
        // Raycast down to check if grounded
        Vector3 rayStart = centerOfMass.position;
        float rayDistance = 0.1f;

        isGrounded = Physics.Raycast(rayStart, Vector3.down, rayDistance, groundLayer);
    }

    void HandleBalance()
    {
        if (!isGrounded) return;

        // Calculate center of mass offset from support polygon
        Vector3 comOffset = centerOfMass.position - transform.position;
        comOffset.y = 0; // Only consider horizontal offset

        if (comOffset.magnitude > balanceThreshold)
        {
            // Apply corrective forces to maintain balance
            Vector3 correctiveForce = -comOffset.normalized * recoverySpeed * Time.deltaTime;
            rb.AddForceAtPosition(correctiveForce, centerOfMass.position, ForceMode.VelocityChange);
        }
    }

    public void Move(Vector3 direction, bool isRunning = false)
    {
        if (!isGrounded) return;

        float currentSpeed = isRunning ? runSpeed : walkSpeed;
        targetVelocity = transform.forward * direction.z * currentSpeed +
                        transform.right * direction.x * currentSpeed;

        // Apply movement force
        Vector3 velocityChange = (targetVelocity - rb.velocity);
        velocityChange.y = 0; // Don't affect vertical velocity

        rb.AddForce(velocityChange, ForceMode.VelocityChange);
    }

    public void Turn(float turnAmount)
    {
        float turnVelocity = turnAmount * turnSpeed * Mathf.Deg2Rad;
        rb.angularVelocity = new Vector3(0, turnVelocity, 0);
    }
}
```

## Performance Optimization

### Level of Detail (LOD) System for Robotics

```csharp
using UnityEngine;

public class RobotLODController : MonoBehaviour
{
    [System.Serializable]
    public class LODLevel
    {
        public string name;
        public float distance;
        public GameObject[] objects;
        public int triangleBudget;
    }

    public LODLevel[] lodLevels;
    public Transform viewer; // Camera or reference point
    public float updateInterval = 0.1f;

    private float lastUpdateTime = 0f;

    void Start()
    {
        if (viewer == null)
            viewer = Camera.main.transform;

        // Initially hide all LOD levels except the first one
        for (int i = 1; i < lodLevels.Length; i++)
        {
            SetLODActive(i, false);
        }
    }

    void Update()
    {
        if (Time.time - lastUpdateTime > updateInterval)
        {
            UpdateLOD();
            lastUpdateTime = Time.time;
        }
    }

    void UpdateLOD()
    {
        if (viewer == null) return;

        float distance = Vector3.Distance(transform.position, viewer.position);

        // Find the appropriate LOD level
        int activeLOD = 0;
        for (int i = 0; i < lodLevels.Length; i++)
        {
            if (distance <= lodLevels[i].distance)
            {
                activeLOD = i;
                break;
            }
        }

        // Activate the selected LOD and deactivate others
        for (int i = 0; i < lodLevels.Length; i++)
        {
            SetLODActive(i, i == activeLOD);
        }
    }

    void SetLODActive(int lodIndex, bool active)
    {
        if (lodIndex < 0 || lodIndex >= lodLevels.Length) return;

        foreach (GameObject obj in lodLevels[lodIndex].objects)
        {
            if (obj != null)
                obj.SetActive(active);
        }
    }
}
```

### Physics Optimization for Multi-Robot Simulation

```csharp
using UnityEngine;
using System.Collections.Generic;

public class PhysicsOptimizationManager : MonoBehaviour
{
    [Header("Simulation Optimization")]
    public int maxActiveRigidbodies = 50;
    public float sleepThreshold = 0.001f;
    public float sleepVelocityThreshold = 0.01f;

    private List<Rigidbody> activeRigidbodies = new List<Rigidbody>();
    private List<Rigidbody> sleepingRigidbodies = new List<Rigidbody>();

    void Start()
    {
        Physics.sleepThreshold = sleepThreshold;
        InitializePhysicsOptimization();
    }

    void InitializePhysicsOptimization()
    {
        Rigidbody[] allRigidbodies = FindObjectsOfType<Rigidbody>();

        foreach (Rigidbody rb in allRigidbodies)
        {
            if (rb.isKinematic)
                continue; // Skip kinematic rigidbodies

            if (activeRigidbodies.Count < maxActiveRigidbodies)
            {
                activeRigidbodies.Add(rb);
                rb.Sleep(); // Initially sleep all
            }
            else
            {
                sleepingRigidbodies.Add(rb);
                rb.Sleep();
            }
        }
    }

    void Update()
    {
        // Wake up rigidbodies that are near active areas
        CheckProximityWakeups();

        // Put to sleep rigidbodies that are inactive
        CheckSleepConditions();
    }

    void CheckProximityWakeups()
    {
        // Get all active robots/controllers
        var activeControllers = FindObjectsOfType<RobotController>();

        foreach (var controller in activeControllers)
        {
            Vector3 controllerPos = controller.transform.position;

            // Wake up nearby rigidbodies
            foreach (var rb in sleepingRigidbodies)
            {
                if (Vector3.Distance(controllerPos, rb.position) < 10.0f) // 10m threshold
                {
                    rb.WakeUp();
                    sleepingRigidbodies.Remove(rb);
                    activeRigidbodies.Add(rb);

                    // Limit active rigidbodies
                    if (activeRigidbodies.Count > maxActiveRigidbodies)
                    {
                        // Put farthest one to sleep
                        PutFarthestToSleep(controllerPos);
                    }

                    break; // Move to next rigidbody after waking one
                }
            }
        }
    }

    void PutFarthestToSleep(Vector3 referencePos)
    {
        if (activeRigidbodies.Count == 0) return;

        Rigidbody farthestRb = activeRigidbodies[0];
        float maxDistance = Vector3.Distance(referencePos, farthestRb.position);

        for (int i = 1; i < activeRigidbodies.Count; i++)
        {
            float distance = Vector3.Distance(referencePos, activeRigidbodies[i].position);
            if (distance > maxDistance)
            {
                maxDistance = distance;
                farthestRb = activeRigidbodies[i];
            }
        }

        farthestRb.Sleep();
        activeRigidbodies.Remove(farthestRb);
        sleepingRigidbodies.Add(farthestRb);
    }

    void CheckSleepConditions()
    {
        List<Rigidbody> toSleep = new List<Rigidbody>();

        foreach (var rb in activeRigidbodies)
        {
            if (rb.velocity.magnitude < sleepVelocityThreshold &&
                rb.angularVelocity.magnitude < sleepVelocityThreshold)
            {
                // Check if any active controller is nearby
                bool nearActiveController = false;
                var controllers = FindObjectsOfType<RobotController>();

                foreach (var controller in controllers)
                {
                    if (Vector3.Distance(rb.position, controller.transform.position) < 5.0f)
                    {
                        nearActiveController = true;
                        break;
                    }
                }

                if (!nearActiveController)
                {
                    toSleep.Add(rb);
                }
            }
        }

        foreach (var rb in toSleep)
        {
            rb.Sleep();
            activeRigidbodies.Remove(rb);
            sleepingRigidbodies.Add(rb);
        }
    }
}
```

## Perception Simulation and Synthetic Data

### Synthetic Data Generation Pipeline

```csharp
using UnityEngine;
using System.Collections;
using System.IO;

public class SyntheticDataGenerator : MonoBehaviour
{
    [Header("Data Generation Settings")]
    public Camera mainCamera;
    public string outputDirectory = "SyntheticData";
    public int imageWidth = 640;
    public int imageHeight = 480;
    public int samplesPerScene = 100;
    public float captureInterval = 0.5f;

    [Header("Annotation Settings")]
    public bool generateDepth = true;
    public bool generateSegmentation = true;
    public bool generateBoundingBoxes = true;

    private RenderTexture rgbTexture;
    private RenderTexture depthTexture;
    private RenderTexture segmentationTexture;
    private Texture2D outputTexture2D;

    void Start()
    {
        SetupRenderTextures();
        StartCoroutine(GenerateSyntheticData());
    }

    void SetupRenderTextures()
    {
        rgbTexture = new RenderTexture(imageWidth, imageHeight, 24);
        depthTexture = new RenderTexture(imageWidth, imageHeight, 24, RenderTextureFormat.Depth);
        segmentationTexture = new RenderTexture(imageWidth, imageHeight, 24);
        outputTexture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);
    }

    IEnumerator GenerateSyntheticData()
    {
        for (int sample = 0; sample < samplesPerScene; sample++)
        {
            // Randomize environment or robot pose
            RandomizeScene();

            // Wait for scene to settle
            yield return new WaitForSeconds(0.1f);

            // Capture data
            CaptureSample(sample);

            // Wait before next capture
            yield return new WaitForSeconds(captureInterval);
        }
    }

    void RandomizeScene()
    {
        // Randomize lighting
        Light[] lights = FindObjectsOfType<Light>();
        foreach (Light light in lights)
        {
            light.intensity = Random.Range(0.5f, 1.5f);
            light.color = Random.ColorHSV(0f, 1f, 0.8f, 1f, 0.8f, 1f);
        }

        // Randomize object positions (within bounds)
        GameObject[] objects = GameObject.FindGameObjectsWithTag("Randomizable");
        foreach (GameObject obj in objects)
        {
            Vector3 randomPos = obj.transform.position;
            randomPos.x += Random.Range(-2f, 2f);
            randomPos.z += Random.Range(-2f, 2f);
            obj.transform.position = randomPos;
        }
    }

    void CaptureSample(int sampleIndex)
    {
        string sampleDir = Path.Combine(outputDirectory, $"sample_{sampleIndex:D4}");
        Directory.CreateDirectory(sampleDir);

        // Capture RGB image
        RenderTexture.active = rgbTexture;
        mainCamera.targetTexture = rgbTexture;
        mainCamera.Render();
        outputTexture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);
        outputTexture2D.Apply();
        byte[] rgbData = outputTexture2D.EncodeToPNG();
        File.WriteAllBytes(Path.Combine(sampleDir, "rgb.png"), rgbData);

        // Capture depth if enabled
        if (generateDepth)
        {
            CaptureDepth(sampleDir);
        }

        // Capture segmentation if enabled
        if (generateSegmentation)
        {
            CaptureSegmentation(sampleDir);
        }

        // Generate annotations
        GenerateAnnotations(sampleDir, sampleIndex);
    }

    void CaptureDepth(string sampleDir)
    {
        // Set up depth shader rendering
        RenderTexture.active = depthTexture;
        // Render depth using custom shader
        // Save depth data
    }

    void CaptureSegmentation(string sampleDir)
    {
        // Set up segmentation rendering with unique colors per object
        RenderTexture.active = segmentationTexture;
        // Render segmentation using semantic segmentation shader
        // Save segmentation data
    }

    void GenerateAnnotations(string sampleDir, int sampleIndex)
    {
        // Generate JSON annotation file
        var annotations = new System.Collections.Generic.Dictionary<string, object>
        {
            {"sample_id", sampleIndex},
            {"timestamp", Time.time},
            {"objects", GetObjectAnnotations()}
        };

        string annotationJson = JsonUtility.ToJson(annotations, true);
        File.WriteAllText(Path.Combine(sampleDir, "annotations.json"), annotationJson);
    }

    object GetObjectAnnotations()
    {
        // Return object positions, bounding boxes, classes, etc.
        return new object(); // Placeholder
    }
}
```

## Best Practices and Troubleshooting

### Common Issues and Solutions

**Performance Issues:**
- Use occlusion culling for large environments
- Implement frustum culling for distant objects
- Use object pooling for frequently instantiated objects
- Optimize shader complexity for real-time performance

**Physics Stability:**
- Use appropriate time steps (typically 0.01-0.02s)
- Adjust solver iteration counts for complex systems
- Use fixed joints instead of position constraints when possible
- Implement proper collision filtering

**ROS Integration:**
- Monitor message queue sizes to prevent overflow
- Use appropriate publish rates to avoid network congestion
- Implement proper error handling for network disconnections
- Use message compression for large data (images, point clouds)

Unity integration provides powerful capabilities for robotics simulation, especially for perception-heavy applications and human-robot interaction scenarios. The combination of photorealistic rendering, robust physics simulation, and ROS integration makes Unity an excellent choice for advanced robotics development and testing.
