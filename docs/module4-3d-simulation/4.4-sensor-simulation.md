---
title: Sensor Simulation
sidebar_label: Sensor Simulation
description: Simulating various sensors for humanoid robots in 3D environments including cameras, LIDAR, IMU, and force sensors
---

# 4.4 Sensor Simulation

## Overview

Sensor simulation is critical for humanoid robotics development, providing realistic sensor data that enables testing of perception, localization, and control algorithms in virtual environments. This chapter covers the simulation of various sensor types commonly used in humanoid robots, including cameras, LIDAR, IMU, force/torque sensors, and specialized humanoid sensors like contact sensors and proprioceptive sensors.

## Camera Simulation

### RGB Camera Simulation

Camera simulation in both Gazebo and Unity provides visual data for perception algorithms:

```xml
<!-- Gazebo RGB Camera Sensor Configuration -->
<sensor name="rgb_camera" type="camera">
  <camera name="head_camera">
    <horizontal_fov>1.047</horizontal_fov> <!-- 60 degrees -->
    <image>
      <width>640</width>
      <height>480</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.1</near>
      <far>10.0</far>
    </clip>
    <noise>
      <type>gaussian</type>
      <mean>0.0</mean>
      <stddev>0.007</stddev>
    </noise>
  </camera>
  <always_on>1</always_on>
  <update_rate>30</update_rate>
  <visualize>true</visualize>
</sensor>
```

```cpp
// Gazebo Camera Plugin with Advanced Features
#include <gazebo/gazebo.hh>
#include <gazebo/sensors/CameraSensor.hh>
#include <gazebo/rendering/Camera.hh>
#include <ros/ros.h>
#include <sensor_msgs/Image.h>
#include <cv_bridge/cv_bridge.h>
#include <opencv2/opencv.hpp>

namespace gazebo
{
  class AdvancedCameraPlugin : public SensorPlugin
  {
    public: virtual void Load(sensors::SensorPtr _sensor, sdf::ElementPtr _sdf)
    {
      this->cameraSensor = std::dynamic_pointer_cast<sensors::CameraSensor>(_sensor);
      if (!this->cameraSensor)
      {
        gzerr << "AdvancedCameraPlugin requires a camera sensor\n";
        return;
      }

      // Initialize ROS
      if (!ros::isInitialized())
      {
        int argc = 0;
        char **argv = NULL;
        ros::init(argc, argv, "gazebo_camera",
                  ros::init_options::NoSigintHandler);
      }

      this->rosNode.reset(new ros::NodeHandle("gazebo_camera"));
      this->imagePub = this->rosNode->advertise<sensor_msgs::Image>("/camera/image_raw", 1);

      // Connect to sensor update event
      this->newFrameConnection = this->cameraSensor->Camera()->ConnectNewFrame(
          std::bind(&AdvancedCameraPlugin::OnNewFrame, this,
                   std::placeholders::_1, std::placeholders::_2, std::placeholders::_3,
                   std::placeholders::_4, std::placeholders::_5));

      this->cameraSensor->SetActive(true);
    }

    private: void OnNewFrame(const unsigned char *_image,
                            unsigned int _width, unsigned int _height,
                            unsigned int _depth, const std::string &_format)
    {
      // Convert Gazebo image to ROS image
      sensor_msgs::ImagePtr msg = boost::make_shared<sensor_msgs::Image>();
      msg->header.stamp = ros::Time::now();
      msg->header.frame_id = "camera_frame";
      msg->width = _width;
      msg->height = _height;
      msg->encoding = "rgb8";
      msg->is_bigendian = 0;
      msg->step = _width * 3; // 3 bytes per pixel for RGB

      // Copy image data
      msg->data.resize(_width * _height * 3);
      memcpy(&msg->data[0], _image, _width * _height * 3);

      this->imagePub.publish(msg);
    }

    private: sensors::CameraSensorPtr cameraSensor;
    private: std::unique_ptr<ros::NodeHandle> rosNode;
    private: ros::Publisher imagePub;
    private: event::ConnectionPtr newFrameConnection;
  };

  GZ_REGISTER_SENSOR_PLUGIN(AdvancedCameraPlugin)
}
```

### Depth Camera Simulation

```xml
<!-- Gazebo Depth Camera Configuration -->
<sensor name="depth_camera" type="depth">
  <camera name="depth_cam">
    <horizontal_fov>1.047</horizontal_fov>
    <image>
      <width>320</width>
      <height>240</height>
      <format>L8</format>
    </image>
    <clip>
      <near>0.1</near>
      <far>10.0</far>
    </clip>
  </camera>
  <always_on>1</always_on>
  <update_rate>30</update_rate>
  <visualize>true</visualize>
</sensor>
```

```csharp
// Unity Depth Camera Simulation
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;

public class UnityDepthCamera : MonoBehaviour
{
    [Header("Camera Configuration")]
    public Camera depthCamera;
    public int depthWidth = 320;
    public int depthHeight = 240;
    public float maxDepth = 10.0f;

    [Header("ROS Configuration")]
    public string depthTopic = "/camera/depth/image_raw";

    private ROSConnection ros;
    private RenderTexture depthTexture;
    private Texture2D depthTexture2D;
    private float lastPublishTime;
    private float publishRate = 30f;

    void Start()
    {
        ros = ROSConnection.GetOrCreateInstance();

        if (depthCamera == null)
            depthCamera = GetComponent<Camera>();

        // Create depth texture
        depthTexture = new RenderTexture(depthWidth, depthHeight, 24, RenderTextureFormat.Depth);
        depthCamera.targetTexture = depthTexture;

        depthTexture2D = new Texture2D(depthWidth, depthHeight, TextureFormat.RFloat, false);

        lastPublishTime = Time.time;
    }

    void Update()
    {
        if (Time.time - lastPublishTime >= 1f / publishRate)
        {
            PublishDepthImage();
            lastPublishTime = Time.time;
        }
    }

    void PublishDepthImage()
    {
        // Render depth texture
        depthCamera.Render();

        // Read depth values
        RenderTexture.active = depthTexture;
        depthTexture2D.ReadPixels(new Rect(0, 0, depthWidth, depthHeight), 0, 0);
        depthTexture2D.Apply();

        // Convert to ROS depth message
        var depthMsg = new ImageMsg();
        depthMsg.header = new std_msgs.Header();
        depthMsg.header.stamp = new builtin_interfaces.Time();
        depthMsg.header.frame_id = transform.name;

        depthMsg.height = (uint)depthHeight;
        depthMsg.width = (uint)depthWidth;
        depthMsg.encoding = "32FC1"; // 32-bit float, single channel
        depthMsg.is_bigendian = 0;
        depthMsg.step = (uint)(depthWidth * sizeof(float));

        // Convert depth data to byte array
        Color[] depthColors = depthTexture2D.GetPixels();
        byte[] depthBytes = new byte[depthWidth * depthHeight * sizeof(float)];

        for (int i = 0; i < depthColors.Length; i++)
        {
            float depthValue = depthColors[i].r * maxDepth; // Scale to max depth
            byte[] floatBytes = System.BitConverter.GetBytes(depthValue);
            for (int j = 0; j < sizeof(float); j++)
            {
                depthBytes[i * sizeof(float) + j] = floatBytes[j];
            }
        }

        depthMsg.data = depthBytes;

        ros.Publish(depthTopic, depthMsg);
    }
}
```

## LIDAR Simulation

### 2D LIDAR Simulation

```xml
<!-- Gazebo 2D LIDAR Configuration -->
<sensor name="laser_2d" type="ray">
  <ray>
    <scan>
      <horizontal>
        <samples>720</samples>
        <resolution>1</resolution>
        <min_angle>-1.570796</min_angle> <!-- -90 degrees -->
        <max_angle>1.570796</max_angle>   <!-- 90 degrees -->
      </horizontal>
    </scan>
    <range>
      <min>0.1</min>
      <max>10.0</max>
      <resolution>0.01</resolution>
    </range>
  </ray>
  <plugin name="laser_controller" filename="libgazebo_ros_laser.so">
    <topicName>/scan</topicName>
    <frameName>laser_frame</frameName>
  </plugin>
  <always_on>1</always_on>
  <update_rate>10</update_rate>
  <visualize>true</visualize>
</sensor>
```

### 3D LIDAR Simulation (Velodyne-style)

```xml
<!-- Gazebo 3D LIDAR Configuration -->
<sensor name="velodyne_VLP_16" type="ray">
  <ray>
    <scan>
      <horizontal>
        <samples>1800</samples>
        <resolution>1</resolution>
        <min_angle>-3.141593</min_angle> <!-- -180 degrees -->
        <max_angle>3.141593</max_angle>   <!-- 180 degrees -->
      </horizontal>
      <vertical>
        <samples>16</samples>
        <resolution>1</resolution>
        <min_angle>-0.261799</min_angle> <!-- -15 degrees -->
        <max_angle>0.261799</max_angle>   <!-- 15 degrees -->
      </vertical>
    </scan>
    <range>
      <min>0.2</min>
      <max>100.0</max>
      <resolution>0.01</resolution>
    </range>
  </ray>
  <plugin name="velodyne_driver" filename="libgazebo_ros_velodyne_laser.so">
    <topicName>/velodyne_points</topicName>
    <frameName>velodyne</frameName>
    <min_range>0.9</min_range>
    <max_range>100.0</max_range>
    <gaussian_noise>0.008</gaussian_noise>
  </plugin>
  <always_on>1</always_on>
  <update_rate>10</update_rate>
  <visualize>false</visualize>
</sensor>
```

## IMU Simulation

### IMU Sensor Configuration

```xml
<!-- Gazebo IMU Sensor Configuration -->
<sensor name="imu_sensor" type="imu">
  <always_on>1</always_on>
  <update_rate>100</update_rate>
  <imu>
    <angular_velocity>
      <x>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.01</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.001</bias_stddev>
        </noise>
      </x>
      <y>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.01</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.001</bias_stddev>
        </noise>
      </y>
      <z>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.01</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.001</bias_stddev>
        </noise>
      </z>
    </angular_velocity>
    <linear_acceleration>
      <x>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.017</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.001</bias_stddev>
        </noise>
      </x>
      <y>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.017</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.001</bias_stddev>
        </noise>
      </y>
      <z>
        <noise type="gaussian">
          <mean>0.0</mean>
          <stddev>0.017</stddev>
          <bias_mean>0.0</bias_mean>
          <bias_stddev>0.001</bias_stddev>
        </noise>
      </z>
    </linear_acceleration>
  </imu>
  <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">
    <topicName>/imu/data</topicName>
    <serviceName>/imu/service</serviceName>
    <gaussianNoise>0.01</gaussianNoise>
    <updateRate>100.0</updateRate>
  </plugin>
</sensor>
```

```cpp
// Advanced IMU Simulation Plugin
#include <gazebo/gazebo.hh>
#include <gazebo/sensors/ImuSensor.hh>
#include <ros/ros.h>
#include <sensor_msgs/Imu.h>
#include <random>

namespace gazebo
{
  class AdvancedIMUPlugin : public SensorPlugin
  {
    public: virtual void Load(sensors::SensorPtr _sensor, sdf::ElementPtr _sdf)
    {
      this->imuSensor = std::dynamic_pointer_cast<sensors::ImuSensor>(_sensor);
      if (!this->imuSensor)
      {
        gzerr << "AdvancedIMUPlugin requires an IMU sensor\n";
        return;
      }

      // Initialize ROS
      if (!ros::isInitialized())
      {
        int argc = 0;
        char **argv = NULL;
        ros::init(argc, argv, "gazebo_imu",
                  ros::init_options::NoSigintHandler);
      }

      this->rosNode.reset(new ros::NodeHandle("gazebo_imu"));
      this->imuPub = this->rosNode->advertise<sensor_msgs::Imu>("/imu/data", 1000);

      // Initialize noise generator
      this->noiseGen = std::mt19937(42); // Fixed seed for reproducibility
      this->gaussianNoise = std::normal_distribution<double>(0.0, 0.01);

      // Connect to sensor update event
      this->updateConnection = this->imuSensor->ConnectUpdated(
          std::bind(&AdvancedIMUPlugin::OnUpdate, this));

      this->imuSensor->SetActive(true);
    }

    private: void OnUpdate()
    {
      ignition::math::Vector3d linearAccel = this->imuSensor->LinearAcceleration();
      ignition::math::Vector3d angularVel = this->imuSensor->AngularVelocity();
      ignition::math::Quaterniond orientation = this->imuSensor->Orientation();

      sensor_msgs::Imu imuMsg;
      imuMsg.header.stamp = ros::Time::now();
      imuMsg.header.frame_id = "imu_link";

      // Add realistic noise to measurements
      imuMsg.linear_acceleration.x = linearAccel.X() + this->gaussianNoise(this->noiseGen);
      imuMsg.linear_acceleration.y = linearAccel.Y() + this->gaussianNoise(this->noiseGen);
      imuMsg.linear_acceleration.z = linearAccel.Z() + this->gaussianNoise(this->noiseGen);

      imuMsg.angular_velocity.x = angularVel.X() + this->gaussianNoise(this->noiseGen);
      imuMsg.angular_velocity.y = angularVel.Y() + this->gaussianNoise(this->noiseGen);
      imuMsg.angular_velocity.z = angularVel.Z() + this->gaussianNoise(this->noiseGen);

      // For orientation, we might want to use magnetometer data or integrate angular velocities
      // For simulation, we'll use the ground truth with some noise
      imuMsg.orientation.w = orientation.W();
      imuMsg.orientation.x = orientation.X() + this->gaussianNoise(this->noiseGen) * 0.001;
      imuMsg.orientation.y = orientation.Y() + this->gaussianNoise(this->noiseGen) * 0.001;
      imuMsg.orientation.z = orientation.Z() + this->gaussianNoise(this->noiseGen) * 0.001;

      // Set covariance matrices (important for robot_localization)
      for (int i = 0; i < 9; i++)
      {
        imuMsg.angular_velocity_covariance[i] = (i % 4 == 0) ? 0.01 : 0.0; // Diagonal
        imuMsg.linear_acceleration_covariance[i] = (i % 4 == 0) ? 0.017*0.017 : 0.0;
        imuMsg.orientation_covariance[i] = (i % 4 == 0) ? 0.01 : 0.0;
      }

      this->imuPub.publish(imuMsg);
    }

    private: sensors::ImuSensorPtr imuSensor;
    private: std::unique_ptr<ros::NodeHandle> rosNode;
    private: ros::Publisher imuPub;
    private: event::ConnectionPtr updateConnection;
    private: std::mt19937 noiseGen;
    private: std::normal_distribution<double> gaussianNoise;
  };

  GZ_REGISTER_SENSOR_PLUGIN(AdvancedIMUPlugin)
}
```

## Force/Torque Sensor Simulation

### Joint Force/Torque Sensors

```xml
<!-- Gazebo Joint Force/Torque Sensor -->
<sensor name="joint_force_torque_sensor" type="force_torque">
  <always_on>1</always_on>
  <update_rate>100</update_rate>
  <force_torque>
    <frame>sensor</frame> <!-- sensor or child -->
    <measure_direction>child_to_parent</measure_direction>
  </force_torque>
  <plugin name="ft_sensor_plugin" filename="libgazebo_ros_ft_sensor.so">
    <topicName>/joint_wrench</topicName>
    <jointName>hip_joint</jointName>
  </plugin>
</sensor>
```

### 6-Axis Force/Torque Sensor

```cpp
// 6-axis Force/Torque Sensor Plugin
#include <gazebo/gazebo.hh>
#include <gazebo/physics/Joint.hh>
#include <gazebo/physics/Link.hh>
#include <ros/ros.h>
#include <geometry_msgs/WrenchStamped.h>

namespace gazebo
{
  class ForceTorqueSensor : public ModelPlugin
  {
    public: void Load(physics::ModelPtr _model, sdf::ElementPtr _sdf)
    {
      this->model = _model;

      // Get joint to measure forces on
      if (_sdf->HasElement("joint_name"))
      {
        std::string jointName = _sdf->Get<std::string>("joint_name");
        this->joint = _model->GetJoint(jointName);
      }

      if (!ros::isInitialized())
      {
        int argc = 0;
        char **argv = NULL;
        ros::init(argc, argv, "gazebo_ft_sensor",
                  ros::init_options::NoSigintHandler);
      }

      this->rosNode.reset(new ros::NodeHandle("gazebo_ft_sensor"));
      this->wrenchPub = this->rosNode->advertise<geometry_msgs::WrenchStamped>("/ft_sensor/wrench", 1000);

      this->updateConnection = event::Events::ConnectWorldUpdateBegin(
          std::bind(&ForceTorqueSensor::OnUpdate, this));
    }

    private: void OnUpdate()
    {
      if (!this->joint) return;

      geometry_msgs::WrenchStamped wrenchMsg;
      wrenchMsg.header.stamp = ros::Time::now();
      wrenchMsg.header.frame_id = this->joint->GetName() + "_ft_frame";

      // Get joint forces and torques
      auto forces = this->joint->GetForceTorque(0);

      wrenchMsg.wrench.force.x = forces.body2Force.X();
      wrenchMsg.wrench.force.y = forces.body2Force.Y();
      wrenchMsg.wrench.force.z = forces.body2Force.Z();

      wrenchMsg.wrench.torque.x = forces.body2Torque.X();
      wrenchMsg.wrench.torque.y = forces.body2Torque.Y();
      wrenchMsg.wrench.torque.z = forces.body2Torque.Z();

      this->wrenchPub.publish(wrenchMsg);
    }

    private: physics::ModelPtr model;
    private: physics::JointPtr joint;
    private: std::unique_ptr<ros::NodeHandle> rosNode;
    private: ros::Publisher wrenchPub;
    private: event::ConnectionPtr updateConnection;
  };

  GZ_REGISTER_MODEL_PLUGIN(ForceTorqueSensor)
}
```

## Contact Sensors for Humanoid Robotics

### Foot Contact Sensors

```xml
<!-- Gazebo Foot Contact Sensor -->
<gazebo reference="left_foot">
  <sensor name="left_foot_contact" type="contact">
    <always_on>1</always_on>
    <update_rate>200</update_rate>
    <contact>
      <collision>left_foot_collision</collision>
    </contact>
    <plugin name="left_foot_contact_plugin" filename="libgazebo_ros_bumper.so">
      <alwaysOn>true</alwaysOn>
      <updateRate>200.0</updateRate>
      <bumperTopicName>left_foot_contact</bumperTopicName>
      <frameName>left_foot</frameName>
    </plugin>
  </sensor>
</gazebo>
```

```cpp
// Advanced Contact Sensor Plugin for Humanoid Balance
#include <gazebo/gazebo.hh>
#include <gazebo/sensors/ContactSensor.hh>
#include <ros/ros.h>
#include <std_msgs/Bool.h>
#include <geometry_msgs/WrenchStamped.h>

namespace gazebo
{
  class HumanoidContactSensor : public SensorPlugin
  {
    public: virtual void Load(sensors::SensorPtr _sensor, sdf::ElementPtr _sdf)
    {
      this->contactSensor = std::dynamic_pointer_cast<sensors::ContactSensor>(_sensor);
      if (!this->contactSensor)
      {
        gzerr << "HumanoidContactSensor requires a contact sensor\n";
        return;
      }

      // Get sensor parameters
      if (_sdf->HasElement("contact_threshold"))
        this->contactThreshold = _sdf->Get<double>("contact_threshold");
      else
        this->contactThreshold = 1.0; // Default threshold

      if (_sdf->HasElement("sensor_name"))
        this->sensorName = _sdf->Get<std::string>("sensor_name");
      else
        this->sensorName = "contact_sensor";

      // Initialize ROS
      if (!ros::isInitialized())
      {
        int argc = 0;
        char **argv = NULL;
        ros::init(argc, argv, "gazebo_contact",
                  ros::init_options::NoSigintHandler);
      }

      this->rosNode.reset(new ros::NodeHandle("gazebo_contact"));
      this->contactPub = this->rosNode->advertise<std_msgs::Bool>("/contact/" + this->sensorName, 1000);
      this->wrenchPub = this->rosNode->advertise<geometry_msgs::WrenchStamped>("/wrench/" + this->sensorName, 1000);

      // Connect to sensor update event
      this->updateConnection = this->contactSensor->ConnectUpdated(
          std::bind(&HumanoidContactSensor::OnUpdate, this));

      this->contactSensor->SetActive(true);
    }

    private: void OnUpdate()
    {
      msgs::Contacts contacts = this->contactSensor->Contacts();

      std_msgs::Bool contactMsg;
      contactMsg.data = contacts.contact_size() > 0;

      // Calculate total contact force
      ignition::math::Vector3d totalForce(0, 0, 0);
      for (int i = 0; i < contacts.contact_size(); ++i)
      {
        const msgs::Contact& contact = contacts.contact(i);
        for (int j = 0; j < contact.wrench_size(); ++j)
        {
          const msgs::Wrench& wrench = contact.wrench(j);
          totalForce.X() += wrench.body_1_wrench().force().x() + wrench.body_2_wrench().force().x();
          totalForce.Y() += wrench.body_1_wrench().force().y() + wrench.body_2_wrench().force().y();
          totalForce.Z() += wrench.body_1_wrench().force().z() + wrench.body_2_wrench().force().z();
        }
      }

      // Publish contact state
      this->contactPub.publish(contactMsg);

      // Publish wrench if contact force exceeds threshold
      if (totalForce.Length() > this->contactThreshold)
      {
        geometry_msgs::WrenchStamped wrenchMsg;
        wrenchMsg.header.stamp = ros::Time::now();
        wrenchMsg.header.frame_id = this->sensorName + "_frame";

        wrenchMsg.wrench.force.x = totalForce.X();
        wrenchMsg.wrench.force.y = totalForce.Y();
        wrenchMsg.wrench.force.z = totalForce.Z();

        // Set torque to zero for now (could be calculated based on contact points)
        wrenchMsg.wrench.torque.x = 0.0;
        wrenchMsg.wrench.torque.y = 0.0;
        wrenchMsg.wrench.torque.z = 0.0;

        this->wrenchPub.publish(wrenchMsg);
      }
    }

    private: sensors::ContactSensorPtr contactSensor;
    private: std::unique_ptr<ros::NodeHandle> rosNode;
    private: ros::Publisher contactPub;
    private: ros::Publisher wrenchPub;
    private: event::ConnectionPtr updateConnection;
    private: double contactThreshold;
    private: std::string sensorName;
  };

  GZ_REGISTER_SENSOR_PLUGIN(HumanoidContactSensor)
}
```

## Sensor Fusion and Data Processing

### Multi-Sensor Data Integration

```python
#!/usr/bin/env python3
"""
Multi-sensor fusion for humanoid robot state estimation
"""
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu, JointState
from geometry_msgs.msg import Vector3
from nav_msgs.msg import Odometry
from std_msgs.msg import Float64MultiArray
import numpy as np
from scipy.spatial.transform import Rotation as R
import math

class SensorFusionNode(Node):
    def __init__(self):
        super().__init__('sensor_fusion_node')

        # Subscriptions for different sensors
        self.imu_sub = self.create_subscription(
            Imu, '/imu/data', self.imu_callback, 10)
        self.joint_sub = self.create_subscription(
            JointState, '/joint_states', self.joint_callback, 10)
        self.contact_sub = self.create_subscription(
            Bool, '/contact/left_foot_contact', self.contact_callback, 10)
        self.contact_right_sub = self.create_subscription(
            Bool, '/contact/right_foot_contact', self.contact_right_callback, 10)

        # Publisher for fused state
        self.state_pub = self.create_publisher(
            Odometry, '/fused_state', 10)

        # State estimation variables
        self.orientation = R.from_quat([0, 0, 0, 1])  # Initial orientation
        self.angular_velocity = np.array([0.0, 0.0, 0.0])
        self.linear_acceleration = np.array([0.0, 0.0, 0.0])
        self.position = np.array([0.0, 0.0, 0.0])
        self.velocity = np.array([0.0, 0.0, 0.0])

        # Contact state
        self.left_contact = False
        self.right_contact = False

        # Kalman filter parameters
        self.process_noise = 0.1
        self.measurement_noise = 0.1
        self.covariance = np.eye(6) * 0.1  # Initial covariance

        # Timer for fusion loop
        self.timer = self.create_timer(0.01, self.fusion_loop)  # 100Hz

        self.get_logger().info('Sensor fusion node initialized')

    def imu_callback(self, msg):
        """Process IMU data"""
        # Update orientation using gyroscope integration
        dt = 0.01  # Assuming 100Hz rate
        gyro = np.array([
            msg.angular_velocity.x,
            msg.angular_velocity.y,
            msg.angular_velocity.z
        ])

        # Integrate angular velocity
        angle_increment = gyro * dt
        rotation_vector = R.from_rotvec(angle_increment)
        self.orientation = self.orientation * rotation_vector

        # Store linear acceleration (remove gravity component later)
        self.linear_acceleration = np.array([
            msg.linear_acceleration.x,
            msg.linear_acceleration.y,
            msg.linear_acceleration.z
        ])

    def joint_callback(self, msg):
        """Process joint state data"""
        # Use joint encoders for position feedback
        # This can be used to improve pose estimation
        pass

    def contact_callback(self, msg):
        """Process left foot contact"""
        self.left_contact = msg.data

    def contact_right_callback(self, msg):
        """Process right foot contact"""
        self.right_contact = msg.data

    def fusion_loop(self):
        """Main sensor fusion loop"""
        # Predict step (using IMU data)
        dt = 0.01

        # Predict position based on current velocity and acceleration
        self.velocity += self.linear_acceleration * dt
        self.position += self.velocity * dt

        # Update step (correct with additional sensor data)
        # In a real implementation, this would use a proper Kalman filter

        # Check if we have foot contact (for zero-velocity updates)
        if self.left_contact and self.right_contact:
            # Both feet in contact - likely standing still
            self.velocity *= 0.9  # Dampen velocity
        elif self.left_contact or self.right_contact:
            # Single foot contact - reduce velocity in horizontal plane
            self.velocity[0:2] *= 0.95  # Dampen horizontal velocity

        # Publish fused state
        self.publish_fused_state()

    def publish_fused_state(self):
        """Publish the fused state estimate"""
        odom_msg = Odometry()
        odom_msg.header.stamp = self.get_clock().now().to_msg()
        odom_msg.header.frame_id = 'odom'
        odom_msg.child_frame_id = 'base_link'

        # Position
        odom_msg.pose.pose.position.x = float(self.position[0])
        odom_msg.pose.pose.position.y = float(self.position[1])
        odom_msg.pose.pose.position.z = float(self.position[2])

        # Orientation
        quat = self.orientation.as_quat()
        odom_msg.pose.pose.orientation.w = float(quat[3])
        odom_msg.pose.pose.orientation.x = float(quat[0])
        odom_msg.pose.pose.orientation.y = float(quat[1])
        odom_msg.pose.pose.orientation.z = float(quat[2])

        # Velocity
        odom_msg.twist.twist.linear.x = float(self.velocity[0])
        odom_msg.twist.twist.linear.y = float(self.velocity[1])
        odom_msg.twist.twist.linear.z = float(self.velocity[2])

        # Angular velocity
        odom_msg.twist.twist.angular.x = float(self.angular_velocity[0])
        odom_msg.twist.twist.angular.y = float(self.angular_velocity[1])
        odom_msg.twist.twist.angular.z = float(self.angular_velocity[2])

        # Covariance (simplified)
        for i in range(36):
            odom_msg.pose.covariance[i] = float(self.covariance.flatten()[i] if i < len(self.covariance.flatten()) else 0)

        self.state_pub.publish(odom_msg)

def main(args=None):
    rclpy.init(args=args)
    fusion_node = SensorFusionNode()

    try:
        rclpy.spin(fusion_node)
    except KeyboardInterrupt:
        pass
    finally:
        fusion_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Unity Sensor Simulation

### Unity Sensor Manager

```csharp
// UnitySensorManager.cs - Central sensor management in Unity
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;
using RosMessageTypes.Geometry;
using System.Collections.Generic;

public class UnitySensorManager : MonoBehaviour
{
    [Header("Sensor Configuration")]
    public List<UnitySensorBase> sensors = new List<UnitySensorBase>();

    [Header("ROS Integration")]
    public string sensorDataTopic = "/sensor_data";

    private ROSConnection ros;
    private float lastPublishTime;
    private float publishRate = 50f; // Hz

    void Start()
    {
        ros = ROSConnection.GetOrCreateInstance();
        lastPublishTime = Time.time;

        // Initialize all sensors
        InitializeSensors();
    }

    void InitializeSensors()
    {
        // Find all sensor components in children
        UnitySensorBase[] foundSensors = GetComponentsInChildren<UnitySensorBase>();
        sensors.AddRange(foundSensors);

        foreach (UnitySensorBase sensor in sensors)
        {
            sensor.Initialize(ros);
        }
    }

    void Update()
    {
        if (Time.time - lastPublishTime >= 1f / publishRate)
        {
            PublishSensorData();
            lastPublishTime = Time.time;
        }
    }

    void PublishSensorData()
    {
        // Aggregate all sensor data
        var sensorMsg = new JointStateMsg();
        sensorMsg.header = new std_msgs.Header();
        sensorMsg.header.stamp = new builtin_interfaces.Time();
        sensorMsg.header.frame_id = "sensor_aggregate";

        // Add data from each sensor
        foreach (UnitySensorBase sensor in sensors)
        {
            sensor.UpdateSensorData(ref sensorMsg);
        }

        ros.Publish(sensorDataTopic, sensorMsg);
    }
}

// Base class for Unity sensors
public abstract class UnitySensorBase : MonoBehaviour
{
    public string sensorName;
    public string topicName;
    public float updateRate = 30f;

    protected ROSConnection ros;
    protected float lastUpdateTime;

    public virtual void Initialize(ROSConnection rosConnection)
    {
        this.ros = rosConnection;
        this.lastUpdateTime = Time.time;
    }

    public virtual void UpdateSensorData(ref JointStateMsg sensorMsg)
    {
        if (Time.time - lastUpdateTime >= 1f / updateRate)
        {
            ProcessSensorData();
            lastUpdateTime = Time.time;
        }
    }

    protected abstract void ProcessSensorData();
}

// Camera sensor implementation
public class UnityCameraSensor : UnitySensorBase
{
    public Camera cameraComponent;
    public int imageWidth = 640;
    public int imageHeight = 480;

    private RenderTexture renderTexture;
    private Texture2D texture2D;

    public override void Initialize(ROSConnection rosConnection)
    {
        base.Initialize(rosConnection);

        if (cameraComponent == null)
            cameraComponent = GetComponent<Camera>();

        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);
        cameraComponent.targetTexture = renderTexture;
        texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);
    }

    protected override void ProcessSensorData()
    {
        // Render and capture image
        RenderTexture.active = renderTexture;
        texture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);
        texture2D.Apply();

        // Convert to ROS message and publish
        var imageMsg = new ImageMsg();
        imageMsg.header = new std_msgs.Header();
        imageMsg.header.stamp = new builtin_interfaces.Time();
        imageMsg.header.frame_id = sensorName;

        imageMsg.height = (uint)imageHeight;
        imageMsg.width = (uint)imageWidth;
        imageMsg.encoding = "rgb8";
        imageMsg.is_bigendian = 0;
        imageMsg.step = (uint)(imageWidth * 3);

        byte[] imageData = texture2D.EncodeToPNG();
        imageMsg.data = imageData;

        ros.Publish(topicName, imageMsg);
    }
}
```

## Sensor Calibration and Validation

### Sensor Calibration Process

```python
#!/usr/bin/env python3
"""
Sensor calibration for humanoid robot simulation
"""
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu, JointState
from geometry_msgs.msg import Vector3
import numpy as np
from scipy.optimize import minimize
import yaml

class SensorCalibrationNode(Node):
    def __init__(self):
        super().__init__('sensor_calibration_node')

        # Subscriptions
        self.imu_sub = self.create_subscription(
            Imu, '/imu/data', self.imu_callback, 10)
        self.joint_sub = self.create_subscription(
            JointState, '/joint_states', self.joint_callback, 10)

        # Calibration parameters
        self.calibration_data = {
            'imu_bias': np.zeros(6),  # [acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z]
            'imu_scale': np.ones(6),
            'joint_offsets': {},
            'data_buffer': []
        }

        # Calibration state
        self.calibration_state = 'idle'  # idle, collecting, processing
        self.data_collection_count = 0
        self.max_data_points = 1000

        # Timer for calibration process
        self.timer = self.create_timer(0.1, self.calibration_timer)

        self.get_logger().info('Sensor calibration node initialized')

    def imu_callback(self, msg):
        """Collect IMU data for calibration"""
        if self.calibration_state == 'collecting':
            # Store raw sensor data
            raw_data = {
                'timestamp': self.get_clock().now(),
                'linear_accel': np.array([
                    msg.linear_acceleration.x,
                    msg.linear_acceleration.y,
                    msg.linear_acceleration.z
                ]),
                'angular_vel': np.array([
                    msg.angular_velocity.x,
                    msg.angular_velocity.y,
                    msg.angular_velocity.z
                ])
            }
            self.calibration_data['data_buffer'].append(raw_data)
            self.data_collection_count += 1

            if self.data_collection_count >= self.max_data_points:
                self.process_calibration_data()

    def joint_callback(self, msg):
        """Process joint state data"""
        # Store initial joint positions for offset calculation
        if self.calibration_state == 'collecting':
            for i, name in enumerate(msg.name):
                if name not in self.calibration_data['joint_offsets']:
                    self.calibration_data['joint_offsets'][name] = msg.position[i]

    def start_calibration(self):
        """Start the calibration process"""
        self.get_logger().info('Starting sensor calibration...')
        self.calibration_state = 'collecting'
        self.data_collection_count = 0
        self.calibration_data['data_buffer'] = []

    def process_calibration_data(self):
        """Process collected data to compute calibration parameters"""
        if not self.calibration_data['data_buffer']:
            self.get_logger().warn('No calibration data collected')
            return

        # Convert to numpy arrays for processing
        linear_acc_data = np.array([d['linear_accel'] for d in self.calibration_data['data_buffer']])
        angular_vel_data = np.array([d['angular_vel'] for d in self.calibration_data['data_buffer']])

        # Calculate IMU bias (assuming robot is stationary during calibration)
        acc_bias = np.mean(linear_acc_data, axis=0)
        # Adjust for gravity (assuming Z-axis is up)
        acc_bias[2] -= 9.81  # Remove gravity component

        gyro_bias = np.mean(angular_vel_data, axis=0)

        # Calculate scale factors (assuming ideal values)
        # For acceleration, when stationary, magnitude should be 9.81 m/s^2
        acc_magnitude = np.linalg.norm(linear_acc_data - acc_bias, axis=1)
        acc_scale = 9.81 / np.mean(acc_magnitude)

        # Store calibration results
        self.calibration_data['imu_bias'][:3] = acc_bias
        self.calibration_data['imu_bias'][3:] = gyro_bias
        self.calibration_data['imu_scale'][:3] = acc_scale
        self.calibration_data['imu_scale'][3:] = 1.0  # Assume gyro scale is 1.0

        self.get_logger().info(f'Calibration completed. Acc bias: {acc_bias}, Gyro bias: {gyro_bias}')

        # Save calibration to file
        self.save_calibration()

        # Return to idle state
        self.calibration_state = 'idle'

    def save_calibration(self):
        """Save calibration parameters to file"""
        calib_data = {
            'imu_bias': self.calibration_data['imu_bias'].tolist(),
            'imu_scale': self.calibration_data['imu_scale'].tolist(),
            'joint_offsets': {k: float(v) for k, v in self.calibration_data['joint_offsets'].items()}
        }

        with open('/tmp/sensor_calibration.yaml', 'w') as f:
            yaml.dump(calib_data, f)

        self.get_logger().info('Calibration parameters saved to /tmp/sensor_calibration.yaml')

    def calibration_timer(self):
        """Timer callback for calibration process"""
        if self.calibration_state == 'idle':
            # Start calibration automatically after 5 seconds
            if self.get_clock().now().nanoseconds > 5e9:
                self.start_calibration()

def main(args=None):
    rclpy.init(args=args)
    calibration_node = SensorCalibrationNode()

    try:
        rclpy.spin(calibration_node)
    except KeyboardInterrupt:
        pass
    finally:
        calibration_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Performance Optimization for Sensor Simulation

### Sensor Update Optimization

```cpp
// Optimized sensor manager for multiple sensors
#include <gazebo/gazebo.hh>
#include <gazebo/physics/physics.hh>
#include <ros/ros.h>
#include <vector>
#include <memory>

namespace gazebo
{
  class OptimizedSensorManager : public WorldPlugin
  {
    public: void Load(physics::WorldPtr _world, sdf::ElementPtr _sdf)
    {
      this->world = _world;

      // Initialize ROS
      if (!ros::isInitialized())
      {
        int argc = 0;
        char **argv = NULL;
        ros::init(argc, argv, "gazebo_sensor_manager",
                  ros::init_options::NoSigintHandler);
      }

      this->rosNode.reset(new ros::NodeHandle("gazebo_sensor_manager"));

      // Set up optimized update loop
      this->updateConnection = event::Events::ConnectWorldUpdateBegin(
          std::bind(&OptimizedSensorManager::OnUpdate, this));
    }

    private: void OnUpdate()
    {
      common::Time currentTime = this->world->SimTime();

      // Only update sensors that need updating based on their rate
      for (auto& sensor : this->sensors)
      {
        if ((currentTime - sensor->lastUpdateTime).Double() >= sensor->updatePeriod)
        {
          sensor->Update();
          sensor->lastUpdateTime = currentTime;
        }
      }

      // Batch publish sensor data to reduce ROS overhead
      this->BatchPublish();
    }

    private: void BatchPublish()
    {
      // Publish all sensor data in batches to reduce communication overhead
      // Implementation would depend on specific sensor types
    }

    private: physics::WorldPtr world;
    private: std::unique_ptr<ros::NodeHandle> rosNode;
    private: event::ConnectionPtr updateConnection;
    private: std::vector<std::shared_ptr<SensorInterface>> sensors;
  };

  GZ_REGISTER_WORLD_PLUGIN(OptimizedSensorManager)
}
```

## Troubleshooting Sensor Simulation Issues

### Common Sensor Simulation Problems

1. **High CPU Usage:**
   - Reduce sensor update rates where possible
   - Use lower resolution for cameras
   - Limit the number of active sensors during simulation

2. **Inconsistent Data:**
   - Verify sensor noise parameters
   - Check for proper frame transformations
   - Ensure correct time synchronization

3. **Integration Issues:**
   - Verify ROS topic names and message types
   - Check sensor frame IDs match TF tree
   - Validate sensor mounting positions

4. **Accuracy Problems:**
   - Calibrate sensors in simulation
   - Adjust noise parameters to match real sensors
   - Validate sensor models against real hardware

## Summary

Sensor simulation is a critical component of humanoid robotics development, providing realistic data for perception, localization, and control algorithms. By properly configuring and calibrating various sensor types including cameras, LIDAR, IMU, and contact sensors, developers can create accurate simulation environments that closely match real-world conditions. Advanced techniques such as sensor fusion and optimization ensure that simulated sensors provide reliable and efficient data streams for humanoid robot algorithms. Proper validation and troubleshooting practices help maintain the fidelity of the simulation environment.