---
title: Sim-to-Real Gap
sidebar_label: Sim-to-Real Gap
description: Understanding and bridging the gap between simulation and real-world humanoid robotics applications
---

# 4.5 Sim-to-Real Gap

## Overview

The sim-to-real gap represents the fundamental challenge in robotics where algorithms and behaviors that work well in simulation fail when deployed on real robots. This gap is particularly pronounced in humanoid robotics due to the complex dynamics, sensor noise, actuator limitations, and environmental uncertainties that are difficult to perfectly model in simulation. Understanding and addressing this gap is crucial for successful deployment of humanoid robots.

## Understanding the Sim-to-Real Gap

### Sources of the Gap

The sim-to-real gap stems from multiple sources that compound when dealing with complex humanoid systems:

1. **Modeling Inaccuracies:**
   - Mass and inertia estimation errors
   - Joint friction and backlash not modeled
   - Flexible joints and structural compliance
   - Unmodeled dynamics and vibrations

2. **Sensor Imperfections:**
   - Noise, bias, and drift in real sensors
   - Time delays and synchronization issues
   - Limited field of view and resolution
   - Environmental interference (lighting, magnetic fields)

3. **Actuator Limitations:**
   - Torque and speed constraints
   - Gear backlash and motor dynamics
   - Power limitations and thermal effects
   - Control loop delays and discretization

4. **Environmental Factors:**
   - Surface properties (friction, compliance)
   - External disturbances and forces
   - Temperature and humidity effects
   - Wear and tear of components

### Quantifying the Gap

```python
#!/usr/bin/env python3
"""
Tool for quantifying sim-to-real gap in humanoid robotics
"""
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.transform import Rotation as R
import pickle

class Sim2RealGapAnalyzer:
    def __init__(self):
        self.sim_data = []
        self.real_data = []
        self.metrics = {}

    def add_simulation_data(self, data):
        """Add simulation data for comparison"""
        self.sim_data.append(data)

    def add_real_world_data(self, data):
        """Add real-world data for comparison"""
        self.real_data.append(data)

    def calculate_position_error(self):
        """Calculate position tracking error between sim and real"""
        if len(self.sim_data) != len(self.real_data):
            return float('inf')

        errors = []
        for sim, real in zip(self.sim_data, self.real_data):
            sim_pos = np.array(sim.get('position', [0, 0, 0]))
            real_pos = np.array(real.get('position', [0, 0, 0]))
            error = np.linalg.norm(sim_pos - real_pos)
            errors.append(error)

        self.metrics['position_error_mean'] = np.mean(errors)
        self.metrics['position_error_std'] = np.std(errors)
        return self.metrics['position_error_mean']

    def calculate_orientation_error(self):
        """Calculate orientation error between sim and real"""
        if len(self.sim_data) != len(self.real_data):
            return float('inf')

        errors = []
        for sim, real in zip(self.sim_data, self.real_data):
            sim_quat = np.array(sim.get('orientation', [0, 0, 0, 1]))
            real_quat = np.array(real.get('orientation', [0, 0, 0, 1]))

            # Convert to rotation objects and calculate angle difference
            sim_rot = R.from_quat(sim_quat)
            real_rot = R.from_quat(real_quat)
            relative_rot = sim_rot.inv() * real_rot
            angle_error = relative_rot.magnitude()  # in radians

            errors.append(angle_error)

        self.metrics['orientation_error_mean'] = np.mean(errors)
        self.metrics['orientation_error_std'] = np.std(errors)
        return self.metrics['orientation_error_mean']

    def calculate_control_effort(self):
        """Compare control effort between simulation and reality"""
        sim_effort = [data.get('control_effort', 0) for data in self.sim_data]
        real_effort = [data.get('control_effort', 0) for data in self.real_data]

        self.metrics['sim_avg_effort'] = np.mean(sim_effort)
        self.metrics['real_avg_effort'] = np.mean(real_effort)
        self.metrics['effort_ratio'] = self.metrics['real_avg_effort'] / (self.metrics['sim_avg_effort'] + 1e-8)

        return self.metrics['effort_ratio']

    def generate_gap_report(self):
        """Generate comprehensive gap analysis report"""
        self.calculate_position_error()
        self.calculate_orientation_error()
        self.calculate_control_effort()

        report = f"""
        Sim-to-Real Gap Analysis Report
        ===============================

        Position Error:
        - Mean: {self.metrics['position_error_mean']:.4f} m
        - Std:  {self.metrics['position_error_std']:.4f} m

        Orientation Error:
        - Mean: {self.metrics['orientation_error_mean']:.4f} rad
        - Std:  {self.metrics['orientation_error_std']:.4f} rad

        Control Effort:
        - Simulation: {self.metrics['sim_avg_effort']:.4f}
        - Real World: {self.metrics['real_avg_effort']:.4f}
        - Ratio:      {self.metrics['effort_ratio']:.4f}

        Gap Severity: {'HIGH' if self.metrics['position_error_mean'] > 0.1 else 'MEDIUM' if self.metrics['position_error_mean'] > 0.05 else 'LOW'}
        """

        return report

    def visualize_comparison(self):
        """Create visualization of sim vs real comparison"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # Position comparison
        sim_positions = np.array([data.get('position', [0, 0, 0]) for data in self.sim_data])
        real_positions = np.array([data.get('position', [0, 0, 0]) for data in self.real_data])

        axes[0, 0].plot(sim_positions[:, 0], label='Simulation X', alpha=0.7)
        axes[0, 0].plot(real_positions[:, 0], label='Real X', alpha=0.7)
        axes[0, 0].set_title('X Position Comparison')
        axes[0, 0].legend()

        axes[0, 1].plot(sim_positions[:, 1], label='Simulation Y', alpha=0.7)
        axes[0, 1].plot(real_positions[:, 1], label='Real Y', alpha=0.7)
        axes[0, 1].set_title('Y Position Comparison')
        axes[0, 1].legend()

        # Error over time
        errors = [np.linalg.norm(sim_pos - real_pos)
                 for sim_pos, real_pos in zip(sim_positions, real_positions)]
        axes[1, 0].plot(errors)
        axes[1, 0].set_title('Position Error Over Time')
        axes[1, 0].set_ylabel('Error (m)')

        # Control effort comparison
        sim_effort = [data.get('control_effort', 0) for data in self.sim_data]
        real_effort = [data.get('control_effort', 0) for data in self.real_data]
        axes[1, 1].plot(sim_effort, label='Simulation', alpha=0.7)
        axes[1, 1].plot(real_effort, label='Real World', alpha=0.7)
        axes[1, 1].set_title('Control Effort Comparison')
        axes[1, 1].legend()

        plt.tight_layout()
        plt.savefig('sim2real_comparison.png')
        plt.show()

# Example usage
if __name__ == "__main__":
    analyzer = Sim2RealGapAnalyzer()

    # Example data (in practice, this would come from actual sim/real experiments)
    for i in range(100):
        # Simulated data (ideal)
        sim_data = {
            'position': [i * 0.01, 0.0, 0.8],  # Moving in straight line
            'orientation': [0, 0, 0, 1],  # No rotation
            'control_effort': 0.5
        }

        # Real data (with noise and errors)
        real_data = {
            'position': [i * 0.01 + np.random.normal(0, 0.01),
                        np.random.normal(0, 0.005),
                        0.8 + np.random.normal(0, 0.02)],  # Slight variations
            'orientation': [np.random.normal(0, 0.01),
                           np.random.normal(0, 0.01),
                           np.random.normal(0, 0.01), 1],
            'control_effort': 0.5 + np.random.normal(0, 0.1)  # More effort needed
        }

        analyzer.add_simulation_data(sim_data)
        analyzer.add_real_world_data(real_data)

    print(analyzer.generate_gap_report())
```

## Domain Randomization

Domain randomization is a technique to reduce the sim-to-real gap by randomizing simulation parameters:

```python
#!/usr/bin/env python3
"""
Domain randomization for humanoid robot simulation
"""
import numpy as np
import random

class DomainRandomizer:
    def __init__(self):
        self.randomization_ranges = {
            # Physical properties
            'mass_range': (0.8, 1.2),  # 80% to 120% of nominal
            'friction_range': (0.1, 1.0),
            'inertia_range': (0.9, 1.1),

            # Control parameters
            'control_delay_range': (0.001, 0.01),  # 1-10ms delay
            'actuator_noise_range': (0.0, 0.05),   # 0-5% noise

            # Sensor parameters
            'sensor_noise_range': (0.001, 0.01),   # Noise level
            'sensor_bias_range': (-0.01, 0.01),    # Bias range
            'sensor_delay_range': (0.001, 0.005),  # 1-5ms delay

            # Environmental parameters
            'gravity_range': (9.7, 9.9),           # Gravity variation
            'ground_friction_range': (0.4, 0.8),   # Ground friction
        }

    def randomize_robot_properties(self, robot_model):
        """Randomize robot physical properties"""
        randomized_params = {}

        # Randomize mass properties
        for link_name in robot_model.links:
            nominal_mass = robot_model.links[link_name].mass
            randomized_params[f'{link_name}_mass'] = nominal_mass * random.uniform(
                self.randomization_ranges['mass_range'][0],
                self.randomization_ranges['mass_range'][1]
            )

            # Randomize friction
            randomized_params[f'{link_name}_friction'] = random.uniform(
                self.randomization_ranges['friction_range'][0],
                self.randomization_ranges['friction_range'][1]
            )

        return randomized_params

    def randomize_control_parameters(self):
        """Randomize control-related parameters"""
        control_params = {
            'control_delay': random.uniform(
                self.randomization_ranges['control_delay_range'][0],
                self.randomization_ranges['control_delay_range'][1]
            ),
            'actuator_noise': random.uniform(
                self.randomization_ranges['actuator_noise_range'][0],
                self.randomization_ranges['actuator_noise_range'][1]
            )
        }
        return control_params

    def randomize_sensors(self):
        """Randomize sensor parameters"""
        sensor_params = {
            'noise_level': random.uniform(
                self.randomization_ranges['sensor_noise_range'][0],
                self.randomization_ranges['sensor_noise_range'][1]
            ),
            'bias_range': (
                random.uniform(
                    self.randomization_ranges['sensor_bias_range'][0],
                    0
                ),
                random.uniform(
                    0,
                    self.randomization_ranges['sensor_bias_range'][1]
                )
            ),
            'delay': random.uniform(
                self.randomization_ranges['sensor_delay_range'][0],
                self.randomization_ranges['sensor_delay_range'][1]
            )
        }
        return sensor_params

    def randomize_environment(self):
        """Randomize environmental parameters"""
        env_params = {
            'gravity': random.uniform(
                self.randomization_ranges['gravity_range'][0],
                self.randomization_ranges['gravity_range'][1]
            ),
            'ground_friction': random.uniform(
                self.randomization_ranges['ground_friction_range'][0],
                self.randomization_ranges['ground_friction_range'][1]
            )
        }
        return env_params

    def apply_randomization(self, simulation_env):
        """Apply randomization to the simulation environment"""
        robot_params = self.randomize_robot_properties(simulation_env.robot_model)
        control_params = self.randomize_control_parameters()
        sensor_params = self.randomize_sensors()
        env_params = self.randomize_environment()

        # Apply to simulation
        simulation_env.set_robot_properties(robot_params)
        simulation_env.set_control_parameters(control_params)
        simulation_env.set_sensor_parameters(sensor_params)
        simulation_env.set_environment_parameters(env_params)

        return {
            'robot': robot_params,
            'control': control_params,
            'sensor': sensor_params,
            'environment': env_params
        }

# Usage in training loop
def training_with_domain_randomization():
    randomizer = DomainRandomizer()
    simulation_env = create_simulation_environment()

    for episode in range(1000):  # Training episodes
        # Apply new randomization for each episode
        randomization_params = randomizer.apply_randomization(simulation_env)

        # Train policy with randomized environment
        policy = train_policy(simulation_env)

        # Evaluate on a set of randomized environments
        evaluation_score = evaluate_policy(policy, randomization_params)

        print(f"Episode {episode}, Score: {evaluation_score}")

if __name__ == "__main__":
    training_with_domain_randomization()
```

## System Identification for Model Improvement

```python
#!/usr/bin/env python3
"""
System identification for improving humanoid robot simulation models
"""
import numpy as np
from scipy.optimize import minimize
from scipy.integrate import odeint
import matplotlib.pyplot as plt

class SystemIdentifier:
    def __init__(self):
        self.model_params = {}
        self.identification_data = []

    def collect_identification_data(self, robot, input_signal, sample_time=0.01):
        """Collect data for system identification"""
        # Apply input signal and record response
        states = []
        inputs = []
        times = np.arange(0, len(input_signal) * sample_time, sample_time)

        robot.reset()
        for i, u in enumerate(input_signal):
            state = robot.get_state()
            states.append(state.copy())
            inputs.append(u)

            robot.apply_control(u)
            robot.step(sample_time)

        self.identification_data = {
            'time': times,
            'input': np.array(inputs),
            'output': np.array(states)
        }

        return self.identification_data

    def model_error_function(self, params, target_output):
        """Calculate error between model prediction and target"""
        # Example: simple second-order system model
        # This would be replaced with a humanoid-specific model
        def system_model(y, t, params):
            # Example: mass-spring-damper system
            m, c, k = params
            x, v = y
            dxdt = v
            dvdt = (-c * v - k * x) / m
            return [dxdt, dvdt]

        initial_state = self.identification_data['output'][0][:2]  # x, v
        solution = odeint(system_model, initial_state,
                         self.identification_data['time'], args=(params,))

        predicted_output = solution[:, 0]  # Position
        target = target_output

        # Calculate error
        error = np.mean((predicted_output - target) ** 2)
        return error

    def identify_parameters(self, initial_guess, target_output):
        """Identify system parameters using optimization"""
        result = minimize(
            self.model_error_function,
            initial_guess,
            args=(target_output),
            method='BFGS'
        )

        self.model_params = {
            'identified_params': result.x,
            'error': result.fun,
            'success': result.success
        }

        return self.model_params

    def update_simulation_model(self, sim_robot):
        """Update simulation model with identified parameters"""
        if self.model_params['success']:
            # Apply identified parameters to simulation
            sim_robot.update_dynamics_parameters(self.model_params['identified_params'])
            print(f"Updated simulation with identified parameters: {self.model_params['identified_params']}")
        else:
            print("Parameter identification failed")

# Example: Identifying parameters for a simple joint
def example_joint_identification():
    # Simulate collecting data from a real joint
    def real_joint_dynamics(t, u, noise_level=0.01):
        # Simulate real joint with unknown parameters
        m, c, k = 1.2, 0.5, 10.0  # Unknown real parameters
        x, v = 0, 0  # Initial state

        dt = 0.01
        trajectory = []

        for i in range(len(u)):
            # Simple integration of mass-spring-damper
            a = (u[i] - c * v - k * x) / m
            v += a * dt
            x += v * dt
            trajectory.append([x + np.random.normal(0, noise_level),
                              v + np.random.normal(0, noise_level*0.1)])

        return np.array(trajectory)

    # Generate input signal
    t = np.linspace(0, 10, 1000)
    input_signal = np.sin(2 * np.pi * 0.5 * t) + 0.5 * np.sin(2 * np.pi * 2 * t)  # Multi-frequency input

    # Collect "real" data
    real_trajectory = real_joint_dynamics(t, input_signal)

    # Set up system identifier
    identifier = SystemIdentifier()

    # Create mock identification data
    identifier.identification_data = {
        'time': t,
        'input': input_signal,
        'output': real_trajectory
    }

    # Identify parameters (initial guess: [m, c, k])
    initial_guess = [1.0, 0.3, 8.0]
    target_output = real_trajectory[:, 0]  # Just the position for this example

    identified_params = identifier.identify_parameters(initial_guess, target_output)
    print(f"Identified parameters: {identified_params}")

    return identifier

if __name__ == "__main__":
    identifier = example_joint_identification()
```

## Robust Control Techniques

```python
#!/usr/bin/env python3
"""
Robust control techniques for handling sim-to-real gap
"""
import numpy as np
from scipy.linalg import solve_continuous_are
from scipy import signal

class RobustController:
    def __init__(self, nominal_model, uncertainty_bounds):
        self.nominal_A = nominal_model['A']
        self.nominal_B = nominal_model['B']
        self.nominal_C = nominal_model['C']
        self.uncertainty_bounds = uncertainty_bounds

        # Design nominal LQR controller
        self.Q = np.eye(nominal_model['A'].shape[0])  # State cost
        self.R = np.eye(nominal_model['B'].shape[1])  # Control cost
        self.K_nominal = self.design_lqr_controller()

    def design_lqr_controller(self):
        """Design LQR controller for nominal system"""
        # Solve Riccati equation
        P = solve_continuous_are(self.nominal_A, self.nominal_B, self.Q, self.R)

        # Calculate feedback gain
        K = np.linalg.inv(self.R) @ self.nominal_B.T @ P
        return K

    def adaptive_control_update(self, state_error, control_error, learning_rate=0.01):
        """Update control parameters based on real-time error"""
        # Simple parameter adaptation law
        # This is a basic example - real implementations would be more sophisticated
        adaptation_term = learning_rate * np.outer(control_error, state_error)

        # Update control gain
        self.K_adaptive = self.K_nominal + adaptation_term

        return self.K_adaptive

    def robust_control_law(self, state, reference, uncertainty_estimate=None):
        """Robust control law with uncertainty compensation"""
        error = reference - state

        # Nominal control
        u_nominal = self.K_nominal @ error

        # Uncertainty compensation if estimate available
        if uncertainty_estimate is not None:
            u_uncertainty = self.uncertainty_compensation(state, uncertainty_estimate)
        else:
            u_uncertainty = np.zeros_like(u_nominal)

        # Total control
        u_total = u_nominal + u_uncertainty

        return u_total

    def uncertainty_compensation(self, state, uncertainty_estimate):
        """Compensate for model uncertainty"""
        # This would depend on the specific uncertainty structure
        # Example: matched uncertainty compensation
        compensation = -np.linalg.pinv(self.nominal_B) @ uncertainty_estimate
        return compensation

    def sliding_mode_control(self, state, reference, switching_gain=10.0):
        """Sliding mode control for robustness"""
        error = reference - state
        s = error  # Simple sliding surface (could be more complex)

        # Sliding mode control law
        u_sm = switching_gain * np.sign(s)

        return u_sm

class HumanoidBalanceController:
    def __init__(self):
        # Simplified humanoid model for balance control
        # State: [x_pos, x_vel, pitch_angle, pitch_vel]
        self.A = np.array([
            [0, 1, 0, 0],
            [0, -b_x, 0, 0],  # Damping term
            [0, 0, 0, 1],
            [0, 0, -g/l, -b_theta]  # Pendulum dynamics
        ])

        # Control input: [force_x, torque_pitch]
        self.B = np.array([
            [0, 0],
            [1/m, 0],
            [0, 0],
            [0, 1/J]
        ])

        self.C = np.eye(4)  # Full state feedback

        # Robust controller
        self.robust_ctrl = RobustController(
            {'A': self.A, 'B': self.B, 'C': self.C},
            uncertainty_bounds={'process': 0.1, 'measurement': 0.05}
        )

    def balance_control(self, current_state, desired_state):
        """Balance control with robustness"""
        # Calculate robust control input
        u_robust = self.robust_ctrl.robust_control_law(current_state, desired_state)

        # Add sliding mode for extra robustness
        u_sm = self.robust_ctrl.sliding_mode_control(current_state, desired_state)

        # Combined control
        u_total = u_robust + 0.1 * u_sm  # Weighted combination

        return u_total

# H-infinity controller design
def h_infinity_design(A, B1, B2, C1, C2, gamma=1.0):
    """
    Design H-infinity controller to minimize effect of disturbances
    A: system matrix
    B1: disturbance input matrix
    B2: control input matrix
    C1: performance output matrix
    C2: measurement output matrix
    gamma: performance bound
    """
    # This is a simplified version - full H-infinity design is complex
    # In practice, use specialized control toolboxes like python-control

    try:
        from control import hinf
        # Use python-control package for full H-infinity synthesis
        # K, CL, gam = hinf.synthesize(A, B1, B2, C1, C2, gamma)
        pass
    except ImportError:
        print("Python-control package not available for full H-infinity design")
        # Return LQR as approximation
        return solve_lqr_approximation(A, B2, C1, C2)

def solve_lqr_approximation(A, B, Q, R):
    """LQR as approximation to H-infinity"""
    P = solve_continuous_are(A, B, Q.T @ Q, R)
    K = np.linalg.inv(R) @ B.T @ P
    return K
```

## Reality Gap Bridging Techniques

### Curriculum Learning

```python
#!/usr/bin/env python3
"""
Curriculum learning approach to bridge sim-to-real gap
"""
import numpy as np
import random

class CurriculumSim2Real:
    def __init__(self):
        self.curriculum_levels = [
            {
                'name': 'Basic Movement',
                'complexity': 0.1,
                'tasks': ['simple_walk', 'balance_stand'],
                'similarity_to_real': 0.3
            },
            {
                'name': 'Simple Obstacles',
                'complexity': 0.3,
                'tasks': ['walk_over_small_gap', 'simple_turn'],
                'similarity_to_real': 0.5
            },
            {
                'name': 'Dynamic Environments',
                'complexity': 0.6,
                'tasks': ['walk_on_slight_incline', 'avoid_moving_obstacles'],
                'similarity_to_real': 0.7
            },
            {
                'name': 'Realistic Conditions',
                'complexity': 0.9,
                'tasks': ['full_realistic_task'],
                'similarity_to_real': 0.9
            }
        ]

        self.current_level = 0
        self.performance_history = []

    def evaluate_performance(self, policy, level):
        """Evaluate policy performance at current curriculum level"""
        # This would run evaluation in simulation with current level parameters
        # Return performance score (0-1)
        score = np.random.random()  # Placeholder
        return score

    def should_advance_level(self, current_score, threshold=0.8):
        """Determine if we should advance to next curriculum level"""
        if current_score >= threshold:
            return True
        return False

    def adjust_simulation_parameters(self, level_idx):
        """Adjust simulation to match current curriculum level"""
        level = self.curriculum_levels[level_idx]

        # Example parameter adjustments
        params = {
            'noise_level': level['complexity'] * 0.1,
            'friction_range': (0.3, 0.3 + level['complexity'] * 0.4),
            'control_delay': level['complexity'] * 0.02,
            'model_uncertainty': level['complexity'] * 0.15
        }

        return params

    def run_curriculum_training(self):
        """Run full curriculum training process"""
        for episode in range(1000):
            # Get current level parameters
            sim_params = self.adjust_simulation_parameters(self.current_level)

            # Train on current level
            policy = self.train_on_level(self.current_level, sim_params)

            # Evaluate performance
            score = self.evaluate_performance(policy, self.current_level)
            self.performance_history.append(score)

            # Check if we can advance to next level
            if (self.current_level < len(self.curriculum_levels) - 1 and
                self.should_advance_level(score)):

                self.current_level += 1
                print(f"Advancing to curriculum level: {self.curriculum_levels[self.current_level]['name']}")

            # Periodically test on real-like conditions
            if episode % 50 == 0:
                real_score = self.test_on_realistic_conditions(policy)
                print(f"Episode {episode}, Current Level: {self.current_level}, Score: {score:.3f}, Real-like Score: {real_score:.3f}")

    def train_on_level(self, level_idx, sim_params):
        """Train policy on specific curriculum level"""
        # This would implement actual training (e.g., PPO, SAC)
        # For now, return a placeholder policy
        return lambda state: np.random.random(6)  # Example action

    def test_on_realistic_conditions(self, policy):
        """Test policy under conditions similar to real world"""
        # Set simulation parameters to be very close to real
        realistic_params = {
            'noise_level': 0.08,
            'friction_range': (0.4, 0.7),
            'control_delay': 0.015,
            'model_uncertainty': 0.12
        }

        # Evaluate policy with these parameters
        score = np.random.random()  # Placeholder
        return score

# Usage
curriculum = CurriculumSim2Real()
# curriculum.run_curriculum_training()  # Uncomment to run
```

### Transfer Learning with Domain Adaptation

```python
#!/usr/bin/env python3
"""
Domain adaptation techniques for sim-to-real transfer
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class DomainAdaptationNetwork(nn.Module):
    def __init__(self, input_dim, feature_dim=256):
        super(DomainAdaptationNetwork, self).__init__()

        # Feature extractor
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, feature_dim),
            nn.ReLU()
        )

        # Task-specific classifier
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # Example: single output
        )

        # Domain classifier (for domain adversarial training)
        self.domain_classifier = nn.Sequential(
            nn.Linear(feature_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 2)  # 2 domains: sim and real
        )

    def forward(self, x, domain_label=None):
        features = self.feature_extractor(x)

        task_output = self.classifier(features)

        if domain_label is not None:
            # Also return domain classification for adversarial training
            domain_output = self.domain_classifier(features.detach())  # Detach to stop gradient
            return task_output, domain_output
        else:
            return task_output

class DomainAdversarialLoss(nn.Module):
    def __init__(self):
        super(DomainAdversarialLoss, self).__init__()
        self.task_criterion = nn.MSELoss()
        self.domain_criterion = nn.CrossEntropyLoss()

    def forward(self, task_pred, task_target, domain_pred, domain_target, lambda_reg=0.1):
        # Task loss
        task_loss = self.task_criterion(task_pred, task_target)

        # Domain loss (we want to fool the domain classifier)
        domain_loss = self.domain_criterion(domain_pred, domain_target)

        # Total loss: minimize task loss, maximize domain confusion
        total_loss = task_loss - lambda_reg * domain_loss

        return total_loss, task_loss, domain_loss

def train_domain_adaptation(sim_data, real_data, epochs=100):
    """Train with domain adaptation"""
    model = DomainAdaptationNetwork(input_dim=sim_data.shape[1])
    domain_loss_fn = DomainAdversarialLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        # Combine sim and real data
        all_data = torch.cat([
            torch.FloatTensor(sim_data),
            torch.FloatTensor(real_data)
        ], dim=0)

        # Create domain labels (0 for sim, 1 for real)
        domain_labels = torch.cat([
            torch.zeros(sim_data.shape[0]),
            torch.ones(real_data.shape[0])
        ], dim=0).long()

        # Task targets (example: some function of the input)
        task_targets = torch.sum(all_data, dim=1, keepdim=True)  # Example target

        optimizer.zero_grad()

        # Forward pass
        task_pred, domain_pred = model(all_data, domain_labels)

        # Compute losses
        total_loss, task_loss, domain_loss = domain_loss_fn(
            task_pred, task_targets, domain_pred, domain_labels
        )

        total_loss.backward()
        optimizer.step()

        if epoch % 20 == 0:
            print(f"Epoch {epoch}, Task Loss: {task_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}")

# Example usage
def example_domain_adaptation():
    # Generate example simulation and real data
    sim_data = np.random.randn(1000, 10) * 0.5  # Simulation data
    real_data = np.random.randn(500, 10) * 0.6 + 0.1  # Real data (slightly different distribution)

    train_domain_adaptation(sim_data, real_data)

if __name__ == "__main__":
    example_domain_adaptation()
```

## Validation and Testing Strategies

### Systematic Validation Approach

```python
#!/usr/bin/env python3
"""
Validation framework for sim-to-real transfer
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import json

class Sim2RealValidator:
    def __init__(self):
        self.validation_results = {
            'sim_performance': [],
            'real_performance': [],
            'gap_metrics': [],
            'transfer_success': []
        }

    def run_validation_test(self, policy, sim_env, real_env, test_scenarios):
        """Run comprehensive validation across scenarios"""
        results = {}

        for scenario in test_scenarios:
            # Test in simulation
            sim_result = self.evaluate_policy(policy, sim_env, scenario)

            # Test in reality (or high-fidelity sim)
            real_result = self.evaluate_policy(policy, real_env, scenario)

            # Calculate gap metrics
            gap_metrics = self.calculate_gap_metrics(sim_result, real_result)

            results[scenario['name']] = {
                'sim': sim_result,
                'real': real_result,
                'gap': gap_metrics
            }

        return results

    def evaluate_policy(self, policy, env, scenario):
        """Evaluate policy in given environment and scenario"""
        # Reset environment to scenario initial conditions
        env.reset(scenario['initial_state'])

        total_reward = 0
        trajectory = []
        max_steps = scenario.get('max_steps', 1000)

        for step in range(max_steps):
            # Get current state
            state = env.get_state()

            # Get action from policy
            action = policy(state)

            # Apply action
            next_state, reward, done, info = env.step(action)
            total_reward += reward
            trajectory.append({
                'state': state,
                'action': action,
                'reward': reward,
                'done': done
            })

            if done:
                break

        return {
            'total_reward': total_reward,
            'trajectory': trajectory,
            'success_rate': info.get('success_rate', 0),
            'efficiency': info.get('efficiency', 1.0)
        }

    def calculate_gap_metrics(self, sim_result, real_result):
        """Calculate various gap metrics"""
        metrics = {}

        # Performance gap
        metrics['reward_gap'] = abs(sim_result['total_reward'] - real_result['total_reward'])
        metrics['reward_ratio'] = real_result['total_reward'] / (sim_result['total_reward'] + 1e-8)

        # Success rate gap
        metrics['success_gap'] = abs(sim_result['success_rate'] - real_result['success_rate'])

        # Efficiency gap
        metrics['efficiency_gap'] = abs(sim_result['efficiency'] - real_result['efficiency'])

        # Trajectory similarity (simplified)
        if len(sim_result['trajectory']) > 0 and len(real_result['trajectory']) > 0:
            sim_states = np.array([t['state'] for t in sim_result['trajectory']])
            real_states = np.array([t['state'] for t in real_result['trajectory']])

            # Align trajectories (take minimum length)
            min_len = min(len(sim_states), len(real_states))
            sim_states = sim_states[:min_len]
            real_states = real_states[:min_len]

            state_mse = np.mean((sim_states - real_states) ** 2)
            metrics['trajectory_mse'] = state_mse
        else:
            metrics['trajectory_mse'] = float('inf')

        # Overall gap score (weighted combination)
        metrics['overall_gap'] = (
            0.3 * metrics['reward_gap'] +
            0.3 * metrics['success_gap'] +
            0.2 * metrics['efficiency_gap'] +
            0.2 * metrics['trajectory_mse']
        )

        return metrics

    def generate_validation_report(self, validation_results):
        """Generate comprehensive validation report"""
        report = {
            'summary': {},
            'detailed_results': validation_results,
            'recommendations': []
        }

        # Calculate overall statistics
        gap_values = [result['gap']['overall_gap'] for result in validation_results.values()]

        report['summary'] = {
            'mean_gap': np.mean(gap_values),
            'std_gap': np.std(gap_values),
            'max_gap': np.max(gap_values),
            'min_gap': np.min(gap_values),
            'success_rate': np.mean([result['gap']['reward_ratio'] for result in validation_results.values()])
        }

        # Generate recommendations based on gap size
        mean_gap = report['summary']['mean_gap']
        if mean_gap < 0.1:
            report['recommendations'].append("Gap is small - policy should transfer well to real robot")
        elif mean_gap < 0.3:
            report['recommendations'].append("Moderate gap - consider additional domain randomization")
        else:
            report['recommendations'].append("Large gap - significant sim-to-real work needed")

        return report

    def visualize_validation_results(self, validation_results):
        """Create visualizations of validation results"""
        scenarios = list(validation_results.keys())
        sim_rewards = [validation_results[s]['sim']['total_reward'] for s in scenarios]
        real_rewards = [validation_results[s]['real']['total_reward'] for s in scenarios]
        gaps = [validation_results[s]['gap']['overall_gap'] for s in scenarios]

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Reward comparison
        x = np.arange(len(scenarios))
        width = 0.35
        axes[0, 0].bar(x - width/2, sim_rewards, width, label='Simulation', alpha=0.8)
        axes[0, 0].bar(x + width/2, real_rewards, width, label='Reality', alpha=0.8)
        axes[0, 0].set_xlabel('Scenario')
        axes[0, 0].set_ylabel('Total Reward')
        axes[0, 0].set_title('Sim vs Real Performance')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(scenarios, rotation=45)
        axes[0, 0].legend()

        # Gap metrics
        axes[0, 1].bar(scenarios, gaps)
        axes[0, 1].set_xlabel('Scenario')
        axes[0, 1].set_ylabel('Gap Metric')
        axes[0, 1].set_title('Sim-to-Real Gap by Scenario')
        axes[0, 1].tick_params(axis='x', rotation=45)

        # Success rate comparison
        sim_success = [validation_results[s]['sim']['success_rate'] for s in scenarios]
        real_success = [validation_results[s]['real']['success_rate'] for s in scenarios]
        axes[1, 0].plot(scenarios, sim_success, 'o-', label='Simulation', linewidth=2)
        axes[1, 0].plot(scenarios, real_success, 's-', label='Reality', linewidth=2)
        axes[1, 0].set_xlabel('Scenario')
        axes[1, 0].set_ylabel('Success Rate')
        axes[1, 0].set_title('Success Rate Comparison')
        axes[1, 0].legend()
        axes[1, 0].tick_params(axis='x', rotation=45)

        # Reward ratio
        reward_ratios = [validation_results[s]['gap']['reward_ratio'] for s in scenarios]
        axes[1, 1].bar(scenarios, reward_ratios)
        axes[1, 1].axhline(y=1.0, color='r', linestyle='--', label='Perfect Transfer')
        axes[1, 1].set_xlabel('Scenario')
        axes[1, 1].set_ylabel('Real/Sim Reward Ratio')
        axes[1, 1].set_title('Transfer Efficiency')
        axes[1, 1].legend()
        axes[1, 1].tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('sim2real_validation_report.png', dpi=300, bbox_inches='tight')
        plt.show()

# Example test scenarios
def create_test_scenarios():
    scenarios = [
        {
            'name': 'balance_still',
            'initial_state': [0, 0, 0, 0],  # [x_pos, x_vel, pitch, pitch_vel]
            'max_steps': 500,
            'task': 'maintain_balance'
        },
        {
            'name': 'slow_walk',
            'initial_state': [0, 0, 0, 0],
            'max_steps': 1000,
            'task': 'walk_forward_slowly'
        },
        {
            'name': 'turn_in_place',
            'initial_state': [0, 0, 0, 0],
            'max_steps': 800,
            'task': 'rotate_in_place'
        }
    ]
    return scenarios

if __name__ == "__main__":
    validator = Sim2RealValidator()
    scenarios = create_test_scenarios()

    # This would normally connect to actual sim/real environments
    # For demo, we'll use placeholders
    print("Sim-to-Real validation framework initialized")
    print(f"Test scenarios: {[s['name'] for s in scenarios]}")
```

## Best Practices for Minimizing the Gap

### Comprehensive Best Practices Guide

```python
class Sim2RealBestPractices:
    """Collection of best practices for minimizing sim-to-real gap"""

    def __init__(self):
        self.practices = {
            'modeling': [],
            'sensor_simulation': [],
            'control_design': [],
            'validation': []
        }

    def modeling_best_practices(self):
        """Best practices for accurate modeling"""
        practices = [
            "Use system identification to determine real robot parameters",
            "Include flexible joint models and gear backlash",
            "Model actuator dynamics and limitations accurately",
            "Account for structural compliance in robot frame",
            "Include thermal effects on motor performance",
            "Model sensor mounting compliance and vibrations"
        ]
        return practices

    def sensor_simulation_practices(self):
        """Best practices for sensor simulation"""
        practices = [
            "Add realistic noise models based on real sensor specifications",
            "Include sensor delays and update rates",
            "Model sensor saturation and dead zones",
            "Simulate environmental interference effects",
            "Include sensor-to-sensor calibration errors",
            "Model time synchronization issues between sensors"
        ]
        return practices

    def control_design_practices(self):
        """Best practices for robust control design"""
        practices = [
            "Design controllers with stability margins for model uncertainty",
            "Use adaptive control to handle parameter variations",
            "Implement robust control techniques (H-infinity, mu-synthesis)",
            "Include integral action for disturbance rejection",
            "Design controllers with anti-windup for actuator limits",
            "Use sensor fusion to reduce reliance on single sensors"
        ]
        return practices

    def validation_practices(self):
        """Best practices for validation"""
        practices = [
            "Test on multiple robot instances to account for hardware variation",
            "Validate across different environmental conditions",
            "Use progressively more realistic simulation conditions",
            "Implement continuous monitoring of key performance indicators",
            "Create standardized test scenarios for comparison",
            "Document all assumptions and limitations"
        ]
        return practices

def print_best_practices():
    """Print comprehensive best practices"""
    practices = Sim2RealBestPractices()

    print("=== SIM-TO-REAL BEST PRACTICES ===\n")

    print("1. MODELING BEST PRACTICES:")
    for i, practice in enumerate(practices.modeling_best_practices(), 1):
        print(f"   {i}. {practice}")

    print("\n2. SENSOR SIMULATION BEST PRACTICES:")
    for i, practice in enumerate(practices.sensor_simulation_practices(), 1):
        print(f"   {i}. {practice}")

    print("\n3. CONTROL DESIGN BEST PRACTICES:")
    for i, practice in enumerate(practices.control_design_practices(), 1):
        print(f"   {i}. {practice}")

    print("\n4. VALIDATION BEST PRACTICES:")
    for i, practice in enumerate(practices.validation_practices(), 1):
        print(f"   {i}. {practice}")

if __name__ == "__main__":
    print_best_practices()
```

## Summary

The sim-to-real gap remains one of the most challenging aspects of humanoid robotics development. Successfully bridging this gap requires a multi-faceted approach combining:

1. **Accurate Modeling**: Using system identification and domain randomization to create more realistic simulation models
2. **Robust Control**: Implementing controllers that can handle uncertainty and disturbances
3. **Progressive Validation**: Systematically testing and validating across increasing levels of realism
4. **Continuous Monitoring**: Tracking performance metrics and adapting to changing conditions

Modern approaches like domain adaptation, curriculum learning, and advanced system identification techniques are making the gap more manageable, but it remains an active area of research. The key is to view sim-to-real not as a problem to be solved once, but as an ongoing challenge that requires continuous attention throughout the robot development lifecycle.