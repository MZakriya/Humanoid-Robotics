# 1.5 Sensorimotor Learning Principles

## Introduction

Sensorimotor learning represents a fundamental aspect of embodied intelligence, where agents acquire skills and behaviors through direct interaction with their environment. Unlike traditional machine learning approaches that rely on passive data processing, sensorimotor learning occurs through active exploration and continuous interaction between an agent's sensors and actuators. This learning paradigm is essential for creating adaptive, robust, and efficient embodied systems that can operate effectively in complex, dynamic environments.

## Theoretical Foundations of Sensorimotor Learning

### Active Learning and Exploration

Sensorimotor learning is fundamentally based on the principle of active learning, where the agent's actions directly influence the sensory information it receives. This creates a closed-loop learning process where:

1. **Action-Dependent Perception**: The agent's movements determine what sensory information is available
2. **Exploratory Behavior**: Agents actively explore their sensorimotor space to discover patterns and relationships
3. **Self-Supervised Learning**: Learning occurs through the agent's own interactions rather than external supervision

### The Sensorimotor Contingency Framework

The sensorimotor contingency theory provides a formal framework for understanding how agents learn through interaction:

```
ΔS = f(ΔM, E, S)
```

Where:
- `ΔS`: Change in sensory input
- `ΔM`: Change in motor command
- `E`: Environmental state
- `S`: Current sensory state
- `f`: The contingency function

Agents learn these contingencies through repeated interactions, building internal models of how their actions affect sensory input.

### Morphological Computation in Learning

Morphological computation refers to the idea that the physical properties of an agent's body contribute to computation, reducing the cognitive load on the controller. This principle affects learning by:

- **Exploiting Passive Dynamics**: Using body properties to achieve behaviors with minimal active control
- **Reducing Control Complexity**: Leveraging physical properties to simplify control requirements
- **Enhancing Robustness**: Creating behaviors that are naturally stable due to physical properties

## Key Learning Mechanisms

### 1. Reinforcement Learning in Sensorimotor Contexts

Reinforcement learning (RL) in embodied systems focuses on learning policies that map sensorimotor states to actions to maximize cumulative reward:

```python
import numpy as np
import random
from typing import Dict, List, Tuple, Any, Optional
import torch
import torch.nn as nn
import torch.optim as optim

class DeepSensorimotorReinforcementLearner:
    """
    Deep reinforcement learning for sensorimotor tasks with continuous state-action spaces.
    """

    def __init__(self, sensor_dim: int, motor_dim: int, hidden_dim: int = 256,
                 learning_rate: float = 0.001, gamma: float = 0.99,
                 tau: float = 0.005, buffer_size: int = 1000000):
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim
        self.gamma = gamma  # Discount factor
        self.tau = tau      # Soft update parameter
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Actor network (policy)
        self.actor = ActorNetwork(sensor_dim, motor_dim, hidden_dim).to(self.device)
        self.actor_target = ActorNetwork(sensor_dim, motor_dim, hidden_dim).to(self.device)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)

        # Critic network (Q-function)
        self.critic = CriticNetwork(sensor_dim, motor_dim, hidden_dim).to(self.device)
        self.critic_target = CriticNetwork(sensor_dim, motor_dim, hidden_dim).to(self.device)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)

        # Experience replay buffer
        self.buffer = ExperienceReplay(buffer_size, sensor_dim, motor_dim)

        # Initialize target networks
        self.hard_update(self.actor_target, self.actor)
        self.hard_update(self.critic_target, self.critic)

    def select_action(self, sensor_state: np.ndarray, add_noise: bool = True) -> np.ndarray:
        """
        Select action based on current sensor state.
        """
        state_tensor = torch.FloatTensor(sensor_state).unsqueeze(0).to(self.device)
        action = self.actor(state_tensor).cpu().data.numpy().flatten()

        if add_noise:
            noise = np.random.normal(0, 0.1, size=self.motor_dim)
            action = np.clip(action + noise, -1, 1)

        return action

    def train(self, batch_size: int = 128) -> Optional[Dict[str, float]]:
        """
        Train the networks using a batch of experiences.
        """
        if len(self.buffer) < batch_size:
            return None

        # Sample batch from replay buffer
        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.FloatTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.BoolTensor(dones).unsqueeze(1).to(self.device)

        # Critic update
        with torch.no_grad():
            next_actions = self.actor_target(next_states)
            next_q_values = self.critic_target(next_states, next_actions)
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        current_q_values = self.critic(states, actions)
        critic_loss = nn.MSELoss()(current_q_values, target_q_values)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor update
        predicted_actions = self.actor(states)
        actor_loss = -self.critic(states, predicted_actions).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Soft update target networks
        self.soft_update(self.actor_target, self.actor)
        self.soft_update(self.critic_target, self.critic)

        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item()
        }

    def soft_update(self, target_net: nn.Module, source_net: nn.Module) -> None:
        """
        Soft update target network parameters.
        """
        for target_param, param in zip(target_net.parameters(), source_net.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)

    def hard_update(self, target_net: nn.Module, source_net: nn.Module) -> None:
        """
        Hard update target network parameters.
        """
        target_net.load_state_dict(source_net.state_dict())

class ActorNetwork(nn.Module):
    """
    Actor network for the DDPG algorithm.
    """
    def __init__(self, sensor_dim: int, motor_dim: int, hidden_dim: int):
        super(ActorNetwork, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(sensor_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, motor_dim),
            nn.Tanh()  # Output actions in [-1, 1]
        )

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.network(state)

class CriticNetwork(nn.Module):
    """
    Critic network for the DDPG algorithm.
    """
    def __init__(self, sensor_dim: int, motor_dim: int, hidden_dim: int):
        super(CriticNetwork, self).__init__()

        self.state_net = nn.Sequential(
            nn.Linear(sensor_dim, hidden_dim),
            nn.ReLU()
        )

        self.action_net = nn.Sequential(
            nn.Linear(motor_dim, hidden_dim),
            nn.ReLU()
        )

        self.q_net = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        state_features = self.state_net(state)
        action_features = self.action_net(action)
        q_input = torch.cat([state_features, action_features], dim=1)
        return self.q_net(q_input)

class ExperienceReplay:
    """
    Experience replay buffer for off-policy learning.
    """
    def __init__(self, capacity: int, sensor_dim: int, motor_dim: int):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def push(self, state: np.ndarray, action: np.ndarray,
             reward: float, next_state: np.ndarray, done: bool) -> None:
        """
        Add experience to the buffer.
        """
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)

        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size: int) -> Tuple[np.ndarray, ...]:
        """
        Sample a batch of experiences.
        """
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done

    def __len__(self) -> int:
        return len(self.buffer)
```

### 2. Imitation Learning and Social Learning

Imitation learning enables agents to acquire sensorimotor skills by observing and replicating demonstrated behaviors:

```python
class ImitationLearningSystem:
    """
    System for learning sensorimotor behaviors through imitation.
    """

    def __init__(self, sensor_dim: int, motor_dim: int, hidden_dim: int = 256):
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Behavioral cloning network
        self.behavioral_net = BehavioralCloningNetwork(sensor_dim, motor_dim, hidden_dim).to(self.device)
        self.optimizer = optim.Adam(self.behavioral_net.parameters(), lr=0.001)
        self.criterion = nn.MSELoss()

    def behavioral_cloning_train(self, expert_states: np.ndarray,
                               expert_actions: np.ndarray,
                               epochs: int = 100) -> List[float]:
        """
        Train using behavioral cloning (supervised learning from expert demonstrations).
        """
        states_tensor = torch.FloatTensor(expert_states).to(self.device)
        actions_tensor = torch.FloatTensor(expert_actions).to(self.device)

        losses = []
        for epoch in range(epochs):
            self.optimizer.zero_grad()
            predicted_actions = self.behavioral_net(states_tensor)
            loss = self.criterion(predicted_actions, actions_tensor)
            loss.backward()
            self.optimizer.step()
            losses.append(loss.item())

        return losses

    def predict_action(self, sensor_state: np.ndarray) -> np.ndarray:
        """
        Predict action based on current sensor state using learned policy.
        """
        state_tensor = torch.FloatTensor(sensor_state).unsqueeze(0).to(self.device)
        action = self.behavioral_net(state_tensor).cpu().data.numpy().flatten()
        return action

class BehavioralCloningNetwork(nn.Module):
    """
    Network for behavioral cloning.
    """
    def __init__(self, sensor_dim: int, motor_dim: int, hidden_dim: int):
        super(BehavioralCloningNetwork, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(sensor_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, motor_dim)
        )

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        return self.network(state)
```

### 3. Self-Supervised Learning

Self-supervised learning enables agents to learn from sensorimotor patterns without external supervision:

```python
class SelfSupervisedSensorimotorLearner:
    """
    System for self-supervised learning of sensorimotor representations.
    """

    def __init__(self, sensor_dim: int, motor_dim: int, representation_dim: int = 128):
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim
        self.representation_dim = representation_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Encoder for sensorimotor representations
        self.encoder = SensorimotorEncoder(sensor_dim, motor_dim, representation_dim).to(self.device)

        # Predictive model for next state prediction
        self.predictor = NextStatePredictor(representation_dim, motor_dim, sensor_dim).to(self.device)

        self.optimizer = optim.Adam(list(self.encoder.parameters()) +
                                  list(self.predictor.parameters()), lr=0.001)
        self.criterion = nn.MSELoss()

    def predict_next_state(self, current_state: torch.Tensor,
                          motor_command: torch.Tensor) -> torch.Tensor:
        """
        Predict the next sensory state given current state and motor command.
        """
        representation = self.encoder(current_state, motor_command)
        predicted_next_state = self.predictor(representation, motor_command)
        return predicted_next_state

    def train_predictive_model(self, sensor_sequences: np.ndarray,
                             motor_sequences: np.ndarray,
                             epochs: int = 100) -> List[float]:
        """
        Train the predictive model using self-supervised learning.
        """
        losses = []
        for epoch in range(epochs):
            total_loss = 0
            for i in range(len(sensor_sequences) - 1):
                current_sensor = torch.FloatTensor(sensor_sequences[i]).to(self.device)
                motor_command = torch.FloatTensor(motor_sequences[i]).to(self.device)
                next_sensor = torch.FloatTensor(sensor_sequences[i + 1]).to(self.device)

                predicted_next = self.predict_next_state(current_sensor.unsqueeze(0),
                                                      motor_command.unsqueeze(0))

                loss = self.criterion(predicted_next, next_sensor.unsqueeze(0))

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / max(1, len(sensor_sequences) - 1)
            losses.append(avg_loss)

        return losses

class SensorimotorEncoder(nn.Module):
    """
    Encoder that creates representations from sensorimotor data.
    """
    def __init__(self, sensor_dim: int, motor_dim: int, representation_dim: int):
        super(SensorimotorEncoder, self).__init__()

        self.sensor_encoder = nn.Sequential(
            nn.Linear(sensor_dim, representation_dim),
            nn.ReLU(),
            nn.Linear(representation_dim, representation_dim)
        )

        self.motor_encoder = nn.Sequential(
            nn.Linear(motor_dim, representation_dim),
            nn.ReLU(),
            nn.Linear(representation_dim, representation_dim)
        )

        self.fusion = nn.Sequential(
            nn.Linear(2 * representation_dim, representation_dim),
            nn.ReLU()
        )

    def forward(self, sensor_data: torch.Tensor, motor_data: torch.Tensor) -> torch.Tensor:
        sensor_repr = self.sensor_encoder(sensor_data)
        motor_repr = self.motor_encoder(motor_data)
        combined_repr = torch.cat([sensor_repr, motor_repr], dim=1)
        return self.fusion(combined_repr)

class NextStatePredictor(nn.Module):
    """
    Predicts next state given current representation and motor command.
    """
    def __init__(self, representation_dim: int, motor_dim: int, sensor_dim: int):
        super(NextStatePredictor, self).__init__()

        self.predictor = nn.Sequential(
            nn.Linear(representation_dim + motor_dim, representation_dim),
            nn.ReLU(),
            nn.Linear(representation_dim, representation_dim),
            nn.ReLU(),
            nn.Linear(representation_dim, sensor_dim)
        )

    def forward(self, representation: torch.Tensor, motor_command: torch.Tensor) -> torch.Tensor:
        combined_input = torch.cat([representation, motor_command], dim=1)
        return self.predictor(combined_input)
```

### 4. Developmental Learning and Skill Acquisition

Developmental learning involves gradual skill acquisition through practice and increasing complexity:

```python
class DevelopmentalLearningSystem:
    """
    System for developmental learning with progressive skill acquisition.
    """

    def __init__(self):
        self.skill_primitives = {}  # Basic sensorimotor patterns
        self.skill_compositions = {}  # Combined skills
        self.skill_hierarchy = {}  # Hierarchical skill organization
        self.complexity_level = 0  # Current learning complexity level

    def learn_skill_primitive(self, skill_name: str,
                            sensorimotor_pattern: Dict[str, Any]) -> None:
        """
        Learn a basic sensorimotor skill primitive.
        """
        self.skill_primitives[skill_name] = {
            'pattern': sensorimotor_pattern,
            'proficiency': 0.0,
            'attempts': 0,
            'successes': 0
        }

    def compose_skills(self, composition_name: str,
                      skill_list: List[str],
                      composition_params: Dict[str, Any]) -> None:
        """
        Combine basic skills into more complex behaviors.
        """
        if all(skill in self.skill_primitives for skill in skill_list):
            self.skill_compositions[composition_name] = {
                'skills': skill_list,
                'parameters': composition_params,
                'proficiency': 0.0
            }

    def adaptive_curriculum(self, task_success_rate: float) -> bool:
        """
        Adjust task difficulty based on performance.
        """
        if task_success_rate > 0.8 and self.complexity_level < 5:
            self.complexity_level += 1
            return True  # Increased difficulty
        elif task_success_rate < 0.5 and self.complexity_level > 0:
            self.complexity_level -= 1
            return False  # Decreased difficulty
        return None  # No change

    def evaluate_skill_proficiency(self, skill_name: str,
                                 performance_data: Dict[str, float]) -> float:
        """
        Evaluate and update skill proficiency based on performance.
        """
        if skill_name in self.skill_primitives:
            skill = self.skill_primitives[skill_name]
            skill['attempts'] += 1
            if performance_data.get('success', False):
                skill['successes'] += 1

            proficiency = skill['successes'] / max(1, skill['attempts'])
            skill['proficiency'] = proficiency
            return proficiency
        return 0.0
```

## Advanced Learning Strategies

### Morphological Computation Integration

Leveraging the physical properties of the body to simplify learning:

```python
class MorphologicalComputationLearner:
    """
    Learning system that integrates morphological computation principles.
    """

    def __init__(self, body_properties: Dict[str, float]):
        self.body_properties = body_properties
        self.passive_dynamics_model = self.build_passive_dynamics_model(body_properties)
        self.active_control_strategy = None
        self.computation_balance = 0.5  # Balance between passive and active control

    def build_passive_dynamics_model(self, body_props: Dict[str, float]) -> Dict[str, Any]:
        """
        Build a model of the body's passive dynamics.
        """
        return {
            'stability_properties': self.calculate_stability_properties(body_props),
            'energy_efficiency_factors': self.calculate_energy_factors(body_props),
            'compliance_characteristics': self.calculate_compliance(body_props),
            'resonance_frequencies': self.calculate_resonance(body_props)
        }

    def calculate_stability_properties(self, body_props: Dict[str, float]) -> Dict[str, float]:
        """
        Calculate stability-related properties of the body.
        """
        center_of_mass_height = body_props.get('com_height', 1.0)
        base_of_support = body_props.get('base_support', 0.5)

        return {
            'static_stability': base_of_support / (2 * center_of_mass_height),
            'dynamic_stability': self.estimate_dynamic_stability(body_props)
        }

    def adapt_control_strategy(self, task_requirements: Dict[str, float]) -> None:
        """
        Adapt control strategy based on body properties and task requirements.
        """
        # Determine how much control should be active vs. passive
        if self.body_properties.get('compliant_joints', False):
            self.computation_balance = 0.3  # More reliance on passive dynamics
        elif self.body_properties.get('good_sensory_feedback', False):
            self.computation_balance = 0.7  # More active control possible
        else:
            self.computation_balance = 0.5  # Balanced approach

        # Adjust control parameters based on morphological properties
        self.active_control_strategy = self.design_control_strategy(
            self.passive_dynamics_model,
            task_requirements,
            self.computation_balance
        )

    def design_control_strategy(self, passive_model: Dict[str, Any],
                              task_reqs: Dict[str, float],
                              balance: float) -> Dict[str, Any]:
        """
        Design an appropriate control strategy based on morphological properties.
        """
        strategy = {
            'feedback_gains': self.calculate_feedback_gains(passive_model, task_reqs),
            'reference_trajectories': self.calculate_reference_trajectories(task_reqs),
            'stability_margins': self.calculate_stability_margins(passive_model),
            'energy_efficiency_weights': self.calculate_efficiency_weights(passive_model)
        }
        return strategy

    def calculate_feedback_gains(self, passive_model: Dict[str, Any],
                               task_reqs: Dict[str, float]) -> Dict[str, float]:
        """
        Calculate appropriate feedback gains based on passive dynamics.
        """
        gains = {}

        # Reduce gains for directions where passive dynamics provide stability
        if passive_model['stability_properties']['static_stability'] > 0.5:
            gains['postural_gains'] = 0.3  # Lower gains due to good passive stability
        else:
            gains['postural_gains'] = 0.8  # Higher gains needed for stability

        # Adjust for task requirements
        if task_reqs.get('precision', False):
            gains['precision_gains'] = 1.0
        if task_reqs.get('speed', False):
            gains['speed_gains'] = 0.7

        return gains
```

### Curriculum Learning Implementation

Structured learning with gradually increasing complexity:

```python
class CurriculumLearningSystem:
    """
    System for curriculum-based sensorimotor learning.
    """

    def __init__(self):
        self.current_level = 0
        self.skill_primitives = []
        self.complexity_thresholds = [0.7, 0.8, 0.85, 0.9, 0.95]  # Success thresholds
        self.curriculum = [
            self.level_0_basic_exploration,
            self.level_1_simple_patterns,
            self.level_2_pattern_combinations,
            self.level_3_environmental_adaptation,
            self.level_4_goal_directed_behavior
        ]

    def advance_curriculum(self, current_performance: float) -> bool:
        """
        Determine if the agent should advance to the next curriculum level.
        """
        if (self.current_level < len(self.complexity_thresholds) and
            current_performance >= self.complexity_thresholds[self.current_level]):
            self.current_level += 1
            return True
        return False

    def level_0_basic_exploration(self) -> Dict[str, Any]:
        """
        Basic exploration of sensorimotor space.
        """
        return {
            'task': 'random_exploration',
            'duration': 1000,  # Time steps
            'exploration_noise': 0.5,
            'focus': 'sensorimotor_mapping_discovery'
        }

    def level_1_simple_patterns(self) -> Dict[str, Any]:
        """
        Learning basic sensorimotor patterns.
        """
        return {
            'task': 'simple_reaching',
            'duration': 2000,
            'targets': [(0.5, 0.5), (0.2, 0.8), (0.8, 0.2)],
            'tolerance': 0.1,
            'focus': 'basic_pattern_learning'
        }

    def level_2_pattern_combinations(self) -> Dict[str, Any]:
        """
        Combining simple patterns into more complex behaviors.
        """
        return {
            'task': 'sequence_execution',
            'duration': 3000,
            'sequences': [
                ['reach', 'grasp', 'lift'],
                ['move', 'place', 'release']
            ],
            'tolerance': 0.05,
            'focus': 'pattern_combination'
        }

    def level_3_environmental_adaptation(self) -> Dict[str, Any]:
        """
        Adapting behaviors to different environmental conditions.
        """
        return {
            'task': 'adaptive_manipulation',
            'duration': 4000,
            'environment_changes': ['friction', 'object_weight', 'surface_stability'],
            'tolerance': 0.05,
            'focus': 'environmental_adaptation'
        }

    def level_4_goal_directed_behavior(self) -> Dict[str, Any]:
        """
        Adding goal-oriented components to learned behaviors.
        """
        return {
            'task': 'complex_task_completion',
            'duration': 5000,
            'goals': ['assemble_parts', 'navigate_obstacles', 'collaborate_with_human'],
            'tolerance': 0.02,
            'focus': 'goal_directed_behavior'
        }

    def get_current_task(self) -> Dict[str, Any]:
        """
        Get the current task based on curriculum level.
        """
        return self.curriculum[self.current_level]()
```

## Learning Algorithms and Techniques

### Hebbian Learning for Sensorimotor Integration

Learning through correlation between sensory and motor signals:

```python
class HebbianSensorimotorLearner:
    """
    Hebbian learning for sensorimotor integration.
    """

    def __init__(self, sensor_dim: int, motor_dim: int, learning_rate: float = 0.01):
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim
        self.learning_rate = learning_rate
        self.weights = np.random.normal(0, 0.1, (sensor_dim, motor_dim))
        self.trace_decay = 0.9  # For eligibility traces
        self.eligibility_trace = np.zeros((sensor_dim, motor_dim))

    def update_weights(self, sensory_input: np.ndarray,
                      motor_output: np.ndarray) -> np.ndarray:
        """
        Update weights using Hebbian learning rule.
        """
        # Update eligibility trace
        self.eligibility_trace = (self.trace_decay * self.eligibility_trace +
                                 np.outer(sensory_input, motor_output))

        # Hebbian update: Δw = η * trace * (post - target)
        # For sensorimotor learning, we want to strengthen connections that lead to
        # successful outcomes
        delta_weights = self.learning_rate * self.eligibility_trace
        self.weights += delta_weights

        # Apply weight decay to prevent unbounded growth
        self.weights *= 0.999

        return self.weights

    def predict_motor_output(self, sensory_input: np.ndarray) -> np.ndarray:
        """
        Predict motor output based on sensory input using learned weights.
        """
        return np.tanh(np.dot(sensory_input, self.weights))  # Nonlinear activation

    def oja_learning_rule(self, sensory_input: np.ndarray,
                         motor_output: np.ndarray) -> np.ndarray:
        """
        Apply Oja's learning rule (normalized Hebbian learning).
        """
        # Oja's rule: Δw = η * (x*y - w*y²)
        # This prevents unbounded weight growth
        delta_weights = (self.learning_rate *
                        (np.outer(sensory_input, motor_output) -
                         self.weights * (motor_output ** 2)))

        self.weights += delta_weights
        return self.weights
```

### Homeostatic Plasticity for Stable Learning

Maintaining stable sensorimotor patterns while allowing adaptation:

```python
class HomeostaticSensorimotorLearner:
    """
    Homeostatic plasticity for maintaining stable sensorimotor patterns.
    """

    def __init__(self, sensor_dim: int, motor_dim: int,
                 target_activity: float = 0.5,
                 homeostatic_rate: float = 0.001):
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim
        self.target_activity = target_activity
        self.homeostatic_rate = homeostatic_rate

        # Initialize weights
        self.weights = np.random.normal(0, 0.1, (sensor_dim, motor_dim))

        # Track neural activities for homeostatic regulation
        self.neural_activities = np.full(motor_dim, target_activity)
        self.activity_averages = np.full(motor_dim, target_activity)
        self.activity_time_constants = np.full(motor_dim, 1000)  # Time steps

    def homeostatic_update(self, current_activities: np.ndarray,
                          time_step: int) -> np.ndarray:
        """
        Update weights based on homeostatic principles.
        """
        # Update activity averages using exponential moving average
        alpha = 1.0 / self.activity_time_constants
        self.activity_averages = ((1 - alpha) * self.activity_averages +
                                 alpha * current_activities)

        # Calculate homeostatic errors
        homeostatic_errors = self.target_activity - self.activity_averages

        # Adjust weights to reduce homeostatic errors
        for i in range(self.motor_dim):
            if abs(homeostatic_errors[i]) > 0.01:  # Only adjust if significant error
                # Reduce weights if activity is too high, increase if too low
                self.weights[:, i] *= (1 + self.homeostatic_rate * homeostatic_errors[i])

        return self.weights

    def update_with_homeostasis(self, sensory_input: np.ndarray,
                               motor_output: np.ndarray,
                               time_step: int) -> np.ndarray:
        """
        Update weights with both Hebbian and homeostatic components.
        """
        # Hebbian update
        hebbian_update = np.outer(sensory_input, motor_output) * 0.01
        self.weights += hebbian_update

        # Homeostatic update
        self.homeostatic_update(motor_output, time_step)

        # Normalize weights to prevent unbounded growth
        norm = np.linalg.norm(self.weights)
        if norm > 10.0:  # Threshold to prevent excessive growth
            self.weights *= 10.0 / norm

        return self.weights
```

## Challenges and Solutions in Sensorimotor Learning

### High-Dimensional Sensorimotor Spaces

Addressing the curse of dimensionality in sensorimotor learning:

```python
class HighDimensionalSensorimotorLearner:
    """
    Learning system designed for high-dimensional sensorimotor spaces.
    """

    def __init__(self, sensor_dim: int, motor_dim: int):
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim

        # Dimensionality reduction components
        self.sensor_encoder = self.build_sensor_encoder()
        self.motor_decoder = self.build_motor_decoder()

        # Modular learning approach
        self.modular_learners = self.initialize_modular_learners()

        # Attention mechanisms
        self.attention_weights = np.ones(sensor_dim) / sensor_dim

    def build_sensor_encoder(self):
        """
        Build encoder for sensor dimensionality reduction.
        """
        # This could be implemented with PCA, autoencoders, or other methods
        class SensorEncoder:
            def __init__(self, input_dim, compressed_dim=64):
                self.input_dim = input_dim
                self.compressed_dim = compressed_dim
                self.projection_matrix = np.random.normal(0, 0.1,
                                                        (input_dim, compressed_dim))

            def encode(self, sensor_data):
                return np.tanh(np.dot(sensor_data, self.projection_matrix))

        return SensorEncoder(self.sensor_dim)

    def build_motor_decoder(self):
        """
        Build decoder for motor dimensionality reconstruction.
        """
        class MotorDecoder:
            def __init__(self, compressed_dim=64, output_dim=64):
                self.compressed_dim = compressed_dim
                self.output_dim = output_dim
                self.projection_matrix = np.random.normal(0, 0.1,
                                                        (compressed_dim, output_dim))

            def decode(self, compressed_motor_data):
                return np.tanh(np.dot(compressed_motor_data, self.projection_matrix))

        return MotorDecoder(output_dim=self.motor_dim)

    def initialize_modular_learners(self):
        """
        Initialize multiple learners for different sensorimotor subspaces.
        """
        # Create learners for different functional modules
        modules = {}

        # Example: separate learners for balance, manipulation, navigation
        if self.sensor_dim >= 10:
            modules['balance'] = self.create_learner(5, 3)  # 5 sensors, 3 motors
            modules['manipulation'] = self.create_learner(8, 4)  # 8 sensors, 4 motors
            modules['navigation'] = self.create_learner(6, 2)  # 6 sensors, 2 motors

        return modules

    def create_learner(self, sub_sensor_dim: int, sub_motor_dim: int):
        """
        Create a learner for a specific sensorimotor subspace.
        """
        return {
            'weights': np.random.normal(0, 0.1, (sub_sensor_dim, sub_motor_dim)),
            'learning_rate': 0.01,
            'performance': 0.0
        }
```

### Real-Time Learning Constraints

Managing learning within real-time operational constraints:

```python
class RealTimeSensorimotorLearner:
    """
    Learning system designed for real-time operation.
    """

    def __init__(self, sensor_dim: int, motor_dim: int, max_compute_time: float = 0.01):
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim
        self.max_compute_time = max_compute_time  # Maximum time per update (seconds)

        # Fast learning algorithms
        self.fast_learner = self.initialize_fast_learner()

        # Incremental learning components
        self.experience_buffer = []
        self.buffer_size = 100  # Small buffer for batch processing

        # Prioritized learning
        self.learning_priorities = np.ones(motor_dim)  # Higher priority = more learning

    def fast_update(self, sensor_state: np.ndarray,
                   motor_command: np.ndarray,
                   reward: float) -> None:
        """
        Perform fast, incremental learning update.
        """
        start_time = time.time()

        # Use simple, fast learning rule
        self.incremental_update(sensor_state, motor_command, reward)

        # Check time constraint
        elapsed_time = time.time() - start_time
        if elapsed_time > self.max_compute_time:
            print(f"Warning: Learning update exceeded time constraint: {elapsed_time:.4f}s")

    def incremental_update(self, sensor_state: np.ndarray,
                          motor_command: np.ndarray,
                          reward: float) -> None:
        """
        Perform incremental parameter update.
        """
        # Simple eligibility trace update for fast learning
        if not hasattr(self, 'eligibility_trace'):
            self.eligibility_trace = np.zeros((self.sensor_dim, self.motor_dim))

        # Update eligibility trace
        self.eligibility_trace = 0.9 * self.eligibility_trace + np.outer(sensor_state, motor_command)

        # Apply reward-modulated learning
        learning_rate = 0.001
        self.fast_learner['weights'] += learning_rate * reward * self.eligibility_trace

    def batch_update_if_time_available(self) -> bool:
        """
        Perform more complex batch learning if time permits.
        """
        if len(self.experience_buffer) >= self.buffer_size:
            start_time = time.time()

            # Perform batch learning update
            self.batch_learning_update()

            elapsed_time = time.time() - start_time
            return elapsed_time <= self.max_compute_time
        return False

    def batch_learning_update(self) -> None:
        """
        Perform batch learning update using accumulated experiences.
        """
        # Process buffered experiences in a batch
        if self.experience_buffer:
            # Convert to numpy arrays for efficient processing
            experiences = np.array(self.experience_buffer)

            # Perform batch update (simplified example)
            for exp in experiences[-10:]:  # Use last 10 experiences
                sensor_state, motor_command, reward = exp
                self.incremental_update(sensor_state, motor_command, reward)

            # Clear processed experiences
            self.experience_buffer = self.experience_buffer[-10:]
```

## Applications in Robotics

### Locomotion Learning

Learning adaptive walking patterns:

```python
class LocomotionLearningSystem:
    """
    System for learning adaptive locomotion patterns.
    """

    def __init__(self, num_legs: int, num_joints_per_leg: int):
        self.num_legs = num_legs
        self.num_joints_per_leg = num_joints_per_leg
        self.total_joints = num_legs * num_joints_per_leg

        # Central pattern generators for rhythmic locomotion
        self.cpg_network = self.initialize_cpg_network()

        # Adaptive control for terrain adaptation
        self.terrain_adaptation = self.initialize_terrain_adaptation()

        # Learning mechanisms for gait optimization
        self.gait_optimizer = self.initialize_gait_optimizer()

    def initialize_cpg_network(self):
        """
        Initialize Central Pattern Generator network for rhythmic locomotion.
        """
        class CPGNetwork:
            def __init__(self, num_oscillators: int):
                self.num_oscillators = num_oscillators
                self.omega = 2 * np.pi * 1.0  # Radians per second (1 Hz)
                self.phase_couplings = self.initialize_couplings()
                self.state = np.random.uniform(0, 2*np.pi, num_oscillators)

            def initialize_couplings(self):
                # Create phase coupling matrix for coordinated movement
                coupling_matrix = np.zeros((self.num_oscillators, self.num_oscillators))
                # Add coupling between legs for coordination
                for i in range(self.num_oscillators):
                    for j in range(self.num_oscillators):
                        if i != j:
                            coupling_matrix[i, j] = 0.1  # Weak coupling
                return coupling_matrix

            def update(self, dt: float = 0.01):
                # Update oscillator phases based on coupling
                dtheta = np.zeros(self.num_oscillators)
                for i in range(self.num_oscillators):
                    dtheta[i] = self.omega
                    for j in range(self.num_oscillators):
                        dtheta[i] += self.phase_couplings[i, j] * np.sin(self.state[j] - self.state[i])

                self.state += dtheta * dt
                self.state = np.mod(self.state, 2 * np.pi)

                return np.sin(self.state)  # Output rhythmic signals

        return CPGNetwork(self.total_joints)

    def adapt_to_terrain(self, terrain_sensors: np.ndarray) -> np.ndarray:
        """
        Adapt locomotion pattern based on terrain feedback.
        """
        # Analyze terrain properties
        terrain_properties = {
            'roughness': np.std(terrain_sensors),
            'slope': self.estimate_slope(terrain_sensors),
            'stability': self.estimate_stability(terrain_sensors)
        }

        # Adjust gait parameters based on terrain
        gait_modifications = self.terrain_adaptation.process(terrain_properties)

        # Generate adapted motor commands
        cpg_output = self.cpg_network.update()
        adapted_commands = cpg_output + gait_modifications

        return adapted_commands

    def learn_optimal_gait(self, performance_feedback: Dict[str, float]) -> None:
        """
        Learn optimal gait parameters based on performance feedback.
        """
        # Update gait parameters to improve performance
        if performance_feedback.get('efficiency', 0) < 0.7:
            # Adjust for better energy efficiency
            self.gait_optimizer.update_energy_params()
        if performance_feedback.get('stability', 0) < 0.8:
            # Adjust for better stability
            self.gait_optimizer.update_stability_params()
```

### Manipulation Learning

Learning dexterous manipulation skills:

```python
class ManipulationLearningSystem:
    """
    System for learning dexterous manipulation skills.
    """

    def __init__(self, num_fingers: int, num_joints_per_finger: int):
        self.num_fingers = num_fingers
        self.num_joints_per_finger = num_joints_per_finger
        self.total_joints = num_fingers * num_joints_per_finger

        # Grasp learning system
        self.grasp_learner = self.initialize_grasp_learner()

        # Force control system
        self.force_controller = self.initialize_force_controller()

        # Tool use learning
        self.tool_use_learner = self.initialize_tool_use_learner()

    def learn_grasp_strategy(self, object_properties: Dict[str, Any]) -> bool:
        """
        Learn appropriate grasp strategy for an object.
        """
        # Analyze object properties
        grasp_params = {
            'size': object_properties.get('size', 0.1),
            'weight': object_properties.get('weight', 0.5),
            'shape': object_properties.get('shape', 'unknown'),
            'surface_texture': object_properties.get('texture', 'smooth')
        }

        # Determine appropriate grasp type
        grasp_type = self.classify_grasp_type(grasp_params)

        # Execute grasp with learned parameters
        success = self.execute_grasp(grasp_type, grasp_params)

        # Update learning based on outcome
        self.grasp_learner.update(grasp_params, success)

        return success

    def classify_grasp_type(self, obj_params: Dict[str, Any]) -> str:
        """
        Classify appropriate grasp type based on object properties.
        """
        if obj_params['size'] < 0.05 and obj_params['weight'] < 0.2:
            return 'precision_pinch'
        elif obj_params['size'] < 0.1 and obj_params['weight'] < 0.5:
            return 'three_finger_grasp'
        else:
            return 'power_grasp'

    def execute_grasp(self, grasp_type: str, obj_params: Dict[str, Any]) -> bool:
        """
        Execute grasp with learned parameters.
        """
        # Generate grasp trajectory based on learned models
        grasp_trajectory = self.generate_grasp_trajectory(grasp_type, obj_params)

        # Execute with force control
        success = self.force_controller.execute_with_feedback(
            grasp_trajectory, obj_params
        )

        return success

    def generate_grasp_trajectory(self, grasp_type: str,
                                obj_params: Dict[str, Any]) -> np.ndarray:
        """
        Generate appropriate grasp trajectory.
        """
        # This would generate joint angle trajectories for the grasp
        trajectory = np.zeros((100, self.total_joints))  # 100 time steps

        # Generate trajectory based on grasp type and object properties
        for t in range(100):
            # Calculate finger positions based on approach and grasp phases
            approach_phase = min(1.0, t / 30)  # First 30% is approach
            grasp_phase = max(0.0, min(1.0, (t - 30) / 70))  # Remaining is grasp

            for f in range(self.num_fingers):
                # Calculate finger joint angles based on phase
                for j in range(self.num_joints_per_finger):
                    base_angle = 0.0 if j == 0 else 0.5  # Different base angles for joints
                    approach_contribution = base_angle * (1 - approach_phase)
                    grasp_contribution = (base_angle + 1.0) * grasp_phase
                    trajectory[t, f * self.num_joints_per_finger + j] = (
                        approach_contribution + grasp_contribution
                    )

        return trajectory
```

## Evaluation and Assessment

### Performance Metrics for Sensorimotor Learning

```python
class SensorimotorLearningEvaluator:
    """
    System for evaluating sensorimotor learning performance.
    """

    def __init__(self):
        self.performance_history = []
        self.learning_curves = {}
        self.stability_metrics = {}
        self.adaptation_speeds = {}

    def evaluate_learning_efficiency(self, task_performance: Dict[str, Any]) -> Dict[str, float]:
        """
        Evaluate the efficiency of sensorimotor learning.
        """
        metrics = {}

        # Learning speed: How quickly performance improves
        if 'learning_curve' in task_performance:
            curve = task_performance['learning_curve']
            metrics['learning_speed'] = self.calculate_learning_slope(curve)

        # Sample efficiency: Performance per unit of experience
        if 'experience_samples' in task_performance and 'final_performance' in task_performance:
            metrics['sample_efficiency'] = (
                task_performance['final_performance'] /
                max(1, task_performance['experience_samples'])
            )

        # Transfer efficiency: How well learning transfers to new situations
        if 'transfer_performance' in task_performance:
            metrics['transfer_efficiency'] = task_performance['transfer_performance']

        return metrics

    def calculate_learning_slope(self, learning_curve: List[float]) -> float:
        """
        Calculate the slope of the learning curve.
        """
        if len(learning_curve) < 2:
            return 0.0

        # Use linear regression to find the slope
        x = np.arange(len(learning_curve))
        y = np.array(learning_curve)

        # Calculate slope using least squares
        slope = np.polyfit(x, y, 1)[0]
        return slope

    def evaluate_behavioral_quality(self, behavior_data: Dict[str, Any]) -> Dict[str, float]:
        """
        Evaluate the quality of learned behaviors.
        """
        metrics = {}

        # Stability: Consistency of performance over time
        if 'performance_over_time' in behavior_data:
            performance = behavior_data['performance_over_time']
            metrics['stability'] = 1.0 / (np.std(performance) + 1e-8)  # Higher is better

        # Robustness: Performance under perturbations
        if 'perturbation_performance' in behavior_data:
            perturbed_performance = behavior_data['perturbation_performance']
            nominal_performance = behavior_data.get('nominal_performance', 1.0)
            metrics['robustness'] = np.mean(perturbed_performance) / nominal_performance

        # Adaptation: Ability to adjust to new conditions
        if 'adaptation_performance' in behavior_data:
            metrics['adaptation'] = np.mean(behavior_data['adaptation_performance'])

        return metrics

    def evaluate_computational_efficiency(self, resource_usage: Dict[str, Any]) -> Dict[str, float]:
        """
        Evaluate computational requirements of learning system.
        """
        metrics = {}

        # Real-time performance
        if 'update_times' in resource_usage:
            avg_update_time = np.mean(resource_usage['update_times'])
            max_update_time = np.max(resource_usage['update_times'])
            metrics['avg_update_time'] = avg_update_time
            metrics['max_update_time'] = max_update_time
            metrics['real_time_capability'] = float(avg_update_time < 0.01)  # < 10ms

        # Memory usage
        if 'memory_usage' in resource_usage:
            metrics['memory_efficiency'] = 1.0 / (resource_usage['memory_usage'] + 1e-8)

        # Scalability: How performance scales with complexity
        if 'complexity_scaling' in resource_usage:
            complexity, time_usage = resource_usage['complexity_scaling']
            # Calculate scaling exponent
            if len(complexity) > 1:
                log_complexity = np.log(complexity[1:])
                log_time = np.log(time_usage[1:])
                scaling_exp = np.polyfit(log_complexity, log_time, 1)[0]
                metrics['scaling_exponent'] = scaling_exp

        return metrics

    def generate_comprehensive_report(self, all_metrics: Dict[str, Dict[str, float]]) -> str:
        """
        Generate a comprehensive evaluation report.
        """
        report = []
        report.append("# Sensorimotor Learning Evaluation Report")
        report.append("")

        for category, metrics in all_metrics.items():
            report.append(f"## {category.replace('_', ' ').title()} Metrics")
            report.append("")
            for metric, value in metrics.items():
                report.append(f"- {metric.replace('_', ' ').title()}: {value:.4f}")
            report.append("")

        return "\n".join(report)
```

## Future Directions and Advanced Topics

### Neuromorphic Implementation

Implementing sensorimotor learning on neuromorphic hardware:

```python
class NeuromorphicSensorimotorLearner:
    """
    Sensorimotor learning system designed for neuromorphic hardware.
    """

    def __init__(self, num_neurons: int, sensor_dim: int, motor_dim: int):
        self.num_neurons = num_neurons
        self.sensor_dim = sensor_dim
        self.motor_dim = motor_dim

        # Spiking neural network components
        self.spiking_network = self.initialize_spiking_network()
        self.sensory_input_layer = self.initialize_input_layer()
        self.motor_output_layer = self.initialize_output_layer()

        # Event-based learning
        self.event_based_learner = self.initialize_event_learner()

    def initialize_spiking_network(self):
        """
        Initialize spiking neural network for sensorimotor processing.
        """
        class SpikingNetwork:
            def __init__(self, num_neurons: int):
                self.num_neurons = num_neurons
                self.membrane_potentials = np.zeros(num_neurons)
                self.spike_times = []
                self.synaptic_weights = np.random.normal(0, 0.1, (num_neurons, num_neurons))

            def update_neurons(self, input_currents: np.ndarray, dt: float = 0.001):
                """
                Update spiking neurons with input currents.
                """
                # Simple leaky integrate-and-fire model
                leak_rate = 0.1
                threshold = 1.0

                self.membrane_potentials += dt * (input_currents - leak_rate * self.membrane_potentials)

                # Generate spikes for neurons that exceed threshold
                spikes = self.membrane_potentials > threshold
                self.membrane_potentials[spikes] = 0.0  # Reset after spike

                return spikes

        return SpikingNetwork(self.num_neurons)

    def spike_timing_dependent_plasticity(self, pre_spikes: np.ndarray,
                                        post_spikes: np.ndarray,
                                        weights: np.ndarray) -> np.ndarray:
        """
        Apply STDP learning rule for synaptic plasticity.
        """
        # STDP: Strengthen synapses where pre-synaptic spike precedes post-synaptic spike
        # and weaken where post precedes pre
        time_window = 20e-3  # 20ms time window

        # Simplified STDP rule
        weight_changes = np.zeros_like(weights)

        for i in range(len(pre_spikes)):
            for j in range(len(post_spikes)):
                if pre_spikes[i] and post_spikes[j]:
                    time_diff = abs(i - j) * 0.001  # Convert to seconds
                    if time_diff < time_window:
                        # Apply STDP rule based on timing
                        if i < j:  # Pre before post
                            weight_changes[i, j] += 0.01
                        else:  # Post before pre
                            weight_changes[i, j] -= 0.01

        return weights + weight_changes
```

## Conclusion

Sensorimotor learning represents a crucial paradigm in embodied intelligence, enabling agents to acquire complex behaviors through direct interaction with their environment. The principles of active learning, self-supervision, and morphological computation provide a foundation for creating adaptive, robust, and efficient embodied systems.

Key insights from this chapter include:

1. **Active Learning**: Sensorimotor learning is fundamentally active, with agents learning through their own exploratory behavior rather than passive observation.

2. **Embodied Learning**: The physical properties of the agent's body contribute to learning, with morphological computation reducing cognitive load.

3. **Multiple Learning Mechanisms**: Different learning mechanisms (reinforcement, imitation, self-supervised, developmental) can be combined for comprehensive skill acquisition.

4. **Real-Time Constraints**: Practical implementation requires balancing learning effectiveness with real-time operational constraints.

5. **Modular Approaches**: High-dimensional sensorimotor spaces require modular and hierarchical learning approaches.

The future of sensorimotor learning lies in the integration of these principles with advanced machine learning techniques, neuromorphic computing, and bio-inspired algorithms. As these systems become more sophisticated, they will enable robots and artificial agents to learn complex behaviors with the same efficiency and adaptability as biological systems.

The challenge remains in scaling these approaches to complex real-world applications while maintaining the robustness and adaptability that make sensorimotor learning so powerful. Success in this endeavor will be crucial for creating truly autonomous embodied intelligence systems.
