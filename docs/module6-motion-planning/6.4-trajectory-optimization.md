---
id: module6-motion-planning-6.4-trajectory-optimization
title: "Trajectory Optimization"
slug: /module6-motion-planning-6.4-trajectory-optimization
---

# Trajectory Optimization

Trajectory optimization is a critical component of motion planning that focuses on finding time-parameterized paths that optimize specific performance criteria while satisfying various constraints. Unlike geometric path planning, which only determines spatial paths, trajectory optimization considers the temporal aspect of motion, including velocity, acceleration, and higher-order derivatives.

## Overview and Fundamentals

Trajectory optimization involves finding a time-parameterized function q(t) that describes the robot's configuration over time, where t ∈ [t₀, t_f]. The optimization process typically minimizes a cost functional:

J[q(t)] = ∫[t₀ to t_f] L(q(t), q̇(t), q̈(t), t) dt + φ(q(t_f))

subject to:
- Dynamic constraints: f(q, q̇, q̈, u, t) = 0
- Kinematic constraints: g(q, q̇, t) ≤ 0
- Boundary conditions: q(t₀) = q₀, q(t_f) = q_f
- Control limits: u_min ≤ u ≤ u_max

Where L is the running cost, φ is the terminal cost, and u represents control inputs.

## Mathematical Formulation

The general trajectory optimization problem can be formulated as:

minimize: J(x(·), u(·)) = φ(x(t_f)) + ∫[t₀ to t_f] L(x(t), u(t), t) dt

subject to:
- ẋ(t) = f(x(t), u(t), t)  (system dynamics)
- h(x(t), u(t), t) ≤ 0     (path constraints)
- x(t₀) = x₀               (initial state)
- x(t_f) = x_f             (final state)
- u_min ≤ u(t) ≤ u_max     (control bounds)

Where x represents the state vector (configuration and derivatives), u represents control inputs, and f represents the system dynamics.

## Direct vs. Indirect Methods

### Direct Methods

Direct methods discretize the continuous optimization problem into a finite-dimensional nonlinear programming (NLP) problem. The trajectory is parameterized using a finite set of parameters.

#### Direct Collocation
In direct collocation, both states and controls are discretized, and the dynamics are enforced at specific collocation points using numerical integration schemes.

```python
import numpy as np
from scipy.optimize import minimize
from typing import List, Tuple, Callable
import matplotlib.pyplot as plt

class DirectCollocationTrajectoryOptimizer:
    def __init__(self,
                 dynamics_func: Callable[[np.ndarray, np.ndarray], np.ndarray],
                 num_segments: int = 20,
                 poly_deg: int = 3):
        """
        Direct collocation trajectory optimizer
        Args:
            dynamics_func: Function that returns dx/dt = f(x, u)
            num_segments: Number of time segments
            poly_deg: Degree of interpolating polynomial
        """
        self.dynamics_func = dynamics_func
        self.num_segments = num_segments
        self.poly_deg = poly_deg
        self.dt = 1.0 / num_segments  # Normalized time interval

        # Legendre-Gauss-Lobatto points for collocation
        if poly_deg == 2:
            self.tau = np.array([-1.0, 0.0, 1.0])  # 3 LGL points
        elif poly_deg == 3:
            self.tau = np.array([-1.0, 0.0, 1.0])  # Simplified for demonstration
        else:
            # For higher degree, use actual LGL points
            self.tau = self._compute_lgl_points(poly_deg)

    def _compute_lgl_points(self, n: int) -> np.ndarray:
        """Compute Legendre-Gauss-Lobatto points"""
        # For simplicity, return evenly spaced points
        # In practice, use actual LGL points
        return np.linspace(-1, 1, n + 1)

    def optimize_trajectory(self,
                           x0: np.ndarray,
                           xf: np.ndarray,
                           control_bounds: Tuple[np.ndarray, np.ndarray],
                           state_bounds: Tuple[np.ndarray, np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Optimize trajectory using direct collocation
        Args:
            x0: Initial state
            xf: Final state
            control_bounds: (umin, umax) control bounds
            state_bounds: (xmin, xmax) state bounds (optional)
        Returns:
            (states, controls) trajectory
        """
        nx = len(x0)  # Number of state variables
        nu = len(control_bounds[0])  # Number of control variables

        # Decision variables: [x_0, u_0, x_1, u_1, ..., x_N, u_N]
        n_vars = (self.num_segments + 1) * nx + self.num_segments * nu

        # Initial guess
        x_guess = np.linspace(x0, xf, self.num_segments + 1).flatten()
        u_guess = np.zeros(self.num_segments * nu)  # Zero control initially
        initial_guess = np.concatenate([x_guess, u_guess])

        # Define bounds
        x_lb = np.tile(state_bounds[0] if state_bounds else -np.inf, self.num_segments + 1)
        x_ub = np.tile(state_bounds[1] if state_bounds else np.inf, self.num_segments + 1)
        u_lb = np.tile(control_bounds[0], self.num_segments)
        u_ub = np.tile(control_bounds[1], self.num_segments)
        bounds = list(zip(np.concatenate([x_lb, u_lb]),
                         np.concatenate([x_ub, u_ub])))

        # Constraints
        constraints = []

        # Boundary constraints
        def initial_state_constraint(vars_flat):
            x_init = vars_flat[:nx]
            return x_init - x0

        def final_state_constraint(vars_flat):
            x_final = vars_flat[nx*(self.num_segments):nx*(self.num_segments+1)]
            return x_final - xf

        constraints.append({'type': 'eq', 'fun': initial_state_constraint})
        constraints.append({'type': 'eq', 'fun': final_state_constraint})

        # Dynamics constraints via collocation
        for k in range(self.num_segments):
            for j in range(1, len(self.tau)):  # Skip first LGL point
                def dynamics_constraint(vars_flat, seg=k, point=j):
                    # Extract states and controls for this segment
                    x_start = vars_flat[seg*nx:(seg+1)*nx]
                    x_end = vars_flat[(seg+1)*nx:(seg+2)*nx]
                    u = vars_flat[(self.num_segments+1)*nx + seg*nu:(self.num_segments+1)*nx + (seg+1)*nu]

                    # For simplicity, use Euler integration
                    # In practice, use proper collocation equations
                    x_dot = self.dynamics_func(np.concatenate([x_start, x_end]), u)
                    return x_end - x_start - self.dt * x_dot

                constraints.append({'type': 'eq', 'fun': dynamics_constraint})

        # Objective function: minimize control effort
        def objective(vars_flat):
            u_vars = vars_flat[(self.num_segments+1)*nx:]
            u_matrix = u_vars.reshape(self.num_segments, nu)
            # Minimize sum of squared controls
            return np.sum(np.sum(u_matrix**2, axis=1))

        # Solve optimization problem
        result = minimize(objective, initial_guess, method='SLSQP',
                         bounds=bounds, constraints=constraints)

        if result.success:
            # Extract optimized trajectory
            x_opt = result.x[:nx*(self.num_segments+1)].reshape(self.num_segments+1, nx)
            u_opt = result.x[(self.num_segments+1)*nx:].reshape(self.num_segments, nu)
            return x_opt, u_opt
        else:
            raise RuntimeError(f"Optimization failed: {result.message}")

# Example: Double integrator system
def double_integrator_dynamics(x, u):
    """
    Dynamics for double integrator: ẍ = u
    State: x = [position, velocity]
    Control: u = acceleration
    """
    pos = x[0]
    vel = x[1]
    acc = u[0]

    return np.array([vel, acc])  # [dpos/dt, dvel/dt]

# Example usage
def example_double_integrator():
    optimizer = DirectCollocationTrajectoryOptimizer(
        dynamics_func=double_integrator_dynamics,
        num_segments=50,
        poly_deg=2
    )

    # Initial and final states
    x0 = np.array([0.0, 0.0])  # Start at position 0 with zero velocity
    xf = np.array([1.0, 0.0])  # End at position 1 with zero velocity

    # Control bounds (acceleration limits)
    umin = np.array([-2.0])
    umax = np.array([2.0])

    # State bounds (position and velocity limits)
    xmin = np.array([-2.0, -5.0])
    xmax = np.array([3.0, 5.0])

    try:
        states, controls = optimizer.optimize_trajectory(
            x0, xf, (umin, umax), (xmin, xmax)
        )

        print(f"Optimization successful!")
        print(f"Initial state: {states[0]}")
        print(f"Final state: {states[-1]}")
        print(f"Trajectory length: {len(states)} points")

        # Plot results
        time = np.linspace(0, 1, len(states))
        plt.figure(figsize=(12, 8))

        plt.subplot(2, 2, 1)
        plt.plot(time, states[:, 0], 'b-', linewidth=2, label='Position')
        plt.xlabel('Time (s)')
        plt.ylabel('Position')
        plt.title('Position vs Time')
        plt.grid(True)
        plt.legend()

        plt.subplot(2, 2, 2)
        plt.plot(time, states[:, 1], 'r-', linewidth=2, label='Velocity')
        plt.xlabel('Time (s)')
        plt.ylabel('Velocity')
        plt.title('Velocity vs Time')
        plt.grid(True)
        plt.legend()

        # Control input (extend time array for controls)
        time_ctrl = np.linspace(0, 1, len(controls)+1)
        time_ctrl = (time_ctrl[:-1] + time_ctrl[1:])/2  # Midpoints
        plt.subplot(2, 2, 3)
        plt.step(time_ctrl, controls[:, 0], 'g-', where='post', linewidth=2, label='Control')
        plt.xlabel('Time (s)')
        plt.ylabel('Control (Acceleration)')
        plt.title('Control vs Time')
        plt.grid(True)
        plt.legend()

        plt.subplot(2, 2, 4)
        plt.plot(states[:, 0], states[:, 1], 'm-', linewidth=2)
        plt.xlabel('Position')
        plt.ylabel('Velocity')
        plt.title('Phase Portrait')
        plt.grid(True)

        plt.tight_layout()
        plt.show()

        return states, controls

    except RuntimeError as e:
        print(f"Optimization failed: {e}")
        return None, None

if __name__ == "__main__":
    example_double_integrator()
```

#### Direct Shooting
Direct shooting methods parameterize only the control inputs and integrate the dynamics forward to obtain states.

```python
import numpy as np
from scipy.integrate import solve_ivp
from scipy.optimize import minimize
import matplotlib.pyplot as plt

class DirectShootingOptimizer:
    def __init__(self,
                 dynamics_func: Callable[[float, np.ndarray, np.ndarray], np.ndarray],
                 num_control_points: int = 20):
        """
        Direct shooting trajectory optimizer
        Args:
            dynamics_func: Function dx/dt = f(t, x, u)
            num_control_points: Number of control discretization points
        """
        self.dynamics_func = dynamics_func
        self.num_control_points = num_control_points
        self.tf = 1.0  # Final time (normalized)

    def simulate_trajectory(self, x0: np.ndarray, control_params: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Simulate trajectory given control parameters
        Args:
            x0: Initial state
            control_params: Optimized control parameters
        Returns:
            (time_points, state_trajectory)
        """
        # Reshape control parameters into piecewise constant controls
        nu = len(control_params) // self.num_control_points
        controls = control_params.reshape(self.num_control_points, nu)

        # Define control function that interpolates between control points
        def control_function(t):
            # Map time to control index
            t_norm = t / self.tf
            idx = int(t_norm * self.num_control_points)
            idx = min(idx, self.num_control_points - 1)
            return controls[idx]

        # Define ODE function with piecewise constant control
        def ode_func(t, x):
            u = control_function(t)
            return self.dynamics_func(t, x, u)

        # Time points for integration
        t_eval = np.linspace(0, self.tf, 100)

        # Integrate the system
        sol = solve_ivp(ode_func, [0, self.tf], x0, t_eval=t_eval,
                       method='RK45', rtol=1e-6, atol=1e-9)

        return sol.t, sol.y.T

    def optimize_trajectory(self,
                           x0: np.ndarray,
                           xf: np.ndarray,
                           control_bounds: Tuple[np.ndarray, np.ndarray],
                           state_bounds: Tuple[np.ndarray, np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Optimize trajectory using direct shooting
        Args:
            x0: Initial state
            xf: Final state
            control_bounds: (umin, umax) control bounds
            state_bounds: (xmin, xmax) state bounds (optional)
        Returns:
            (time, states, controls) optimized trajectory
        """
        nx = len(x0)
        nu = len(control_bounds[0])

        # Total number of optimization variables (control parameters)
        n_vars = self.num_control_points * nu

        # Initial guess for controls (start with zeros)
        initial_guess = np.zeros(n_vars)

        # Define bounds for optimization variables
        u_lb = np.tile(control_bounds[0], self.num_control_points)
        u_ub = np.tile(control_bounds[1], self.num_control_points)
        bounds = list(zip(u_lb, u_ub))

        # Objective function: minimize control effort and reach final state
        def objective(control_params):
            t, states = self.simulate_trajectory(x0, control_params)

            # Terminal cost (deviation from final state)
            terminal_cost = np.sum((states[-1] - xf)**2)

            # Control effort cost
            controls = control_params.reshape(self.num_control_points, nu)
            control_effort = np.sum(np.sum(controls**2, axis=1)) * (self.tf / self.num_control_points)

            # State constraint violation penalty
            state_violation = 0.0
            if state_bounds:
                xmin, xmax = state_bounds
                for state in states:
                    for i in range(nx):
                        if state[i] < xmin[i]:
                            state_violation += (xmin[i] - state[i])**2
                        elif state[i] > xmax[i]:
                            state_violation += (state[i] - xmax[i])**2

            return terminal_cost + 0.1 * control_effort + 100.0 * state_violation

        # Constraints
        constraints = []

        # Solve optimization problem
        result = minimize(objective, initial_guess, method='SLSQP',
                         bounds=bounds, options={'ftol': 1e-6})

        if result.success:
            t, states = self.simulate_trajectory(x0, result.x)

            # Extract controls from optimized parameters
            controls = result.x.reshape(self.num_control_points, nu)

            return t, states, controls
        else:
            raise RuntimeError(f"Optimization failed: {result.message}")

# Example: Simple double integrator for a robot
def simple_robot_dynamics(t, x, u):
    """
    Simple robot dynamics: position and velocity states with acceleration control
    x = [pos_x, pos_y, vel_x, vel_y]
    u = [acc_x, acc_y]
    """
    pos_x, pos_y, vel_x, vel_y = x
    acc_x, acc_y = u

    return np.array([vel_x, vel_y, acc_x, acc_y])

# Example usage
def example_robot_trajectory():
    optimizer = DirectShootingOptimizer(
        dynamics_func=simple_robot_dynamics,
        num_control_points=50
    )

    # Initial and final states
    x0 = np.array([0.0, 0.0, 0.0, 0.0])  # Start at origin with zero velocity
    xf = np.array([2.0, 1.0, 0.0, 0.0])  # End at (2,1) with zero velocity

    # Control bounds (acceleration limits)
    umin = np.array([-1.0, -1.0])
    umax = np.array([1.0, 1.0])

    # State bounds (position and velocity limits)
    xmin = np.array([-3.0, -3.0, -2.0, -2.0])
    xmax = np.array([3.0, 3.0, 2.0, 2.0])

    try:
        time, states, controls = optimizer.optimize_trajectory(
            x0, xf, (umin, umax), (xmin, xmax)
        )

        print(f"Optimization successful!")
        print(f"Initial state: {states[0]}")
        print(f"Final state: {states[-1]}")
        print(f"Final state error: {np.linalg.norm(states[-1] - xf)}")

        # Plot results
        plt.figure(figsize=(15, 5))

        # Plot 1: Trajectory in 2D space
        plt.subplot(1, 3, 1)
        plt.plot(states[:, 0], states[:, 1], 'b-', linewidth=2, label='Trajectory')
        plt.plot(x0[0], x0[1], 'go', markersize=10, label='Start')
        plt.plot(xf[0], xf[1], 'ro', markersize=10, label='Goal')
        plt.xlabel('X Position')
        plt.ylabel('Y Position')
        plt.title('Robot Trajectory')
        plt.grid(True)
        plt.axis('equal')
        plt.legend()

        # Plot 2: Position vs time
        plt.subplot(1, 3, 2)
        plt.plot(time, states[:, 0], 'b-', linewidth=2, label='X Position')
        plt.plot(time, states[:, 1], 'r-', linewidth=2, label='Y Position')
        plt.xlabel('Time (s)')
        plt.ylabel('Position')
        plt.title('Position vs Time')
        plt.grid(True)
        plt.legend()

        # Plot 3: Velocity vs time
        plt.subplot(1, 3, 3)
        plt.plot(time, states[:, 2], 'b-', linewidth=2, label='X Velocity')
        plt.plot(time, states[:, 3], 'r-', linewidth=2, label='Y Velocity')
        plt.xlabel('Time (s)')
        plt.ylabel('Velocity')
        plt.title('Velocity vs Time')
        plt.grid(True)
        plt.legend()

        plt.tight_layout()
        plt.show()

        return time, states, controls

    except RuntimeError as e:
        print(f"Optimization failed: {e}")
        return None, None, None

if __name__ == "__main__":
    example_robot_trajectory()
```

### Indirect Methods

Indirect methods use the calculus of variations to derive necessary conditions for optimality, resulting in a boundary value problem that must be solved.

#### Pontryagin's Minimum Principle

The necessary conditions for optimality are given by:

1. State equation: ẋ(t) = ∂H/∂λ
2. Costate equation: λ̇(t) = -∂H/∂x
3. Optimality condition: ∂H/∂u = 0
4. Hamiltonian: H(x, u, λ, t) = L(x, u, t) + λᵀf(x, u, t)

```python
import numpy as np
from scipy.integrate import solve_ivp
from scipy.optimize import fsolve
import matplotlib.pyplot as plt

class IndirectOptimizer:
    def __init__(self,
                 dynamics_func: Callable[[np.ndarray, np.ndarray], np.ndarray],
                 cost_func: Callable[[np.ndarray, np.ndarray], float]):
        """
        Indirect trajectory optimizer using Pontryagin's Minimum Principle
        Args:
            dynamics_func: Function dx/dt = f(x, u)
            cost_func: Function L(x, u) for running cost
        """
        self.dynamics_func = dynamics_func
        self.cost_func = cost_func

    def hamiltonian(self, x: np.ndarray, u: np.ndarray,
                   lambda_vec: np.ndarray) -> float:
        """
        Compute the Hamiltonian H(x, u, λ) = L(x, u) + λᵀf(x, u)
        """
        running_cost = self.cost_func(x, u)
        state_deriv = self.dynamics_func(x, u)
        return running_cost + np.dot(lambda_vec, state_deriv)

    def optimal_control(self, x: np.ndarray, lambda_vec: np.ndarray) -> np.ndarray:
        """
        Find optimal control u* that minimizes Hamiltonian
        This is a simplified version - in practice, this may require numerical optimization
        """
        # For a simple linear quadratic regulator case:
        # H = x'Qx + u'Ru + λ'(Ax + Bu)
        # ∂H/∂u = 2Ru + λ'B = 0 → u* = -(R^(-1)B'λ)/2
        # This is a placeholder implementation
        return -lambda_vec  # Simple negative feedback

    def boundary_conditions(self, initial_state: np.ndarray,
                          final_state: np.ndarray,
                          initial_time: float = 0.0,
                          final_time: float = 1.0) -> Callable:
        """
        Define boundary conditions for the boundary value problem
        """
        def bc(ya, yb):
            # ya: state and costate at initial time
            # yb: state and costate at final time
            nx = len(initial_state)

            # State boundary conditions
            state_residuals = []
            state_residuals.extend(ya[:nx] - initial_state)  # x(0) = x0
            state_residuals.extend(yb[:nx] - final_state)    # x(tf) = xf

            # Costate boundary conditions (for free final state, λ(tf) = 0)
            costate_residuals = yb[nx:]  # λ(tf) = 0

            return np.concatenate([state_residuals, costate_residuals])

        return bc

    def solve_two_point_bvp(self, x0: np.ndarray, xf: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Solve the two-point boundary value problem
        """
        nx = len(x0)

        # Define the Hamiltonian system: d/dt[x, λ] = [f(x,u), -∂H/∂x]
        def hamiltonian_system(t, y):
            x = y[:nx]
            lambda_vec = y[nx:]

            # Compute optimal control
            u = self.optimal_control(x, lambda_vec)

            # State equation: ẋ = ∂H/∂λ = f(x, u)
            x_dot = self.dynamics_func(x, u)

            # Costate equation: λ̇ = -∂H/∂x
            # For this simple example, we'll approximate the costate derivative
            # In practice, this requires computing ∂f/∂x and ∂L/∂x
            lambda_dot = -x  # Placeholder: actual computation depends on problem

            return np.concatenate([x_dot, lambda_dot])

        # Boundary conditions
        bc = self.boundary_conditions(x0, xf)

        # Initial guess for the solution
        # This is challenging and often requires good initial guess
        t_span = [0, 1]
        y0_guess = np.concatenate([x0, np.zeros(nx)])  # Initial costate guess

        # Solve the BVP using a shooting method approach
        def shooting_function(lambda0):
            # Integrate forward with given initial costate
            y0 = np.concatenate([x0, lambda0])

            sol = solve_ivp(hamiltonian_system, t_span, y0,
                           dense_output=True, rtol=1e-8)

            # Extract final state and costate
            y_final = sol.y[:, -1]
            x_final = y_final[:nx]

            # Return the error in final state boundary condition
            return x_final - xf

        # Solve for the correct initial costate using fsolve
        lambda0_initial = np.zeros(nx)
        lambda0_solution = fsolve(shooting_function, lambda0_initial)

        # Integrate with the correct initial conditions
        y0_final = np.concatenate([x0, lambda0_solution])
        sol = solve_ivp(hamiltonian_system, t_span, y0_final,
                       dense_output=True, rtol=1e-8)

        # Extract state trajectory
        states = sol.y[:nx, :].T
        costates = sol.y[nx:, :].T
        time = sol.t

        return time, states, costates

# Example: Linear quadratic regulator
def lqr_dynamics(x, u):
    """
    Linear system dynamics: ẋ = Ax + Bu
    For a simple double integrator: x = [position, velocity]
    """
    A = np.array([[0, 1], [0, 0]])  # Double integrator
    B = np.array([[0], [1]])        # Control input affects acceleration

    return A @ x + B.flatten() * u[0]

def lqr_cost(x, u):
    """
    Quadratic cost: x'Qx + u'Ru
    """
    Q = np.array([[1, 0], [0, 1]])  # State penalty
    R = np.array([[0.1]])           # Control penalty

    return x.T @ Q @ x + u.T @ R @ u

# Example usage
def example_lqr():
    optimizer = IndirectOptimizer(dynamics_func=lqr_dynamics, cost_func=lqr_cost)

    # Initial and final states
    x0 = np.array([1.0, 0.1])  # Start at position 1, small positive velocity
    xf = np.array([0.0, 0.0])  # End at origin with zero velocity

    try:
        time, states, costates = optimizer.solve_two_point_bvp(x0, xf)

        print(f"Optimization successful!")
        print(f"Initial state: {x0}")
        print(f"Final state: {states[-1]}")
        print(f"Final state error: {np.linalg.norm(states[-1] - xf)}")

        # Plot results
        plt.figure(figsize=(12, 8))

        plt.subplot(2, 2, 1)
        plt.plot(time, states[:, 0], 'b-', linewidth=2, label='Position')
        plt.xlabel('Time (s)')
        plt.ylabel('Position')
        plt.title('Position vs Time')
        plt.grid(True)
        plt.legend()

        plt.subplot(2, 2, 2)
        plt.plot(time, states[:, 1], 'r-', linewidth=2, label='Velocity')
        plt.xlabel('Time (s)')
        plt.ylabel('Velocity')
        plt.title('Velocity vs Time')
        plt.grid(True)
        plt.legend()

        plt.subplot(2, 2, 3)
        plt.plot(states[:, 0], states[:, 1], 'm-', linewidth=2)
        plt.xlabel('Position')
        plt.ylabel('Velocity')
        plt.title('Phase Portrait')
        plt.grid(True)

        plt.subplot(2, 2, 4)
        plt.plot(time, costates[:, 0], 'g-', linewidth=2, label='Costate 1')
        plt.plot(time, costates[:, 1], 'c-', linewidth=2, label='Costate 2')
        plt.xlabel('Time (s)')
        plt.ylabel('Costate')
        plt.title('Costates vs Time')
        plt.grid(True)
        plt.legend()

        plt.tight_layout()
        plt.show()

        return time, states, costates

    except Exception as e:
        print(f"Solution failed: {e}")
        return None, None, None

if __name__ == "__main__":
    example_lqr()
```

## Trajectory Optimization Techniques

### CHOMP (Covariant Hamiltonian Optimization for Motion Planning)

CHOMP is an optimization-based trajectory generation method that operates in the space of feasible trajectories.

```python
import numpy as np
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt

class CHOMP:
    def __init__(self,
                 start: np.ndarray,
                 goal: np.ndarray,
                 obstacles: np.ndarray,
                 num_waypoints: int = 50,
                 step_size: float = 0.01,
                 max_iterations: int = 100):
        """
        CHOMP trajectory optimizer
        Args:
            start: Start configuration
            goal: Goal configuration
            obstacles: Array of obstacle positions
            num_waypoints: Number of waypoints in trajectory
            step_size: Optimization step size
            max_iterations: Maximum optimization iterations
        """
        self.start = start
        self.goal = goal
        self.obstacles = obstacles
        self.num_waypoints = num_waypoints
        self.step_size = step_size
        self.max_iterations = max_iterations

        # Initialize trajectory as straight line
        self.trajectory = np.linspace(start, goal, num_waypoints)

        # Precompute obstacle distances if needed
        self.obstacle_kdtree = None  # Use scipy.spatial.KDTree in practice

    def compute_collision_cost_gradient(self, trajectory_point: np.ndarray) -> np.ndarray:
        """
        Compute gradient of collision cost at a trajectory point
        """
        # Compute distances to all obstacles
        if len(self.obstacles) > 0:
            # For each obstacle, compute distance and gradient
            total_gradient = np.zeros_like(trajectory_point)

            for obs in self.obstacles:
                diff = trajectory_point - obs
                dist = np.linalg.norm(diff)

                # Collision cost increases as we get closer to obstacles
                # Gradient points away from obstacles
                if dist > 0:
                    # Use exponential penalty: cost = exp(-dist/sigma)
                    sigma = 0.2  # Controls the width of the obstacle
                    cost = np.exp(-dist / sigma)
                    gradient = cost * diff / (sigma * dist)
                    total_gradient += gradient

            return total_gradient
        else:
            return np.zeros_like(trajectory_point)

    def compute_smoothness_cost_gradient(self, trajectory: np.ndarray, idx: int) -> np.ndarray:
        """
        Compute gradient of smoothness cost for waypoint at index idx
        """
        if idx == 0 or idx == len(trajectory) - 1:
            # Don't move start and end points
            return np.zeros_like(trajectory[idx])

        # Smoothness cost: minimize (x_{i-1} - 2x_i + x_{i+1})^2
        # Gradient with respect to x_i is: 4x_i - 2x_{i-1} - 2x_{i+1}
        prev_point = trajectory[idx - 1]
        curr_point = trajectory[idx]
        next_point = trajectory[idx + 1]

        smoothness_grad = 4 * curr_point - 2 * prev_point - 2 * next_point
        return smoothness_grad

    def optimize_trajectory(self) -> np.ndarray:
        """
        Optimize the trajectory using CHOMP
        """
        trajectory = self.trajectory.copy()

        for iteration in range(self.max_iterations):
            # Compute updates for each waypoint
            updates = np.zeros_like(trajectory)

            for i in range(1, len(trajectory) - 1):  # Skip start and end
                # Collision cost gradient
                collision_grad = self.compute_collision_cost_gradient(trajectory[i])

                # Smoothness cost gradient
                smoothness_grad = self.compute_smoothness_cost_gradient(trajectory, i)

                # Combined gradient (weighted sum)
                total_grad = 0.8 * collision_grad + 0.2 * smoothness_grad

                # Update waypoint
                updates[i] = -self.step_size * total_grad

            # Apply updates
            trajectory += updates

            # Check for convergence
            if np.linalg.norm(updates) < 1e-6:
                print(f"Converged after {iteration + 1} iterations")
                break

        self.trajectory = trajectory
        return trajectory

    def visualize(self):
        """
        Visualize the optimized trajectory
        """
        if self.trajectory.shape[1] == 2:  # 2D case
            plt.figure(figsize=(10, 8))

            # Plot obstacles
            if len(self.obstacles) > 0:
                plt.scatter(self.obstacles[:, 0], self.obstacles[:, 1],
                          c='red', s=100, alpha=0.7, label='Obstacles')

            # Plot trajectory
            plt.plot(self.trajectory[:, 0], self.trajectory[:, 1],
                    'b-', linewidth=2, label='Optimized Trajectory')
            plt.plot(self.trajectory[0, 0], self.trajectory[0, 1],
                    'go', markersize=10, label='Start')
            plt.plot(self.trajectory[-1, 0], self.trajectory[-1, 1],
                    'ro', markersize=10, label='Goal')

            plt.xlabel('X')
            plt.ylabel('Y')
            plt.title('CHOMP Trajectory Optimization')
            plt.legend()
            plt.grid(True)
            plt.axis('equal')
            plt.show()

# Example usage
def example_chomp():
    # Define start and goal
    start = np.array([0.0, 0.0])
    goal = np.array([5.0, 5.0])

    # Define obstacles
    obstacles = np.array([
        [2.0, 2.0],
        [2.0, 3.0],
        [3.0, 2.0],
        [3.0, 3.0]
    ])

    # Create and run CHOMP optimizer
    chomp = CHOMP(start, goal, obstacles, num_waypoints=30,
                  step_size=0.1, max_iterations=200)

    optimized_trajectory = chomp.optimize_trajectory()

    print(f"Initial trajectory cost: {compute_trajectory_cost(chomp.trajectory.copy(), obstacles)}")
    print(f"Optimized trajectory cost: {compute_trajectory_cost(optimized_trajectory, obstacles)}")

    chomp.visualize()

    return optimized_trajectory

def compute_trajectory_cost(trajectory, obstacles):
    """
    Compute total cost of a trajectory (collision + smoothness)
    """
    # Collision cost
    collision_cost = 0
    for point in trajectory:
        for obs in obstacles:
            dist = np.linalg.norm(point - obs)
            if dist < 0.5:  # Within obstacle influence
                collision_cost += np.exp(-dist / 0.2)

    # Smoothness cost
    smoothness_cost = 0
    for i in range(1, len(trajectory) - 1):
        prev_point = trajectory[i - 1]
        curr_point = trajectory[i]
        next_point = trajectory[i + 1]

        smoothness_cost += np.linalg.norm(2 * curr_point - prev_point - next_point)

    return collision_cost + 0.1 * smoothness_cost

if __name__ == "__main__":
    example_chomp()
```

### STOMP (Stochastic Trajectory Optimization for Motion Planning)

STOMP uses stochastic optimization to generate trajectories by sampling noisy versions of the current trajectory.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist

class STOMP:
    def __init__(self,
                 start: np.ndarray,
                 goal: np.ndarray,
                 obstacles: np.ndarray,
                 num_waypoints: int = 50,
                 num_samples: int = 100,
                 learning_rate: float = 0.01,
                 noise_scale: float = 0.1,
                 max_iterations: int = 100):
        """
        STOMP trajectory optimizer
        Args:
            start: Start configuration
            goal: Goal configuration
            obstacles: Array of obstacle positions
            num_waypoints: Number of waypoints in trajectory
            num_samples: Number of noisy trajectories to sample per iteration
            learning_rate: Learning rate for gradient update
            noise_scale: Scale of noise added to trajectories
            max_iterations: Maximum optimization iterations
        """
        self.start = start
        self.goal = goal
        self.obstacles = obstacles
        self.num_waypoints = num_waypoints
        self.num_samples = num_samples
        self.learning_rate = learning_rate
        self.noise_scale = noise_scale
        self.max_iterations = max_iterations

        # Initialize trajectory as straight line
        self.trajectory = np.linspace(start, goal, num_waypoints)

        # Basis functions for smooth perturbations
        self.basis_functions = self._create_basis_functions()

    def _create_basis_functions(self):
        """
        Create basis functions for smooth trajectory perturbations
        """
        t = np.linspace(0, 1, self.num_waypoints)
        num_basis = 10
        basis = np.zeros((num_basis, self.num_waypoints))

        # Fourier basis functions
        for i in range(num_basis):
            if i == 0:
                basis[i, :] = 1  # Constant
            elif i % 2 == 1:
                basis[i, :] = np.cos(np.pi * (i // 2 + 1) * t)  # Cosine terms
            else:
                basis[i, :] = np.sin(np.pi * (i // 2) * t)      # Sine terms

        return basis

    def compute_trajectory_cost(self, trajectory: np.ndarray) -> float:
        """
        Compute total cost of a trajectory (collision + smoothness + goal)
        """
        cost = 0.0

        # Collision cost
        for point in trajectory:
            min_dist = float('inf')
            for obs in self.obstacles:
                dist = np.linalg.norm(point - obs)
                min_dist = min(min_dist, dist)

            # Exponential penalty for being close to obstacles
            if min_dist < 0.5:
                cost += np.exp(-min_dist / 0.2)

        # Smoothness cost (discrete approximation of integral of squared acceleration)
        if len(trajectory) > 2:
            for i in range(1, len(trajectory) - 1):
                smoothness = np.linalg.norm(
                    trajectory[i-1] - 2*trajectory[i] + trajectory[i+1]
                )
                cost += 0.1 * smoothness

        # Goal cost
        goal_dist = np.linalg.norm(trajectory[-1] - self.goal)
        cost += 10.0 * goal_dist  # High weight for reaching goal

        return cost

    def generate_noisy_trajectories(self) -> np.ndarray:
        """
        Generate noisy versions of the current trajectory
        """
        noisy_trajectories = np.zeros((self.num_samples, self.num_waypoints, len(self.start)))

        for i in range(self.num_samples):
            # Generate random coefficients for basis functions
            coeffs = np.random.normal(0, self.noise_scale,
                                    (len(self.start), self.basis_functions.shape[0]))

            # Create perturbation
            perturbation = np.zeros((self.num_waypoints, len(self.start)))
            for dim in range(len(self.start)):
                for j in range(self.basis_functions.shape[0]):
                    perturbation[:, dim] += coeffs[dim, j] * self.basis_functions[j, :]

            # Apply perturbation to trajectory
            noisy_trajectory = self.trajectory + perturbation

            # Ensure start and end points are fixed
            noisy_trajectory[0] = self.start
            noisy_trajectory[-1] = self.goal

            noisy_trajectories[i] = noisy_trajectory

        return noisy_trajectories

    def optimize_trajectory(self) -> np.ndarray:
        """
        Optimize the trajectory using STOMP
        """
        best_trajectory = self.trajectory.copy()
        best_cost = self.compute_trajectory_cost(best_trajectory)

        for iteration in range(self.max_iterations):
            # Generate noisy trajectories
            noisy_trajectories = self.generate_noisy_trajectories()

            # Evaluate costs for all noisy trajectories
            costs = []
            for traj in noisy_trajectories:
                cost = self.compute_trajectory_cost(traj)
                costs.append(cost)

            costs = np.array(costs)

            # Update trajectory using weighted average of low-cost trajectories
            # Lower cost trajectories get higher weight
            weights = np.exp(-costs / np.std(costs))  # Softmax-like weights
            weights = weights / np.sum(weights)  # Normalize

            # Compute weighted average update
            avg_update = np.zeros_like(self.trajectory)
            for i in range(self.num_samples):
                if i == 0:  # Use first trajectory as reference
                    reference_traj = noisy_trajectories[i]
                avg_update += weights[i] * (noisy_trajectories[i] - reference_traj)

            # Apply update
            self.trajectory = reference_traj + self.learning_rate * avg_update

            # Ensure start and end points are fixed
            self.trajectory[0] = self.start
            self.trajectory[-1] = self.goal

            # Check if we found a better trajectory
            current_cost = self.compute_trajectory_cost(self.trajectory)
            if current_cost < best_cost:
                best_cost = current_cost
                best_trajectory = self.trajectory.copy()

            # Print progress
            if iteration % 20 == 0:
                print(f"Iteration {iteration}, Cost: {current_cost:.4f}")

        self.trajectory = best_trajectory
        return best_trajectory

    def visualize(self):
        """
        Visualize the optimized trajectory
        """
        if self.trajectory.shape[1] == 2:  # 2D case
            plt.figure(figsize=(12, 8))

            # Plot obstacles
            if len(self.obstacles) > 0:
                plt.scatter(self.obstacles[:, 0], self.obstacles[:, 1],
                          c='red', s=100, alpha=0.7, label='Obstacles')

            # Plot initial trajectory (straight line)
            initial_traj = np.linspace(self.start, self.goal, self.num_waypoints)
            plt.plot(initial_traj[:, 0], initial_traj[:, 1],
                    'r--', linewidth=2, alpha=0.5, label='Initial Trajectory')

            # Plot optimized trajectory
            plt.plot(self.trajectory[:, 0], self.trajectory[:, 1],
                    'b-', linewidth=3, label='Optimized Trajectory')

            # Plot start and goal
            plt.plot(self.start[0], self.start[1],
                    'go', markersize=15, label='Start')
            plt.plot(self.goal[0], self.goal[1],
                    'ro', markersize=15, label='Goal')

            plt.xlabel('X')
            plt.ylabel('Y')
            plt.title('STOMP Trajectory Optimization')
            plt.legend()
            plt.grid(True)
            plt.axis('equal')
            plt.show()

# Example usage
def example_stomp():
    # Define start and goal
    start = np.array([0.0, 0.0])
    goal = np.array([5.0, 5.0])

    # Define obstacles
    obstacles = np.array([
        [1.0, 1.0], [1.5, 1.0], [2.0, 1.0],  # Wall 1
        [3.0, 3.0], [3.5, 3.0], [4.0, 3.0]   # Wall 2
    ])

    # Create and run STOMP optimizer
    stomp = STOMP(start, goal, obstacles, num_waypoints=50,
                  num_samples=50, learning_rate=0.1, noise_scale=0.05,
                  max_iterations=100)

    optimized_trajectory = stomp.optimize_trajectory()

    print(f"Optimization completed!")
    print(f"Final trajectory cost: {stomp.compute_trajectory_cost(optimized_trajectory)}")

    stomp.visualize()

    return optimized_trajectory

if __name__ == "__main__":
    example_stomp()
```

## Model Predictive Control (MPC) for Trajectory Optimization

MPC solves finite-horizon optimal control problems repeatedly in a receding horizon fashion.

```python
import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt

class ModelPredictiveController:
    def __init__(self,
                 A: np.ndarray,
                 B: np.ndarray,
                 Q: np.ndarray,
                 R: np.ndarray,
                 prediction_horizon: int = 10,
                 control_horizon: int = 5):
        """
        Linear Model Predictive Controller
        Args:
            A: State transition matrix (x_{k+1} = Ax_k + Bu_k)
            B: Control input matrix
            Q: State cost matrix
            R: Control cost matrix
            prediction_horizon: Number of steps to predict
            control_horizon: Number of control moves to optimize
        """
        self.A = A
        self.B = B
        self.Q = Q
        self.R = R
        self.prediction_horizon = prediction_horizon
        self.control_horizon = control_horizon

        self.nx = A.shape[0]  # Number of states
        self.nu = B.shape[1]  # Number of controls

    def compute_predicted_states(self, x0: np.ndarray, U: np.ndarray) -> np.ndarray:
        """
        Compute predicted state trajectory given initial state and control sequence
        Args:
            x0: Initial state
            U: Control sequence (control_horizon x nu)
        Returns:
            Predicted state trajectory (prediction_horizon+1 x nx)
        """
        X = np.zeros((self.prediction_horizon + 1, self.nx))
        X[0] = x0

        for k in range(self.prediction_horizon):
            if k < self.control_horizon:
                u = U[k]
            else:
                u = np.zeros(self.nu)  # Zero control after control horizon

            X[k+1] = self.A @ X[k] + self.B @ u

        return X

    def mpc_objective(self, U_flat: np.ndarray, x0: np.ndarray,
                     x_ref: np.ndarray) -> float:
        """
        MPC objective function to minimize
        """
        U = U_flat.reshape(self.control_horizon, self.nu)
        X = self.compute_predicted_states(x0, U)

        cost = 0.0

        # Running cost along trajectory
        for k in range(self.prediction_horizon):
            state_error = X[k] - x_ref
            cost += state_error.T @ self.Q @ state_error

            if k < self.control_horizon:
                cost += U[k].T @ self.R @ U[k]

        # Terminal cost
        final_error = X[self.prediction_horizon] - x_ref
        cost += final_error.T @ self.Q @ final_error

        return cost

    def solve_mpc(self, x0: np.ndarray, x_ref: np.ndarray,
                  umin: np.ndarray = None, umax: np.ndarray = None) -> np.ndarray:
        """
        Solve MPC optimization problem
        Args:
            x0: Current state
            x_ref: Reference state
            umin: Minimum control bounds
            umax: Maximum control bounds
        Returns:
            Optimal control for first time step
        """
        # Initial guess for control sequence
        U_init = np.zeros(self.control_horizon * self.nu)

        # Define bounds
        if umin is not None and umax is not None:
            bounds = [tuple(zip(umin, umax)) for _ in range(self.control_horizon)]
            bounds_flat = [b for sublist in bounds for b in sublist]
        else:
            bounds_flat = [(-10.0, 10.0) for _ in range(self.control_horizon * self.nu)]

        # Minimize the objective
        result = minimize(
            fun=lambda U_flat: self.mpc_objective(U_flat, x0, x_ref),
            x0=U_init,
            method='SLSQP',
            bounds=bounds_flat,
            options={'ftol': 1e-6}
        )

        if result.success:
            U_opt = result.x.reshape(self.control_horizon, self.nu)
            return U_opt[0]  # Return first control input
        else:
            print(f"MPC optimization failed: {result.message}")
            return np.zeros(self.nu)

    def simulate_closed_loop(self, x0: np.ndarray, x_ref: np.ndarray,
                           num_steps: int = 50,
                           umin: np.ndarray = None,
                           umax: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Simulate closed-loop MPC system
        """
        X_sim = np.zeros((num_steps + 1, self.nx))
        U_sim = np.zeros((num_steps, self.nu))

        X_sim[0] = x0
        x_current = x0.copy()

        for k in range(num_steps):
            # Solve MPC problem
            u_opt = self.solve_mpc(x_current, x_ref, umin, umax)

            # Apply control and simulate
            U_sim[k] = u_opt
            x_next = self.A @ x_current + self.B @ u_opt

            # Store for simulation
            X_sim[k+1] = x_next
            x_current = x_next

        return X_sim, U_sim

# Example: Double integrator system
def example_mpc():
    # System: double integrator x = [position, velocity]
    # Dynamics: x(k+1) = A*x(k) + B*u(k)
    dt = 0.1
    A = np.array([[1, dt], [0, 1]])  # Discretized double integrator
    B = np.array([[0.5*dt**2], [dt]])

    # Cost matrices
    Q = np.array([[10, 0], [0, 1]])  # Penalize position error more than velocity
    R = np.array([[0.1]])             # Penalize control effort

    # Create MPC controller
    mpc = ModelPredictiveController(A, B, Q, R,
                                   prediction_horizon=15,
                                   control_horizon=7)

    # Initial and reference states
    x0 = np.array([5.0, 0.0])    # Start at position 5 with zero velocity
    x_ref = np.array([0.0, 0.0]) # Reference is origin

    # Control bounds
    umin = np.array([-2.0])
    umax = np.array([2.0])

    # Simulate closed-loop system
    X_sim, U_sim = mpc.simulate_closed_loop(x0, x_ref, num_steps=100,
                                           umin=umin, umax=umax)

    # Plot results
    plt.figure(figsize=(15, 5))

    # Plot 1: State trajectories
    plt.subplot(1, 3, 1)
    plt.plot(X_sim[:, 0], 'b-', linewidth=2, label='Position')
    plt.plot(X_sim[:, 1], 'r-', linewidth=2, label='Velocity')
    plt.xlabel('Time Step')
    plt.ylabel('State Value')
    plt.title('State Trajectories')
    plt.legend()
    plt.grid(True)

    # Plot 2: Control input
    plt.subplot(1, 3, 2)
    plt.step(range(len(U_sim)), U_sim.flatten(), 'g-', where='post',
             linewidth=2, label='Control')
    plt.axhline(y=umax[0], color='r', linestyle='--', alpha=0.5, label='Control Limits')
    plt.axhline(y=umin[0], color='r', linestyle='--', alpha=0.5)
    plt.xlabel('Time Step')
    plt.ylabel('Control Input')
    plt.title('Control Input')
    plt.legend()
    plt.grid(True)

    # Plot 3: Phase portrait
    plt.subplot(1, 3, 3)
    plt.plot(X_sim[:, 0], X_sim[:, 1], 'b-', linewidth=2, label='Trajectory')
    plt.plot(X_sim[0, 0], X_sim[0, 1], 'go', markersize=10, label='Start')
    plt.plot(X_sim[-1, 0], X_sim[-1, 1], 'ro', markersize=10, label='End')
    plt.xlabel('Position')
    plt.ylabel('Velocity')
    plt.title('Phase Portrait')
    plt.legend()
    plt.grid(True)
    plt.axis('equal')

    plt.tight_layout()
    plt.show()

    print(f"Final state: {X_sim[-1]}")
    print(f"Final state error: {np.linalg.norm(X_sim[-1] - x_ref)}")

    return X_sim, U_sim

if __name__ == "__main__":
    example_mpc()
```

## Applications and Considerations

### Real-time Trajectory Optimization

For real-time applications, several considerations are important:

1. **Computational efficiency**: Use warm-starts, reduced horizons, or simplified models
2. **Robustness**: Account for model uncertainty and disturbances
3. **Stability**: Ensure closed-loop stability through appropriate cost design

### Multi-robot Trajectory Optimization

When coordinating multiple robots, additional constraints and objectives arise:

- Collision avoidance between robots
- Communication constraints
- Task allocation and coordination

### Humanoid Robot Trajectory Optimization

For humanoid robots, special considerations include:

- Balance and stability constraints
- ZMP (Zero Moment Point) constraints
- Joint limit and rate constraints
- Actuator limitations

## Conclusion

Trajectory optimization is a powerful tool for generating dynamically feasible and optimal robot motions. The choice of method depends on the specific requirements of the application, including real-time constraints, optimality requirements, and system complexity. Direct methods are often preferred for their simplicity and ability to handle complex constraints, while indirect methods can be more efficient for certain problems. Modern approaches like CHOMP and STOMP provide effective solutions for high-dimensional trajectory optimization with obstacle avoidance.
