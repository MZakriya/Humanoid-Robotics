# 3.6 Kinematic Applications

## Overview

Kinematic analysis and synthesis form the backbone of robotic system design and operation. This chapter explores the practical applications of kinematic principles across various robotic domains, from industrial automation to humanoid robotics. We examine how kinematic theory translates into real-world solutions, highlighting implementation challenges and advanced techniques.

## Industrial Robot Applications

### Pick-and-Place Operations

Pick-and-place robots represent one of the most common industrial applications, requiring precise positioning and rapid cycle times:

```python
import numpy as np
from scipy.spatial.transform import Rotation as R

class PickAndPlaceController:
    def __init__(self, robot_model):
        self.robot = robot_model
        self.joint_limits = robot_model.joint_limits

    def plan_pick_place_motion(self, pick_position, place_position,
                             pick_orientation=None, place_orientation=None):
        """
        Plan complete pick and place motion with approach/retract phases
        """
        # Define key waypoints
        waypoints = []

        # 1. Pre-pick approach (above pick position)
        pre_pick = pick_position.copy()
        pre_pick[2] += 0.1  # 10cm above pick position
        waypoints.append({
            'position': pre_pick,
            'orientation': pick_orientation or np.eye(3),
            'gripper': 'open',
            'phase': 'pre_approach'
        })

        # 2. Pick position
        waypoints.append({
            'position': pick_position,
            'orientation': pick_orientation or np.eye(3),
            'gripper': 'open',
            'phase': 'pick'
        })

        # 3. Post-pick lift (grasp and lift)
        post_pick = pick_position.copy()
        post_pick[2] += 0.1
        waypoints.append({
            'position': post_pick,
            'orientation': pick_orientation or np.eye(3),
            'gripper': 'closed',
            'phase': 'post_pick'
        })

        # 4. Pre-place approach (above place position)
        pre_place = place_position.copy()
        pre_place[2] += 0.1
        waypoints.append({
            'position': pre_place,
            'orientation': place_orientation or np.eye(3),
            'gripper': 'closed',
            'phase': 'pre_place'
        })

        # 5. Place position
        waypoints.append({
            'position': place_position,
            'orientation': place_orientation or np.eye(3),
            'gripper': 'closed',
            'phase': 'place'
        })

        # 6. Post-place retract
        post_place = place_position.copy()
        post_place[2] += 0.1
        waypoints.append({
            'position': post_place,
            'orientation': place_orientation or np.eye(3),
            'gripper': 'open',
            'phase': 'post_place'
        })

        return self.generate_trajectory(waypoints)

    def generate_trajectory(self, waypoints):
        """
        Generate smooth trajectory through waypoints
        """
        joint_trajectory = []
        cartesian_trajectory = []

        for i in range(len(waypoints) - 1):
            start_wp = waypoints[i]
            end_wp = waypoints[i + 1]

            # Linear Cartesian interpolation
            segment = self.interpolate_cartesian(
                start_wp['position'], end_wp['position'],
                start_wp['orientation'], end_wp['orientation'],
                steps=50
            )

            # Convert to joint space
            joint_segment = []
            for pose in segment:
                try:
                    joint_angles = self.robot.inverse_kinematics(
                        pose[:3, 3],  # Position
                        pose[:3, :3]  # Orientation
                    )
                    joint_segment.append(joint_angles)
                except:
                    # Handle IK failure with alternative solution
                    joint_segment.append(joint_trajectory[-1] if joint_trajectory else np.zeros(6))

            joint_trajectory.extend(joint_segment)
            cartesian_trajectory.extend(segment)

        return {
            'joint_trajectory': np.array(joint_trajectory),
            'cartesian_trajectory': np.array(cartesian_trajectory),
            'waypoints': waypoints
        }

    def interpolate_cartesian(self, start_pos, end_pos, start_rot, end_rot, steps=50):
        """
        Linear interpolation in Cartesian space with proper rotation interpolation
        """
        t_values = np.linspace(0, 1, steps)
        poses = []

        for t in t_values:
            # Linear position interpolation
            pos = (1 - t) * start_pos + t * end_pos

            # Slerp for rotation interpolation
            r_start = R.from_matrix(start_rot)
            r_end = R.from_matrix(end_rot)
            r_interpolated = R.from_quat(
                self.slerp(r_start.as_quat(), r_end.as_quat(), t)
            )
            rot = r_interpolated.as_matrix()

            # Create homogeneous transformation matrix
            pose = np.eye(4)
            pose[:3, :3] = rot
            pose[:3, 3] = pos
            poses.append(pose)

        return poses

    def slerp(self, q1, q2, t):
        """
        Spherical linear interpolation between quaternions
        """
        # Normalize quaternions
        q1 = q1 / np.linalg.norm(q1)
        q2 = q2 / np.linalg.norm(q2)

        # Calculate dot product
        dot = np.dot(q1, q2)

        # If dot product is negative, use negated q2
        if dot < 0.0:
            q2 = -q2
            dot = -dot

        # Calculate angle
        DOT_THRESHOLD = 0.9995
        if dot > DOT_THRESHOLD:
            # Linear interpolation for very close quaternions
            result = q1 + t * (q2 - q1)
            return result / np.linalg.norm(result)

        theta_0 = np.arccos(dot)
        sin_theta_0 = np.sin(theta_0)
        theta = theta_0 * t
        sin_theta = np.sin(theta)

        s0 = np.cos(theta) - dot * sin_theta / sin_theta_0
        s1 = sin_theta / sin_theta_0

        return s0 * q1 + s1 * q2
```

### Assembly Operations

Assembly tasks require precise control and force feedback:

```python
class AssemblyController:
    def __init__(self, robot_model, force_sensor):
        self.robot = robot_model
        self.force_sensor = force_sensor
        self.admittance_controller = AdmittanceController()

    def insert_fastener(self, hole_position, insertion_depth,
                       search_radius=0.005, force_threshold=50):
        """
        Perform fastener insertion with force control
        """
        # Approach phase - move to hole location
        approach_pose = self.robot.forward_kinematics(self.robot.get_current_joints())
        approach_pose[:3, 3] = hole_position + np.array([0, 0, 0.02])  # 2cm above hole

        # Execute approach
        self.move_to_pose(approach_pose, velocity=0.05)

        # Search phase - small circular motion to find hole
        search_path = self.generate_search_pattern(
            hole_position, search_radius, rotations=2
        )

        for pose in search_path:
            self.move_to_pose(pose, velocity=0.01)
            forces = self.force_sensor.get_forces()
            if np.any(np.abs(forces[:2]) < 2.0):  # Low lateral force = found hole
                break

        # Insertion phase - controlled force insertion
        insertion_trajectory = self.generate_insertion_trajectory(
            hole_position, insertion_depth
        )

        for i, pose in enumerate(insertion_trajectory):
            # Apply admittance control based on force feedback
            current_forces = self.force_sensor.get_forces()
            admittance_correction = self.admittance_controller.update(
                current_forces, force_threshold
            )

            corrected_pose = pose.copy()
            corrected_pose[:3, 3] += admittance_correction

            self.move_to_pose(corrected_pose, velocity=0.005)

    def generate_search_pattern(self, center, radius, rotations=2, steps_per_rotation=20):
        """
        Generate circular search pattern
        """
        poses = []
        total_steps = rotations * steps_per_rotation

        for i in range(total_steps):
            angle = 2 * np.pi * i / steps_per_rotation
            offset = np.array([radius * np.cos(angle), radius * np.sin(angle), 0])
            position = center + offset

            pose = np.eye(4)
            pose[:3, 3] = position
            poses.append(pose)

        return poses
```

## Humanoid Robot Applications

### Walking Pattern Generation

```python
class WalkingPatternGenerator:
    def __init__(self, robot_params):
        self.robot_params = robot_params
        self.foot_separation = robot_params['foot_separation']
        self.step_length = robot_params['step_length']
        self.step_height = robot_params['step_height']
        self.zmp_margin = robot_params['zmp_margin']

    def generate_walking_trajectory(self, num_steps, step_type='forward'):
        """
        Generate walking trajectory using inverted pendulum model
        """
        trajectories = {
            'left_foot': [],
            'right_foot': [],
            'com': [],
            'joints': []
        }

        # Initialize support foot based on step number
        support_foot = 'left'  # Start with left foot support

        for step in range(num_steps):
            # Determine current support and swing feet
            swing_foot = 'right' if support_foot == 'left' else 'left'

            # Generate step trajectory
            step_trajectory = self.generate_single_step(
                support_foot, swing_foot, step, step_type
            )

            # Append to overall trajectories
            for key in trajectories:
                trajectories[key].extend(step_trajectory[key])

            # Switch support foot for next step
            support_foot = swing_foot

        return trajectories

    def generate_single_step(self, support_foot, swing_foot, step_num, step_type):
        """
        Generate trajectory for a single step
        """
        # Calculate step target position
        if step_type == 'forward':
            step_offset = np.array([self.step_length, 0, 0])
        elif step_type == 'backward':
            step_offset = np.array([-self.step_length, 0, 0])
        elif step_type == 'lateral':
            step_offset = np.array([0, self.foot_separation, 0]) if step_num % 2 == 0 else np.array([0, -self.foot_separation, 0])
        else:
            step_offset = np.array([0, 0, 0])  # In place

        # Calculate swing foot trajectory (cycloid or polynomial)
        swing_start = self.get_foot_position(swing_foot, step_num)
        swing_target = swing_start + step_offset

        # Generate smooth trajectory with step height
        t_steps = np.linspace(0, 1, 100)
        swing_trajectory = []

        for t in t_steps:
            # Horizontal interpolation
            horizontal_pos = (1 - t) * swing_start[:2] + t * swing_target[:2]

            # Vertical trajectory (cycloid for natural foot path)
            vertical_pos = self.step_height * (1 - np.cos(np.pi * t)) / 2

            # Combine positions
            foot_pos = np.array([
                horizontal_pos[0],
                horizontal_pos[1],
                vertical_pos
            ])

            swing_trajectory.append(foot_pos)

        # Calculate CoM trajectory using inverted pendulum model
        com_trajectory = self.calculate_com_trajectory(swing_trajectory, support_foot)

        return {
            'left_foot': self.get_updated_foot_trajectory('left', swing_foot, swing_trajectory),
            'right_foot': self.get_updated_foot_trajectory('right', swing_foot, swing_trajectory),
            'com': com_trajectory,
            'joints': self.inverse_kinematics_batch(com_trajectory, swing_trajectory, support_foot)
        }

    def calculate_com_trajectory(self, swing_trajectory, support_foot):
        """
        Calculate Center of Mass trajectory for stable walking
        """
        # Use inverted pendulum model: CoM = ZMP + (CoM_height / gravity) * CoM_acceleration
        # For simplicity, we'll use a precomputed stable CoM trajectory
        com_height = self.robot_params['com_height']
        com_trajectory = []

        # Generate CoM trajectory that maintains ZMP within support polygon
        for i, foot_pos in enumerate(swing_trajectory):
            # Simplified CoM position that maintains balance
            if support_foot == 'left':
                zmp_target = np.array([foot_pos[0], foot_pos[1] + self.foot_separation/2, 0])
            else:
                zmp_target = np.array([foot_pos[0], foot_pos[1] - self.foot_separation/2, 0])

            # CoM should be slightly above and ahead of ZMP for stability
            com_pos = zmp_target + np.array([0.02, 0, com_height])  # Small forward offset
            com_trajectory.append(com_pos)

        return np.array(com_trajectory)
```

### Balance Control

```python
class BalanceController:
    def __init__(self, robot_model):
        self.robot = robot_model
        self.zmp_calculator = ZMPCalculator(robot_model)
        self.com_estimator = COMEstimator(robot_model)

    def stabilize_balance(self, measured_forces, measured_torques,
                         target_com_position, dt=0.001):
        """
        Real-time balance stabilization using ZMP control
        """
        # Calculate current ZMP from measured forces
        current_zmp = self.zmp_calculator.calculate_zmp(
            measured_forces, measured_torques
        )

        # Calculate current CoM
        current_com = self.com_estimator.estimate_com(
            self.robot.get_current_joints(),
            self.robot.get_current_velocities()
        )

        # Calculate ZMP error
        zmp_error = target_com_position[:2] - current_zmp[:2]

        # Design feedback controller (PD controller for ZMP error)
        self.zmp_integrator += zmp_error * dt
        zmp_derivative = (zmp_error - self.prev_zmp_error) / dt

        control_output = (
            self.kp_zmp * zmp_error +
            self.ki_zmp * self.zmp_integrator +
            self.kd_zmp * zmp_derivative
        )

        # Apply control to joint torques to adjust balance
        balance_torques = self.calculate_balance_torques(control_output)

        # Update previous error
        self.prev_zmp_error = zmp_error

        return balance_torques

    def calculate_balance_torques(self, zmp_correction):
        """
        Calculate joint torques needed to achieve ZMP correction
        """
        # Map ZMP correction to joint torques using Jacobian transpose
        # This is a simplified approach - real implementation would use
        # more sophisticated optimization methods

        # Calculate desired CoM acceleration
        com_acc_desired = self.map_zmp_to_com_acc(zmp_correction)

        # Use inverse dynamics to calculate required torques
        current_joints = self.robot.get_current_joints()
        current_velocities = self.robot.get_current_velocities()

        # Desired joint accelerations to achieve CoM acceleration
        joint_acc_desired = self.inverse_kinematics_acceleration(
            com_acc_desired, current_joints, current_velocities
        )

        # Calculate torques using inverse dynamics
        torques = self.robot.inverse_dynamics(
            current_joints, current_velocities, joint_acc_desired
        )

        return torques

    def map_zmp_to_com_acc(self, zmp_correction):
        """
        Map ZMP correction to required CoM acceleration
        """
        # Using inverted pendulum model: CoM_acceleration = g/CoM_height * (CoM_position - ZMP)
        com_height = self.robot.get_com_height()
        gravity = 9.81

        # Calculate required CoM position adjustment
        com_adjustment = zmp_correction * (com_height / gravity)

        # Convert to acceleration (simplified)
        com_acc = com_adjustment * (gravity / com_height)

        return np.append(com_acc, [0])  # Add z-component (0 for level balance)
```

## Medical Robotics Applications

### Surgical Robot Control

```python
class SurgicalRobotController:
    def __init__(self, robot_model, haptic_device, safety_limits):
        self.robot = robot_model
        self.haptic_device = haptic_device
        self.safety_limits = safety_limits
        self.tissue_model = TissueInteractionModel()

    def perform_surgical_task(self, target_trajectory, tool_orientation):
        """
        Execute surgical task with haptic feedback and safety constraints
        """
        safety_checker = SafetyConstraintChecker(self.safety_limits)

        for i, target_pose in enumerate(target_trajectory):
            # Verify safety constraints
            if not safety_checker.is_safe(target_pose):
                raise SafetyViolationError("Target pose violates safety constraints")

            # Calculate required joint configuration
            try:
                joint_angles = self.robot.inverse_kinematics(
                    target_pose[:3, 3],  # Position
                    target_pose[:3, :3],  # Orientation
                    initial_guess=self.robot.get_current_joints()
                )
            except InverseKinematicsError:
                # Plan alternative path around obstacle
                joint_angles = self.plan_safe_path_to_pose(target_pose)

            # Apply joint limits
            joint_angles = self.apply_joint_limits(joint_angles)

            # Calculate tissue interaction forces
            interaction_forces = self.tissue_model.calculate_interaction(
                target_pose, self.robot.get_actual_pose()
            )

            # Generate haptic feedback
            haptic_feedback = self.generate_haptic_feedback(interaction_forces)

            # Execute motion with force limiting
            self.execute_motion_with_force_limiting(
                joint_angles, interaction_forces, haptic_feedback
            )

    def generate_haptic_feedback(self, interaction_forces):
        """
        Generate haptic feedback based on tissue interaction
        """
        # Map interaction forces to haptic device
        haptic_force = np.clip(interaction_forces, -10, 10)  # Limit to 10N
        return haptic_force

    def execute_motion_with_force_limiting(self, target_joints,
                                         interaction_forces, haptic_feedback):
        """
        Execute motion while monitoring and limiting interaction forces
        """
        # Calculate motion using admittance control
        admittance_gain = 0.001  # Small admittance for precise control
        force_error = interaction_forces - self.safety_limits['max_force']

        # Adjust motion based on force feedback
        if np.any(np.abs(force_error) > 0.5):  # 0.5N threshold
            # Reduce motion speed or adjust trajectory
            adjustment = -admittance_gain * force_error
            target_joints += adjustment[:len(target_joints)]  # Map forces to joint adjustments

        # Send command to robot
        self.robot.set_joint_targets(target_joints)

        # Send haptic feedback
        self.haptic_device.apply_force(haptic_feedback)
```

## Service Robotics Applications

### Mobile Manipulation

```python
class MobileManipulatorController:
    def __init__(self, base_controller, arm_controller, navigation_system):
        self.base = base_controller
        self.arm = arm_controller
        self.navigation = navigation_system
        self.coordination_manager = TaskCoordinator()

    def perform_mobile_manipulation_task(self, task_description):
        """
        Coordinate base motion and arm manipulation for complex tasks
        """
        # Parse task requirements
        navigation_required = task_description.get('navigate_to', None)
        manipulation_required = task_description.get('manipulation', None)

        if navigation_required:
            # Navigate to task location
            self.navigate_to_location(navigation_required)

        if manipulation_required:
            # Perform manipulation task
            self.execute_manipulation(manipulation_required)

    def navigate_to_location(self, target_pose):
        """
        Navigate mobile base to target location while considering arm workspace
        """
        # Plan navigation path considering arm base position
        arm_base_pose = self.get_arm_base_pose()

        # Check if target is within arm's reachable workspace
        if self.is_in_arm_workspace(target_pose):
            # Direct navigation
            self.navigation.navigate_to(target_pose)
        else:
            # Find optimal base position that enables arm to reach target
            optimal_base_pose = self.find_base_pose_for_arm_accessibility(
                target_pose
            )
            self.navigation.navigate_to(optimal_base_pose)

    def find_base_pose_for_arm_accessibility(self, task_pose):
        """
        Find base pose that allows arm to reach task location
        """
        # Define search space around task location
        search_radius = 2.0  # 2 meter search radius
        step_size = 0.1

        best_pose = None
        min_cost = float('inf')

        # Grid search for optimal base position
        for dx in np.arange(-search_radius, search_radius, step_size):
            for dy in np.arange(-search_radius, search_radius, step_size):
                candidate_pose = np.copy(task_pose)
                candidate_pose[0] += dx
                candidate_pose[1] += dy

                # Check if arm can reach from this base position
                if self.can_arm_reach_from_base(candidate_pose, task_pose):
                    # Calculate cost (e.g., navigation distance, time, energy)
                    cost = self.calculate_navigation_cost(candidate_pose)

                    if cost < min_cost:
                        min_cost = cost
                        best_pose = candidate_pose

        return best_pose if best_pose is not None else task_pose

    def execute_manipulation(self, manipulation_task):
        """
        Execute coordinated manipulation task
        """
        # Decompose manipulation into subtasks
        subtasks = self.decompose_manipulation_task(manipulation_task)

        for subtask in subtasks:
            # Determine if base movement is needed for this subtask
            if self.requires_base_adjustment(subtask):
                # Temporarily move base for better manipulation access
                self.adjust_base_position(subtask)

            # Execute arm manipulation for subtask
            self.arm.execute_task(subtask)

    def decompose_manipulation_task(self, task):
        """
        Decompose complex manipulation task into simpler subtasks
        """
        # Example: "pick object from shelf and place in box"
        # becomes: [approach_shelf, grasp_object, lift_object, approach_box, place_object]

        if task['type'] == 'pick_and_place':
            return [
                {'type': 'approach', 'target': task['pick_location']},
                {'type': 'grasp', 'object': task['object']},
                {'type': 'lift', 'height': 0.1},
                {'type': 'approach', 'target': task['place_location']},
                {'type': 'place', 'target': task['place_location']}
            ]
        else:
            return [task]
```

## Advanced Kinematic Applications

### Redundancy Resolution in Complex Systems

```python
class RedundancyResolver:
    def __init__(self, robot_model, optimization_weights=None):
        self.robot = robot_model
        self.weights = optimization_weights or {
            'minimize_velocity': 1.0,
            'avoid_obstacles': 2.0,
            'avoid_joint_limits': 1.5,
            'energy_efficiency': 0.5
        }

    def resolve_redundancy_optimal(self, desired_task, current_joints,
                                 obstacles=None, joint_limits=None):
        """
        Resolve redundancy using optimization-based approach
        """
        from scipy.optimize import minimize

        def objective_function(joint_velocities):
            """
            Combined objective function with multiple criteria
            """
            # Primary task error
            jacobian = self.robot.calculate_jacobian(current_joints)
            task_error = np.linalg.norm(
                desired_task - jacobian @ joint_velocities
            )

            # Secondary objectives
            velocity_cost = self.weights['minimize_velocity'] * np.sum(joint_velocities**2)

            obstacle_cost = 0
            if obstacles:
                obstacle_cost = self.weights['avoid_obstacles'] * self.calculate_obstacle_cost(
                    current_joints, joint_velocities, obstacles
                )

            joint_limit_cost = 0
            if joint_limits:
                joint_limit_cost = self.weights['avoid_joint_limits'] * self.calculate_joint_limit_cost(
                    current_joints, joint_velocities, joint_limits
                )

            energy_cost = self.weights['energy_efficiency'] * self.calculate_energy_cost(
                joint_velocities
            )

            return task_error + velocity_cost + obstacle_cost + joint_limit_cost + energy_cost

        # Initial guess from pseudoinverse solution
        jacobian = self.robot.calculate_jacobian(current_joints)
        initial_guess = np.linalg.pinv(jacobian) @ desired_task

        # Optimization with constraints
        result = minimize(
            objective_function,
            initial_guess,
            method='SLSQP',
            constraints=self.get_kinematic_constraints(current_joints)
        )

        return result.x

    def calculate_obstacle_cost(self, current_joints, joint_velocities, obstacles):
        """
        Calculate cost based on proximity to obstacles
        """
        total_cost = 0

        # Calculate positions of all links
        link_positions = self.robot.calculate_link_positions(current_joints)

        for link_pos in link_positions:
            for obstacle in obstacles:
                distance = np.linalg.norm(link_pos - obstacle.position)
                if distance < obstacle.safety_radius:
                    # High cost when close to obstacle
                    cost = 1.0 / max(distance, 0.01)  # Avoid division by zero
                    total_cost += cost

        return total_cost

    def calculate_joint_limit_cost(self, current_joints, joint_velocities, joint_limits):
        """
        Calculate cost based on proximity to joint limits
        """
        cost = 0

        for i, (joint_pos, vel, limits) in enumerate(
            zip(current_joints, joint_velocities, joint_limits)
        ):
            # Calculate distance to limits
            dist_to_lower = joint_pos - limits[0]
            dist_to_upper = limits[1] - joint_pos

            # High velocity toward limits increases cost
            if vel > 0 and dist_to_upper < 0.2:  # Approaching upper limit
                cost += vel**2 / max(dist_to_upper, 0.01)
            elif vel < 0 and dist_to_lower < 0.2:  # Approaching lower limit
                cost += (-vel)**2 / max(dist_to_lower, 0.01)

        return cost
```

### Multi-Robot Coordination

```python
class MultiRobotCoordinator:
    def __init__(self, robots, communication_network):
        self.robots = robots
        self.network = communication_network
        self.task_allocator = TaskAllocator()

    def coordinate_multi_robot_task(self, global_task):
        """
        Coordinate multiple robots for complex task execution
        """
        # Decompose global task into subtasks
        subtasks = self.decompose_global_task(global_task)

        # Allocate subtasks to robots based on capabilities and positions
        task_assignments = self.task_allocator.allocate_tasks(
            subtasks, self.robots
        )

        # Execute coordinated task execution
        for robot_id, assigned_tasks in task_assignments.items():
            # Plan coordinated motion considering other robots
            coordinated_plan = self.plan_coordinated_motion(
                robot_id, assigned_tasks, task_assignments
            )

            # Execute plan with collision avoidance
            self.execute_coordinated_plan(robot_id, coordinated_plan)

    def plan_coordinated_motion(self, robot_id, own_tasks, all_assignments):
        """
        Plan motion considering other robots' plans
        """
        robot = self.robots[robot_id]
        other_robots = {rid: r for rid, r in self.robots.items() if rid != robot_id}

        # Get predicted trajectories of other robots
        other_trajectories = {}
        for other_id, other_tasks in all_assignments.items():
            if other_id != robot_id:
                other_trajectories[other_id] = self.predict_robot_trajectory(
                    other_id, other_tasks
                )

        # Plan own trajectory avoiding other robots
        coordinated_trajectory = self.generate_avoidance_trajectory(
            robot, own_tasks, other_trajectories
        )

        return coordinated_trajectory

    def generate_avoidance_trajectory(self, robot, tasks, other_trajectories):
        """
        Generate trajectory that avoids other robots
        """
        # Use velocity obstacles or reciprocal velocity obstacles
        # for collision avoidance
        trajectory = []

        for task in tasks:
            # Calculate desired motion for task
            desired_motion = self.calculate_task_motion(robot, task)

            # Check for conflicts with other robots
            for other_id, other_traj in other_trajectories.items():
                conflict = self.detect_motion_conflict(
                    robot.get_current_state(),
                    desired_motion,
                    other_traj
                )

                if conflict:
                    # Adjust motion to avoid conflict
                    desired_motion = self.resolve_conflict(
                        robot.get_current_state(),
                        desired_motion,
                        other_traj
                    )

            trajectory.append(desired_motion)

        return trajectory
```

## Real-World Implementation Considerations

### Real-Time Performance

```python
class RealTimeKinematicController:
    def __init__(self, robot_model, control_frequency=1000):
        self.robot = robot_model
        self.control_period = 1.0 / control_frequency
        self.kinematic_cache = {}  # Cache for repeated calculations

    def update_kinematics_realtime(self, joint_angles, joint_velocities):
        """
        Real-time kinematic updates with performance optimization
        """
        # Use cached calculations when possible
        cache_key = tuple(np.round(joint_angles, decimals=4))

        if cache_key in self.kinematic_cache:
            cached_result = self.kinematic_cache[cache_key]
        else:
            # Calculate Jacobian and forward kinematics
            jacobian = self.robot.calculate_jacobian(joint_angles)
            pose = self.robot.forward_kinematics(joint_angles)

            cached_result = {
                'jacobian': jacobian,
                'pose': pose
            }

            # Limit cache size
            if len(self.kinematic_cache) > 1000:
                # Remove oldest entries (simplified LRU)
                pass

            self.kinematic_cache[cache_key] = cached_result

        # Calculate forward velocity kinematics
        end_effector_velocity = cached_result['jacobian'] @ joint_velocities

        return {
            'pose': cached_result['pose'],
            'velocity': end_effector_velocity,
            'jacobian': cached_result['jacobian']
        }

    def optimize_computation(self):
        """
        Apply various optimization techniques
        """
        # 1. Pre-compute constant terms
        self.precompute_dh_constants()

        # 2. Use efficient data structures
        self.use_efficient_matrices()

        # 3. Implement parallel computation for multiple chains
        self.parallel_kinematics()

        # 4. Apply numerical optimization
        self.optimized_trigonometry()
```

## Summary

Kinematic applications span across all areas of robotics, from simple pick-and-place operations to complex humanoid locomotion and multi-robot coordination. The successful implementation of kinematic principles requires consideration of real-time performance, safety constraints, and the integration of multiple control systems. Modern robotics increasingly relies on optimization-based approaches to handle redundancy, obstacle avoidance, and multi-objective tasks. The field continues to evolve with advances in computational power, enabling more sophisticated real-time kinematic solutions for increasingly complex robotic applications.