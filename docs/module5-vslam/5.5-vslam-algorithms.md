---
title: VSLAM Algorithms
sidebar_label: VSLAM Algorithms
description: Comprehensive guide to Visual SLAM algorithms and their implementation for humanoid robotics
---

# 5.5 VSLAM Algorithms

## Overview

Visual Simultaneous Localization and Mapping (VSLAM) algorithms enable robots to simultaneously estimate their position in an unknown environment while building a map of that environment using visual sensors. For humanoid robotics applications, VSLAM algorithms must be robust, efficient, and capable of handling the dynamic nature of legged locomotion. This chapter covers the major VSLAM algorithmic approaches, their implementation details, and considerations for humanoid robotics applications.

## Taxonomy of VSLAM Algorithms

### Filter-based Approaches

Filter-based VSLAM algorithms maintain a probabilistic estimate of the robot's state using recursive Bayesian estimation:

```python
#!/usr/bin/env python3
"""
Filter-based VSLAM implementations
"""
import numpy as np
from typing import Tuple, List, Optional
import cv2

class ExtendedKalmanFilterSLAM:
    """Extended Kalman Filter SLAM implementation"""

    def __init__(self, state_dim: int, landmark_dim: int = 3):
        self.state_dim = state_dim  # [x, y, theta] + landmarks
        self.landmark_dim = landmark_dim
        self.state = np.zeros(state_dim)  # Robot pose + landmark positions
        self.covariance = np.eye(state_dim) * 1000  # High initial uncertainty
        self.process_noise = np.eye(state_dim) * 0.1
        self.measurement_noise = np.eye(2) * 0.5  # Range and bearing noise

        # State: [x, y, theta, lm1_x, lm1_y, lm2_x, lm2_y, ...]
        self.robot_dim = 3  # x, y, theta
        self.num_landmarks = (state_dim - self.robot_dim) // self.landmark_dim

    def predict(self, control_input: np.ndarray, dt: float):
        """
        Prediction step: predict state and covariance forward in time

        Args:
            control_input: [v, omega] velocity and angular velocity
            dt: Time step
        """
        v, omega = control_input

        # Extract robot state
        x, y, theta = self.state[:self.robot_dim]

        # Motion model (differential drive)
        if abs(omega) < 1e-6:  # Straight line motion
            dx = v * dt * np.cos(theta)
            dy = v * dt * np.sin(theta)
            dtheta = 0
        else:  # Circular motion
            dx = (v / omega) * (np.sin(theta + omega * dt) - np.sin(theta))
            dy = (v / omega) * (-np.cos(theta + omega * dt) + np.cos(theta))
            dtheta = omega * dt

        # Update robot state
        self.state[0] += dx
        self.state[1] += dy
        self.state[2] = (self.state[2] + dtheta) % (2 * np.pi)

        # Linearize motion model for Jacobian computation
        F = self._compute_motion_jacobian(control_input, dt)

        # Predict covariance
        self.covariance = F @ self.covariance @ F.T + self.process_noise

    def _compute_motion_jacobian(self, control_input: np.ndarray, dt: float) -> np.ndarray:
        """Compute Jacobian of motion model with respect to state"""
        v, omega = control_input
        x, y, theta = self.state[:self.robot_dim]

        F = np.eye(self.state_dim)

        if abs(omega) < 1e-6:  # Straight line
            # Jacobian of robot state w.r.t. robot state
            F[0, 2] = -v * dt * np.sin(theta)
            F[1, 2] = v * dt * np.cos(theta)
        else:  # Circular motion
            F[0, 2] = (v / omega) * (np.cos(theta + omega * dt) - np.cos(theta))
            F[1, 2] = (v / omega) * (np.sin(theta + omega * dt) - np.sin(theta))

        return F

    def update(self, measurements: List[Tuple[int, np.ndarray]]):
        """
        Update step: incorporate landmark measurements

        Args:
            measurements: List of (landmark_id, [range, bearing]) measurements
        """
        for landmark_id, meas in enumerate(measurements):
            range_meas, bearing_meas = meas

            # Compute expected measurement for this landmark
            landmark_idx = self.robot_dim + landmark_id * self.landmark_dim
            if landmark_idx + 1 >= len(self.state):
                continue  # Landmark not in state yet

            lm_x = self.state[landmark_idx]
            lm_y = self.state[landmark_idx + 1]

            robot_x, robot_y, robot_theta = self.state[:self.robot_dim]

            # Expected range and bearing
            dx = lm_x - robot_x
            dy = lm_y - robot_y
            expected_range = np.sqrt(dx**2 + dy**2)
            expected_bearing = np.arctan2(dy, dx) - robot_theta
            expected_bearing = (expected_bearing + np.pi) % (2 * np.pi) - np.pi  # Normalize

            expected_meas = np.array([expected_range, expected_bearing])

            # Measurement Jacobian
            H = self._compute_measurement_jacobian(robot_x, robot_y, lm_x, lm_y)

            # Kalman gain
            S = H @ self.covariance @ H.T + self.measurement_noise
            K = self.covariance @ H.T @ np.linalg.inv(S)

            # Innovation
            innovation = np.array([range_meas, bearing_meas]) - expected_meas
            innovation[1] = (innovation[1] + np.pi) % (2 * np.pi) - np.pi  # Normalize bearing

            # Update state and covariance
            self.state += K @ innovation
            self.covariance = (np.eye(self.state_dim) - K @ H) @ self.covariance

    def _compute_measurement_jacobian(self, robot_x: float, robot_y: float,
                                    lm_x: float, lm_y: float) -> np.ndarray:
        """Compute Jacobian of measurement function"""
        dx = lm_x - robot_x
        dy = lm_y - robot_y
        q = dx**2 + dy**2
        sqrt_q = np.sqrt(q)

        H = np.zeros((2, self.state_dim))

        # Range partial derivatives w.r.t. robot state
        H[0, 0] = -dx / sqrt_q  # dx/dr_x
        H[0, 1] = -dy / sqrt_q  # dx/dr_y

        # Bearing partial derivatives w.r.t. robot state
        H[1, 0] = dy / q       # dtheta/dr_x
        H[1, 1] = -dx / q      # dtheta/dr_y
        H[1, 2] = -1           # dtheta/dtheta

        # Partial derivatives w.r.t. landmark state
        landmark_start = self.robot_dim
        H[0, landmark_start] = dx / sqrt_q    # dx/dlm_x
        H[0, landmark_start + 1] = dy / sqrt_q  # dx/dlm_y
        H[1, landmark_start] = -dy / q       # dtheta/dlm_x
        H[1, landmark_start + 1] = dx / q     # dtheta/dlm_y

        return H

    def add_landmark(self, range_meas: float, bearing_meas: float):
        """Add a new landmark to the state vector"""
        robot_x, robot_y, robot_theta = self.state[:self.robot_dim]

        # Convert polar to Cartesian coordinates
        lm_x = robot_x + range_meas * np.cos(robot_theta + bearing_meas)
        lm_y = robot_y + range_meas * np.sin(robot_theta + bearing_meas)

        # Expand state vector and covariance matrix
        old_size = len(self.state)
        new_size = old_size + self.landmark_dim

        new_state = np.zeros(new_size)
        new_state[:old_size] = self.state
        new_state[old_size:old_size + self.landmark_dim] = [lm_x, lm_y]

        new_cov = np.zeros((new_size, new_size))
        new_cov[:old_size, :old_size] = self.covariance
        # Initialize new covariance elements with high uncertainty
        new_cov[old_size:new_size, old_size:new_size] = np.eye(self.landmark_dim) * 1000

        self.state = new_state
        self.covariance = new_cov
```

### Particle Filter SLAM

Particle filter approaches maintain a set of hypotheses about the robot's state:

```python
class FastSLAMParticleFilter:
    """FastSLAM implementation using particle filters"""

    def __init__(self, num_particles: int = 100, landmark_dim: int = 2):
        self.num_particles = num_particles
        self.landmark_dim = landmark_dim
        self.particles = []
        self.weights = np.ones(num_particles) / num_particles

        # Initialize particles
        for i in range(num_particles):
            # Each particle contains: [x, y, theta, landmark_means, landmark_covariances]
            particle_state = {
                'pose': np.random.normal(0, 0.1, 3),  # [x, y, theta]
                'landmarks': {},  # landmark_id -> {'mean': [x, y], 'cov': 2x2 matrix}
                'map': {}  # For storing landmark associations
            }
            self.particles.append(particle_state)

    def predict(self, control_input: np.ndarray, dt: float, process_noise: float = 0.1):
        """Predict particle states forward in time"""
        v, omega = control_input

        for i, particle in enumerate(self.particles):
            x, y, theta = particle['pose']

            # Add process noise
            noise = np.random.normal(0, process_noise, 3)

            if abs(omega) < 1e-6:  # Straight line motion
                new_x = x + v * dt * np.cos(theta) + noise[0]
                new_y = y + v * dt * np.sin(theta) + noise[1]
                new_theta = (theta + noise[2]) % (2 * np.pi)
            else:  # Circular motion
                new_x = x + (v / omega) * (np.sin(theta + omega * dt) - np.sin(theta)) + noise[0]
                new_y = y + (v / omega) * (-np.cos(theta + omega * dt) + np.cos(theta)) + noise[1]
                new_theta = (theta + omega * dt + noise[2]) % (2 * np.pi)

            particle['pose'] = np.array([new_x, new_y, new_theta])

    def update(self, measurements: List[Tuple[int, np.ndarray]],
               measurement_noise: np.ndarray = None):
        """
        Update particle weights based on measurements

        Args:
            measurements: List of (landmark_id, [range, bearing]) measurements
            measurement_noise: Measurement covariance matrix
        """
        if measurement_noise is None:
            measurement_noise = np.eye(2) * 0.5

        for i, particle in enumerate(self.particles):
            weight_update = 1.0

            for meas_landmark_id, meas in enumerate(measurements):
                range_meas, bearing_meas = meas

                # Check if this landmark has been observed before by this particle
                if meas_landmark_id in particle['landmarks']:
                    # Get expected measurement from particle's landmark estimate
                    lm_state = particle['landmarks'][meas_landmark_id]
                    expected_meas = self._expected_measurement(
                        particle['pose'], lm_state['mean']
                    )

                    # Calculate likelihood of measurement
                    innovation = np.array([range_meas, bearing_meas]) - expected_meas
                    innovation[1] = (innovation[1] + np.pi) % (2 * np.pi) - np.pi  # Normalize bearing

                    # Calculate likelihood (probability of measurement given state)
                    S = lm_state['cov'] + measurement_noise  # Innovation covariance
                    try:
                        inv_S = np.linalg.inv(S)
                        likelihood = np.exp(-0.5 * innovation.T @ inv_S @ innovation)
                        weight_update *= likelihood
                    except np.linalg.LinAlgError:
                        # Handle singular matrix case
                        weight_update *= 1.0
                else:
                    # New landmark - initialize it
                    self._initialize_landmark(particle, meas_landmark_id, range_meas, bearing_meas)

            # Update particle weight
            self.weights[i] *= weight_update

        # Normalize weights
        self.weights = self.weights / np.sum(self.weights)

        # Resample particles if effective sample size is low
        effective_particles = 1.0 / np.sum(self.weights**2)
        if effective_particles < self.num_particles / 2:
            self._resample()

    def _expected_measurement(self, robot_pose: np.ndarray, landmark_pos: np.ndarray) -> np.ndarray:
        """Calculate expected measurement given robot pose and landmark position"""
        x, y, theta = robot_pose
        lm_x, lm_y = landmark_pos

        dx = lm_x - x
        dy = lm_y - y

        expected_range = np.sqrt(dx**2 + dy**2)
        expected_bearing = np.arctan2(dy, dx) - theta
        expected_bearing = (expected_bearing + np.pi) % (2 * np.pi) - np.pi  # Normalize

        return np.array([expected_range, expected_bearing])

    def _initialize_landmark(self, particle: dict, landmark_id: int,
                           range_meas: float, bearing_meas: float):
        """Initialize a new landmark in the particle's map"""
        x, y, theta = particle['pose']

        # Convert polar to Cartesian coordinates
        lm_x = x + range_meas * np.cos(theta + bearing_meas)
        lm_y = y + range_meas * np.sin(theta + bearing_meas)

        # Initialize landmark with high uncertainty
        landmark_cov = np.eye(2) * 1000  # High initial uncertainty

        particle['landmarks'][landmark_id] = {
            'mean': np.array([lm_x, lm_y]),
            'cov': landmark_cov
        }

    def _resample(self):
        """Resample particles based on their weights"""
        # Systematic resampling
        new_particles = []
        cumulative_weights = np.cumsum(self.weights)
        step = 1.0 / self.num_particles
        start = np.random.uniform(0, step)

        for i in range(self.num_particles):
            pointer = start + i * step
            idx = np.searchsorted(cumulative_weights, pointer)
            new_particles.append(self._copy_particle(self.particles[idx]))

        self.particles = new_particles
        self.weights = np.ones(self.num_particles) / self.num_particles

    def _copy_particle(self, particle: dict) -> dict:
        """Create a copy of a particle"""
        import copy
        return copy.deepcopy(particle)

    def get_estimate(self) -> Tuple[np.ndarray, dict]:
        """Get the best estimate from particles"""
        # Weighted average of particle poses
        weighted_pose = np.zeros(3)
        for i, particle in enumerate(self.particles):
            weighted_pose += self.weights[i] * particle['pose']

        # Most likely landmark estimates
        # This is a simplified approach - in practice, you'd need more sophisticated combination
        best_particle_idx = np.argmax(self.weights)
        best_landmarks = self.particles[best_particle_idx]['landmarks']

        return weighted_pose, best_landmarks
```

## Keyframe-based Approaches

Keyframe-based methods store key frames and optimize the pose graph:

```python
class KeyframeSLAM:
    """Keyframe-based SLAM implementation"""

    def __init__(self, keyframe_threshold: float = 0.5,
                 max_keyframes: int = 100):
        self.keyframe_threshold = keyframe_threshold  # Minimum translation for new keyframe
        self.max_keyframes = max_keyframes

        self.keyframes = []  # List of keyframe poses and features
        self.keyframe_features = []  # Features in each keyframe
        self.current_pose = np.eye(4)  # Current camera/robot pose
        self.last_keyframe_pose = np.eye(4)

        # For feature tracking and matching
        self.feature_matcher = cv2.BFMatcher()
        self.detector = cv2.ORB_create()

        # Loop closure detection
        self.loop_detector = self._initialize_loop_detector()

    def _initialize_loop_detector(self):
        """Initialize loop closure detection for keyframe-based approach"""
        # This would typically use BoW or similar approach
        class SimpleLoopDetector:
            def __init__(self):
                self.keyframe_descriptors = []
                self.keyframe_poses = []

            def add_keyframe(self, descriptors, pose):
                self.keyframe_descriptors.append(descriptors)
                self.keyframe_poses.append(pose)

            def detect_loop_closure(self, query_descriptors, min_similarity=0.7):
                if len(self.keyframe_descriptors) < 10:  # Need some history
                    return None, 0.0

                # Simple descriptor matching for loop closure
                best_match_idx = -1
                best_similarity = 0.0

                for i, stored_desc in enumerate(self.keyframe_descriptors[:-5]):  # Skip recent
                    if query_descriptors is None or stored_desc is None:
                        continue

                    matches = cv2.BFMatcher().knnMatch(query_descriptors, stored_desc, k=2)

                    # Count good matches
                    good_matches = []
                    for match_pair in matches:
                        if len(match_pair) == 2:
                            m, n = match_pair
                            if m.distance < 0.75 * n.distance:
                                good_matches.append(m)

                    similarity = len(good_matches) / max(len(query_descriptors) if query_descriptors is not None else 1, 1)

                    if similarity > best_similarity and similarity >= min_similarity:
                        best_similarity = similarity
                        best_match_idx = i

                if best_match_idx >= 0:
                    return best_match_idx, best_similarity
                else:
                    return None, 0.0

        return SimpleLoopDetector()

    def process_frame(self, image: np.ndarray,
                     motion_estimate: np.ndarray = None) -> dict:
        """
        Process a single frame and decide whether to create a keyframe

        Args:
            image: Input image
            motion_estimate: Prior motion estimate (optional)

        Returns:
            Dictionary with processing results
        """
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

        # Extract features
        keypoints = self.detector.detect(gray)
        keypoints, descriptors = self.detector.compute(gray, keypoints)

        if descriptors is None:
            return {'is_keyframe': False, 'pose': self.current_pose}

        # Estimate pose relative to last keyframe using feature matching
        if len(self.keyframes) > 0:
            last_kf_features = self.keyframe_features[-1]
            if last_kf_features is not None:
                pose_update = self._estimate_pose_update(
                    last_kf_features, descriptors, keypoints
                )

                if pose_update is not None:
                    # Update current pose
                    self.current_pose = pose_update @ self.current_pose

        # Check if this should be a new keyframe
        translation_norm = np.linalg.norm(
            self.current_pose[:3, 3] - self.last_keyframe_pose[:3, 3]
        )

        is_keyframe = translation_norm > self.keyframe_threshold

        results = {
            'is_keyframe': is_keyframe,
            'pose': self.current_pose.copy(),
            'num_features': len(keypoints) if keypoints else 0
        }

        if is_keyframe:
            # Add as new keyframe
            self._add_keyframe(image, self.current_pose, keypoints, descriptors)
            results['keyframe_id'] = len(self.keyframes) - 1

            # Check for loop closures
            loop_result = self._check_for_loop_closure(descriptors)
            if loop_result[0] is not None:
                results['loop_closure'] = {
                    'keyframe_id': loop_result[0],
                    'similarity': loop_result[1]
                }

        return results

    def _estimate_pose_update(self, prev_descriptors: np.ndarray,
                            curr_descriptors: np.ndarray,
                            curr_keypoints: List[cv2.KeyPoint]) -> Optional[np.ndarray]:
        """Estimate pose update using feature matching"""
        if prev_descriptors is None or curr_descriptors is None:
            return None

        # Match features
        matches = self.feature_matcher.knnMatch(prev_descriptors, curr_descriptors, k=2)

        # Apply Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)

        if len(good_matches) < 10:  # Minimum for pose estimation
            return None

        # Get corresponding points
        prev_pts = np.float32([prev_descriptors[m.queryIdx] for m in good_matches]).reshape(-1, 1, 2)
        curr_pts = np.float32([curr_descriptors[m.trainIdx] for m in good_matches]).reshape(-1, 1, 2)

        # Compute essential matrix and pose
        E, mask = cv2.findEssentialMat(
            curr_pts, prev_pts,
            focal=1.0, pp=(0, 0),  # Normalized coordinates
            method=cv2.RANSAC,
            prob=0.999,
            threshold=1.0
        )

        if E is not None:
            # Recover pose
            _, R, t, _ = cv2.recoverPose(E, curr_pts, prev_pts, mask=mask)

            # Create transformation matrix
            T = np.eye(4)
            T[:3, :3] = R
            T[:3, 3] = t.flatten()
            return T

        return None

    def _add_keyframe(self, image: np.ndarray, pose: np.ndarray,
                     keypoints: List[cv2.KeyPoint], descriptors: np.ndarray):
        """Add a new keyframe to the system"""
        self.keyframes.append(pose.copy())
        self.keyframe_features.append(descriptors.copy())
        self.last_keyframe_pose = pose.copy()

        # Add to loop detector
        self.loop_detector.add_keyframe(descriptors, pose)

        # Limit number of keyframes if needed
        if len(self.keyframes) > self.max_keyframes:
            # Remove oldest keyframes (simple approach)
            remove_count = len(self.keyframes) - self.max_keyframes
            self.keyframes = self.keyframes[remove_count:]
            self.keyframe_features = self.keyframe_features[remove_count:]

    def _check_for_loop_closure(self, descriptors: np.ndarray) -> Tuple[Optional[int], float]:
        """Check for loop closures with previous keyframes"""
        return self.loop_detector.detect_loop_closure(descriptors)

    def optimize_poses(self):
        """Optimize keyframe poses using bundle adjustment or pose graph optimization"""
        # This would implement pose graph optimization
        # For this example, we'll use a simple approach

        if len(self.keyframes) < 2:
            return

        # In a real implementation, you would:
        # 1. Create a pose graph with constraints from feature matches
        # 2. Add loop closure constraints
        # 3. Optimize using g2o, Ceres, or similar optimization library
        print(f"Optimizing {len(self.keyframes)} keyframes...")

        # Placeholder for optimization
        pass
```

## Direct Methods

Direct methods work directly with pixel intensities rather than features:

```python
class DirectSLAM:
    """Direct SLAM implementation using photometric error minimization"""

    def __init__(self, camera_matrix: np.ndarray, width: int, height: int):
        self.camera_matrix = camera_matrix
        self.width = width
        self.height = height
        self.fx = camera_matrix[0, 0]
        self.fy = camera_matrix[1, 1]
        self.cx = camera_matrix[0, 2]
        self.cy = camera_matrix[1, 2]

        # Current and reference frames
        self.current_frame = None
        self.reference_frame = None
        self.reference_depth = None
        self.current_pose = np.eye(4)

        # Tracking state
        self.is_initialized = False
        self.tracking_quality = 0.0

    def initialize(self, first_frame: np.ndarray, depth_map: np.ndarray = None):
        """Initialize direct SLAM with first frame"""
        gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY) if len(first_frame.shape) == 3 else first_frame
        self.reference_frame = gray.astype(np.float32)

        if depth_map is not None:
            self.reference_depth = depth_map.astype(np.float32)
        else:
            # Create a dummy depth map if not provided
            self.reference_depth = np.ones_like(gray, dtype=np.float32) * 2.0  # 2 meters

        self.is_initialized = True

    def track_frame(self, current_frame: np.ndarray,
                   initial_pose: np.ndarray = None) -> Tuple[np.ndarray, bool]:
        """
        Track current frame against reference frame

        Args:
            current_frame: Current input frame
            initial_pose: Initial pose estimate (optional)

        Returns:
            (estimated_pose, tracking_success)
        """
        if not self.is_initialized:
            self.initialize(current_frame)
            return np.eye(4), True

        gray_current = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) if len(current_frame.shape) == 3 else current_frame
        self.current_frame = gray_current.astype(np.float32)

        if initial_pose is not None:
            pose = initial_pose.copy()
        else:
            pose = self.current_pose.copy()

        # Optimize pose using Gauss-Newton
        optimized_pose, success = self._optimize_pose(pose)

        if success:
            self.current_pose = optimized_pose
            # Update reference frame periodically
            if self._should_update_reference():
                self._update_reference_frame(current_frame)
            return self.current_pose, True
        else:
            return self.current_pose, False

    def _optimize_pose(self, initial_pose: np.ndarray,
                      max_iterations: int = 10) -> Tuple[np.ndarray, bool]:
        """Optimize camera pose using Gauss-Newton method"""
        pose = initial_pose.copy()

        for iteration in range(max_iterations):
            # Compute residuals and Jacobian
            residual, jacobian = self._compute_residual_and_jacobian(pose)

            if len(residual) == 0:
                return initial_pose, False

            # Solve normal equation: J^T * J * dx = -J^T * r
            JTJ = jacobian.T @ jacobian
            JTr = jacobian.T @ residual

            try:
                # Use SVD to solve for pose update
                U, s, Vt = np.linalg.svd(JTJ)
                s_inv = np.where(s > 1e-8, 1.0/s, 0)
                JTJ_inv = Vt.T @ np.diag(s_inv) @ U.T
                dx = JTJ_inv @ JTr

                # Update pose using Lie algebra
                updated_pose = self._exp_se3(dx) @ pose

                # Check for convergence
                if np.linalg.norm(dx) < 1e-6:
                    return updated_pose, True

                pose = updated_pose
            except np.linalg.LinAlgError:
                return initial_pose, False

        return pose, True

    def _compute_residual_and_jacobian(self, pose: np.ndarray):
        """Compute photometric residuals and their Jacobian"""
        height, width = self.reference_frame.shape

        # Generate 3D points from reference depth
        y_coords, x_coords = np.mgrid[0:height, 0:width]

        # Convert pixel coordinates to normalized coordinates
        x_norm = (x_coords - self.cx) / self.fx
        y_norm = (y_coords - self.cy) / self.fy

        # Create 3D points in reference camera frame
        z_ref = self.reference_depth
        x_ref = x_norm * z_ref
        y_ref = y_norm * z_ref

        points_3d_ref = np.stack([x_ref, y_ref, z_ref], axis=-1).reshape(-1, 3)

        # Transform points to current camera frame
        R = pose[:3, :3]
        t = pose[:3, 3]

        points_3d_curr = (R @ points_3d_ref.T).T + t

        # Project to current image
        valid_z = points_3d_curr[:, 2] > 0  # Only points in front of camera
        points_3d_curr = points_3d_curr[valid_z]

        if len(points_3d_curr) < 100:  # Require minimum valid points
            return np.array([]), np.zeros((1, 6))

        # Project to 2D
        x_curr = points_3d_curr[:, 0] / points_3d_curr[:, 2]
        y_curr = points_3d_curr[:, 1] / points_3d_curr[:, 2]

        u_curr = x_curr * self.fx + self.cx
        v_curr = y_curr * self.fy + self.cy

        # Check if projected points are within image bounds
        valid_bounds = (u_curr >= 0) & (u_curr < width) & (v_curr >= 0) & (v_curr < height)
        valid_projection = valid_bounds

        u_int = u_curr[valid_projection].astype(int)
        v_int = v_curr[valid_projection].astype(int)

        # Ensure indices are within bounds
        u_int = np.clip(u_int, 0, width - 1)
        v_int = np.clip(v_int, 0, height - 1)

        # Get pixel coordinates for reference points that have valid projections
        ref_coords = np.where(valid_z)[0][valid_bounds]
        if len(ref_coords) == 0:
            return np.array([]), np.zeros((1, 6))

        ref_u = (ref_coords % width).astype(int)
        ref_v = (ref_coords // width).astype(int)

        # Sample intensities
        ref_intensities = self.reference_frame[ref_v, ref_u]
        curr_intensities = self.current_frame[v_int, u_int]

        # Compute residuals (photometric error)
        residuals = (curr_intensities - ref_intensities).astype(np.float64)

        # Compute Jacobian
        # This is a simplified version - full implementation would be more complex
        jacobian = np.zeros((len(residuals), 6))

        return residuals, jacobian

    def _exp_se3(self, xi: np.ndarray) -> np.ndarray:
        """Exponential map from se(3) to SE(3)"""
        if len(xi) != 6:
            raise ValueError("xi must be 6-dimensional")

        rho = xi[:3]  # Translation part
        phi = xi[3:]  # Rotation part

        theta = np.linalg.norm(phi)

        if theta < 1e-8:
            # Small angle approximation
            R = np.eye(3)
            J = np.eye(3)
        else:
            # Rodrigues' formula for rotation
            axis = phi / theta
            c = np.cos(theta)
            s = np.sin(theta)
            C = 1 - c

            # Skew-symmetric matrix
            axis_skew = np.array([
                [0, -axis[2], axis[1]],
                [axis[2], 0, -axis[0]],
                [-axis[1], axis[0], 0]
            ])

            R = np.eye(3) + s * axis_skew + C * np.outer(axis, axis)

            # Jacobian for translation
            A = s / theta
            B = (1 - c) / (theta**2)
            C_term = (theta - s) / (theta**3)

            J = (A * np.eye(3) +
                 B * axis_skew +
                 C_term * np.outer(axis, axis))

        # Compute translation
        t = J @ rho

        # Create transformation matrix
        T = np.eye(4)
        T[:3, :3] = R
        T[:3, 3] = t

        return T

    def _should_update_reference(self) -> bool:
        """Determine if reference frame should be updated"""
        # Update reference frame if tracking quality is low
        # or if enough translation has occurred
        return np.random.random() < 0.1  # Simple periodic update

    def _update_reference_frame(self, new_frame: np.ndarray):
        """Update the reference frame"""
        gray = cv2.cvtColor(new_frame, cv2.COLOR_BGR2GRAY) if len(new_frame.shape) == 3 else new_frame
        self.reference_frame = gray.astype(np.float32)
```

## Semi-Direct Methods

Semi-direct methods combine the benefits of feature-based and direct methods:

```python
class SemiDirectSLAM:
    """Semi-direct SLAM combining feature tracking and direct alignment"""

    def __init__(self, camera_matrix: np.ndarray):
        self.camera_matrix = camera_matrix
        self.fx = camera_matrix[0, 0]
        self.fy = camera_matrix[1, 1]
        self.cx = camera_matrix[0, 2]
        self.cy = camera_matrix[1, 2]

        # Feature tracking components
        self.detector = cv2.ORB_create(nfeatures=1000)
        self.matcher = cv2.BFMatcher()

        # Direct alignment components
        self.direct_tracker = DirectSLAM(camera_matrix, 640, 480)  # Assuming 640x480

        # Tracking state
        self.current_pose = np.eye(4)
        self.tracked_features = []
        self.reference_features = []
        self.is_initialized = False

    def initialize(self, first_frame: np.ndarray):
        """Initialize semi-direct SLAM"""
        self.direct_tracker.initialize(first_frame)

        # Extract features from first frame
        gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY) if len(first_frame.shape) == 3 else first_frame
        keypoints = self.detector.detect(gray)
        keypoints, descriptors = self.detector.compute(gray, keypoints)

        self.reference_features = {
            'keypoints': keypoints,
            'descriptors': descriptors,
            'image': gray
        }

        self.is_initialized = True

    def track_frame(self, current_frame: np.ndarray) -> Tuple[np.ndarray, dict]:
        """
        Track frame using both feature-based and direct methods

        Returns:
            (estimated_pose, tracking_info)
        """
        if not self.is_initialized:
            self.initialize(current_frame)
            return np.eye(4), {'method': 'initialization', 'features_tracked': 0}

        gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) if len(current_frame.shape) == 3 else current_frame

        # Feature-based tracking
        feature_pose, feature_success = self._track_features(current_frame)

        # Direct tracking
        direct_pose, direct_success = self.direct_tracker.track_frame(
            current_frame, self.current_pose
        )

        # Combine results based on reliability
        if feature_success and direct_success:
            # Use weighted combination
            combined_pose = self._combine_poses(feature_pose, direct_pose, 0.7, 0.3)
            method_used = 'combined'
        elif feature_success:
            combined_pose = feature_pose
            method_used = 'feature_based'
        elif direct_success:
            combined_pose = direct_pose
            method_used = 'direct'
        else:
            # Keep previous pose
            combined_pose = self.current_pose
            method_used = 'failed'

        # Update current pose
        self.current_pose = combined_pose

        # Get tracking statistics
        tracking_info = {
            'method': method_used,
            'feature_success': feature_success,
            'direct_success': direct_success,
            'pose': combined_pose.copy()
        }

        return combined_pose, tracking_info

    def _track_features(self, current_frame: np.ndarray) -> Tuple[np.ndarray, bool]:
        """Track features and estimate pose"""
        gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) if len(current_frame.shape) == 3 else current_frame

        # Detect features in current frame
        curr_keypoints = self.detector.detect(gray)
        curr_keypoints, curr_descriptors = self.detector.compute(gray, curr_keypoints)

        if (curr_descriptors is None or
            self.reference_features['descriptors'] is None or
            len(curr_descriptors) < 10):
            return self.current_pose, False

        # Match features
        matches = self.matcher.knnMatch(
            self.reference_features['descriptors'],
            curr_descriptors, k=2
        )

        # Apply Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)

        if len(good_matches) < 10:  # Minimum for pose estimation
            return self.current_pose, False

        # Extract corresponding points
        ref_pts = np.float32([
            self.reference_features['keypoints'][m.queryIdx].pt for m in good_matches
        ]).reshape(-1, 1, 2)

        curr_pts = np.float32([
            curr_keypoints[m.trainIdx].pt for m in good_matches
        ]).reshape(-1, 1, 2)

        # Estimate essential matrix
        E, mask = cv2.findEssentialMat(
            curr_pts, ref_pts,
            self.camera_matrix,
            method=cv2.RANSAC,
            prob=0.999,
            threshold=1.0
        )

        if E is not None:
            # Recover pose
            _, R, t, mask_pose = cv2.recoverPose(E, curr_pts, ref_pts,
                                               mask=mask, cameraMatrix=self.camera_matrix)

            # Create transformation matrix
            T = np.eye(4)
            T[:3, :3] = R
            T[:3, 3] = t.flatten()

            # Apply to current pose
            new_pose = T @ self.current_pose
            return new_pose, True

        return self.current_pose, False

    def _combine_poses(self, pose1: np.ndarray, pose2: np.ndarray,
                      weight1: float, weight2: float) -> np.ndarray:
        """Combine two pose estimates"""
        # Simple linear interpolation of translation and rotation
        t1 = pose1[:3, 3]
        t2 = pose2[:3, 3]
        combined_t = weight1 * t1 + weight2 * t2

        # For rotation, we could use quaternion interpolation
        # Here we use a simplified approach
        R1 = pose1[:3, :3]
        R2 = pose2[:3, :3]

        # Compute average rotation (this is simplified)
        R_combined = R1 * weight1 + R2 * weight2
        # Orthogonalize the rotation matrix
        U, _, Vt = np.linalg.svd(R_combined)
        R_combined = U @ Vt

        combined_pose = np.eye(4)
        combined_pose[:3, :3] = R_combined
        combined_pose[:3, 3] = combined_t

        return combined_pose
```

## Modern VSLAM Systems

### ORB-SLAM Architecture

ORB-SLAM is one of the most successful feature-based VSLAM systems:

```python
class ORB_SLAM_System:
    """Conceptual implementation of ORB-SLAM architecture"""

    def __init__(self, camera_matrix: np.ndarray):
        self.camera_matrix = camera_matrix

        # Main threads/components
        self.tracking_thread = TrackingThread(camera_matrix)
        self.local_mapping_thread = LocalMappingThread()
        self.loop_closing_thread = LoopClosingThread()

        # System state
        self.system_state = 'NOT_INITIALIZED'
        self.current_frame_id = 0

    def process_frame(self, image: np.ndarray, timestamp: float) -> dict:
        """Process a single frame through the ORB-SLAM pipeline"""
        # Add frame to tracking queue
        frame = Frame(
            image=image,
            timestamp=timestamp,
            frame_id=self.current_frame_id,
            camera_matrix=self.camera_matrix
        )

        self.current_frame_id += 1

        # Process through tracking thread
        tracking_result = self.tracking_thread.process_frame(frame)

        # Add to local mapping if needed
        if tracking_result['need_new_keyframe']:
            self.local_mapping_thread.add_keyframe(frame)

        # System status
        result = {
            'pose': tracking_result.get('pose', np.eye(4)),
            'tracking_state': tracking_result.get('state', 'SYSTEM_NOT_READY'),
            'keyframe_added': tracking_result.get('new_keyframe', False),
            'timestamp': timestamp
        }

        return result

class Frame:
    """Frame representation for VSLAM system"""

    def __init__(self, image: np.ndarray, timestamp: float,
                 frame_id: int, camera_matrix: np.ndarray):
        self.image = image
        self.timestamp = timestamp
        self.frame_id = frame_id
        self.camera_matrix = camera_matrix

        # Extract features
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        self.detector = cv2.ORB_create(nfeatures=2000)
        self.keypoints = self.detector.detect(gray)
        self.keypoints, self.descriptors = self.detector.compute(gray, self.keypoints)

        # Pose (initially identity)
        self.pose = np.eye(4)

        # Associated map points
        self.map_points = []

class TrackingThread:
    """Tracking thread of ORB-SLAM system"""

    def __init__(self, camera_matrix: np.ndarray):
        self.camera_matrix = camera_matrix
        self.state = 'SYSTEM_NOT_READY'
        self.current_pose = np.eye(4)
        self.last_frame = None
        self.keyframes = []

        # ORB feature detector
        self.detector = cv2.ORB_create(nfeatures=2000)
        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING)

    def process_frame(self, frame: Frame) -> dict:
        """Process a frame for tracking"""
        if self.state == 'SYSTEM_NOT_READY':
            self.state = 'NO_IMAGES_YET'

        if self.state == 'NO_IMAGES_YET':
            self.last_frame = frame
            self.state = 'NOT_INITIALIZED'
            return {'state': self.state, 'pose': frame.pose}

        elif self.state == 'NOT_INITIALIZED':
            # Try to initialize the map
            success = self._initialize_map(frame)
            if success:
                self.state = 'OK'
                self.last_frame = frame
                return {'state': self.state, 'pose': frame.pose, 'new_keyframe': True}
            else:
                self.last_frame = frame
                return {'state': self.state, 'pose': frame.pose}

        elif self.state == 'OK':
            # Track the frame
            pose, tracking_success = self._track_frame(frame)

            if tracking_success:
                frame.pose = pose
                self.current_pose = pose

                # Check if we need a new keyframe
                need_keyframe = self._need_new_keyframe(frame)

                self.last_frame = frame
                return {
                    'state': self.state,
                    'pose': pose,
                    'new_keyframe': need_keyframe
                }
            else:
                # Tracking failed, need to relocalize
                self.state = 'LOST'
                return {'state': 'LOST', 'pose': self.current_pose}

        elif self.state == 'LOST':
            # Try to relocalize
            relocalize_success = self._relocalize(frame)
            if relocalize_success:
                self.state = 'OK'
                self.last_frame = frame
                return {'state': 'OK', 'pose': self.current_pose}
            else:
                return {'state': 'LOST', 'pose': self.current_pose}

        return {'state': self.state, 'pose': self.current_pose}

    def _initialize_map(self, frame: Frame) -> bool:
        """Initialize the map from the first two frames"""
        # This is a simplified initialization
        # In ORB-SLAM, this involves stereo initialization or motion-based init
        return True  # Simplified

    def _track_frame(self, frame: Frame) -> Tuple[np.ndarray, bool]:
        """Track the current frame against the map"""
        # In a real implementation, this would involve:
        # 1. Track with motion model
        # 2. Track with local map
        # 3. Relocalization if tracking fails

        # Simplified tracking using feature matching with last frame
        if self.last_frame and self.last_frame.descriptors is not None and frame.descriptors is not None:
            matches = self.matcher.knnMatch(
                self.last_frame.descriptors,
                frame.descriptors,
                k=2
            )

            # Apply Lowe's ratio test
            good_matches = []
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < 0.75 * n.distance:
                        good_matches.append(m)

            if len(good_matches) >= 10:
                # Estimate motion using essential matrix
                last_pts = np.float32([
                    self.last_frame.keypoints[m.queryIdx].pt for m in good_matches
                ]).reshape(-1, 1, 2)

                curr_pts = np.float32([
                    frame.keypoints[m.trainIdx].pt for m in good_matches
                ]).reshape(-1, 1, 2)

                E, mask = cv2.findEssentialMat(
                    curr_pts, last_pts,
                    self.camera_matrix,
                    method=cv2.RANSAC,
                    prob=0.999,
                    threshold=1.0
                )

                if E is not None:
                    _, R, t, mask_pose = cv2.recoverPose(
                        E, curr_pts, last_pts,
                        cameraMatrix=self.camera_matrix
                    )

                    # Create transformation matrix
                    T = np.eye(4)
                    T[:3, :3] = R
                    T[:3, 3] = t.flatten()

                    # Apply to last frame's pose
                    new_pose = T @ self.last_frame.pose
                    return new_pose, True

        return self.current_pose, False

    def _need_new_keyframe(self, frame: Frame) -> bool:
        """Determine if a new keyframe is needed"""
        # In ORB-SLAM, this involves multiple checks:
        # - Tracking quality
        # - Far enough from last keyframe
        # - Enough new map points
        return len(frame.keypoints) > 100  # Simplified condition

class LocalMappingThread:
    """Local mapping thread of ORB-SLAM system"""

    def __init__(self):
        self.keyframes = []
        self.map_points = []
        self.buffers = {
            'keyframe_queue': [],
            'local_keyframes': [],
            'local_points': []
        }

    def add_keyframe(self, frame: Frame):
        """Add a new keyframe to the map"""
        self.keyframes.append(frame)
        # In real implementation, would perform local bundle adjustment

class LoopClosingThread:
    """Loop closing thread of ORB-SLAM system"""

    def __init__(self):
        self.loop_detector = self._initialize_loop_detector()
        self.optimizer = PoseGraphOptimizer()  # From previous examples

    def _initialize_loop_detector(self):
        """Initialize loop closure detection"""
        # This would typically use Bag of Words approach
        return BagOfWordsLoopClosure(vocabulary_size=1000)
```

### LSD-SLAM (Direct Method)

LSD-SLAM is a popular direct method:

```python
class LSD_SLAM_System:
    """LSD-SLAM implementation (Large-Scale Direct SLAM)"""

    def __init__(self, camera_matrix: np.ndarray, width: int, height: int):
        self.camera_matrix = camera_matrix
        self.width = width
        self.height = height
        self.fx = camera_matrix[0, 0]
        self.fy = camera_matrix[1, 1]
        self.cx = camera_matrix[0, 2]
        self.cy = camera_matrix[1, 2]

        # Keyframe management
        self.keyframes = []
        self.current_frame = None
        self.current_pose = np.eye(4)

        # Depth map management
        self.use_inverse_depth_parameterization = True

        # Tracking and mapping parameters
        self.min_grad_threshold = 10.0
        self.frame_skip = 5  # Process every 5th frame
        self.frame_counter = 0

    def process_frame(self, image: np.ndarray,
                     initial_pose: np.ndarray = None) -> dict:
        """Process a frame in LSD-SLAM system"""
        self.frame_counter += 1

        if self.frame_counter % self.frame_skip != 0:
            # Skip this frame for efficiency
            return {
                'pose': self.current_pose,
                'processed': False,
                'keyframe_added': False
            }

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        self.current_frame = gray.astype(np.float32)

        if len(self.keyframes) == 0:
            # Initialize first keyframe
            self._initialize_first_keyframe(image)
            return {
                'pose': self.current_pose,
                'processed': True,
                'keyframe_added': True
            }

        # Track current frame against last keyframe
        success = self._track_frame(initial_pose)

        result = {
            'pose': self.current_pose,
            'processed': True,
            'tracking_success': success
        }

        if success and self._should_add_keyframe():
            self._add_current_as_keyframe()
            result['keyframe_added'] = True

        return result

    def _initialize_first_keyframe(self, image: np.ndarray):
        """Initialize the first keyframe with identity pose"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

        keyframe = {
            'image': gray.astype(np.float32),
            'pose': np.eye(4),
            'id': 0,
            'depth_map': np.ones_like(gray, dtype=np.float32) * 2.0,  # Initial depth
            'valid_pixels': np.ones_like(gray, dtype=bool)
        }

        self.keyframes.append(keyframe)
        self.current_pose = np.eye(4)

    def _track_frame(self, initial_pose: np.ndarray = None) -> bool:
        """Track current frame against reference keyframe"""
        if len(self.keyframes) == 0:
            return False

        ref_keyframe = self.keyframes[-1]  # Last keyframe
        ref_image = ref_keyframe['image']
        ref_depth = ref_keyframe['depth_map']

        if initial_pose is not None:
            pose = initial_pose.copy()
        else:
            pose = self.current_pose.copy()

        # Direct alignment using photometric error minimization
        optimized_pose, success = self._direct_align_frame(
            self.current_frame, ref_image, ref_depth, pose
        )

        if success:
            self.current_pose = optimized_pose
            return True

        return False

    def _direct_align_frame(self, curr_img: np.ndarray, ref_img: np.ndarray,
                           ref_depth: np.ndarray, initial_pose: np.ndarray,
                           max_iterations: int = 10) -> Tuple[np.ndarray, bool]:
        """Direct alignment of current frame to reference frame"""
        pose = initial_pose.copy()

        for iteration in range(max_iterations):
            # Compute residuals and Jacobian
            residual, jacobian, valid_count = self._compute_photometric_residual(
                curr_img, ref_img, ref_depth, pose
            )

            if valid_count < 50:  # Require minimum valid pixels
                return initial_pose, False

            # Solve normal equation: J^T * J * dx = -J^T * r
            JTJ = jacobian.T @ jacobian
            JTr = jacobian.T @ residual

            try:
                # Solve for pose update
                dx = np.linalg.solve(JTJ, -JTr)

                # Update pose
                update_matrix = self._se3_to_SE3(dx)
                pose = update_matrix @ pose

                # Check for convergence
                if np.linalg.norm(dx) < 1e-8:
                    break

            except np.linalg.LinAlgError:
                return initial_pose, False

        return pose, True

    def _compute_photometric_residual(self, curr_img: np.ndarray,
                                     ref_img: np.ndarray, ref_depth: np.ndarray,
                                     pose: np.ndarray):
        """Compute photometric residuals between frames"""
        height, width = ref_img.shape

        # Create pixel coordinates grid
        y_coords, x_coords = np.mgrid[0:height, 0:width]

        # Convert to normalized coordinates
        x_norm = (x_coords - self.cx) / self.fx
        y_norm = (y_coords - self.cy) / self.fy

        # Create 3D points in reference frame
        z_ref = ref_depth
        x_ref = x_norm * z_ref
        y_ref = y_norm * z_ref

        # Stack to get 3D points
        points_3d_ref = np.stack([x_ref, y_ref, z_ref], axis=-1).reshape(-1, 3)

        # Transform to current camera frame
        R = pose[:3, :3]
        t = pose[:3, 3]
        points_3d_curr = (R @ points_3d_ref.T).T + t

        # Only consider points in front of camera
        valid_z = points_3d_curr[:, 2] > 0
        points_3d_curr = points_3d_curr[valid_z]

        if len(points_3d_curr) == 0:
            return np.array([]), np.zeros((1, 6)), 0

        # Project to current image
        x_proj = points_3d_curr[:, 0] / points_3d_curr[:, 2]
        y_proj = points_3d_curr[:, 1] / points_3d_curr[:, 2]

        u_proj = x_proj * self.fx + self.cx
        v_proj = y_proj * self.fy + self.cy

        # Check image bounds
        valid_bounds = (
            (u_proj >= 0) & (u_proj < width) &
            (v_proj >= 0) & (v_proj < height)
        )

        valid_projection = valid_bounds
        valid_indices = np.where(valid_z)[0][valid_bounds]

        if len(valid_projection) == 0:
            return np.array([]), np.zeros((1, 6)), 0

        u_int = u_proj[valid_projection].astype(int)
        v_int = v_proj[valid_projection].astype(int)

        # Ensure indices are within bounds
        u_int = np.clip(u_int, 0, width - 1)
        v_int = np.clip(v_int, 0, height - 1)

        # Get corresponding reference pixels
        ref_u = (valid_indices % width).astype(int)
        ref_v = (valid_indices // width).astype(int)

        # Sample intensities
        ref_intensities = ref_img[ref_v, ref_u]
        curr_intensities = curr_img[v_int, u_int]

        # Compute residuals
        residuals = (curr_intensities - ref_intensities).astype(np.float64)

        # Compute Jacobian (simplified)
        jacobian = np.zeros((len(residuals), 6))

        return residuals, jacobian, len(residuals)

    def _se3_to_SE3(self, xi: np.ndarray) -> np.ndarray:
        """Convert se(3) element to SE(3) transformation matrix"""
        if len(xi) != 6:
            raise ValueError("xi must be 6-dimensional")

        rho = xi[:3]  # Translation part
        phi = xi[3:]  # Rotation part

        theta = np.linalg.norm(phi)

        if theta < 1e-8:
            # Small angle approximation
            R = np.eye(3)
            J = np.eye(3)
        else:
            # Rodrigues' formula
            axis = phi / theta
            c = np.cos(theta)
            s = np.sin(theta)
            C = 1 - c

            # Skew-symmetric matrix
            axis_skew = np.array([
                [0, -axis[2], axis[1]],
                [axis[2], 0, -axis[0]],
                [-axis[1], axis[0], 0]
            ])

            R = np.eye(3) + s * axis_skew + C * np.outer(axis, axis)

            # Jacobian for translation
            A = s / theta
            B = (1 - c) / (theta**2)
            C_term = (theta - s) / (theta**3)

            J = A * np.eye(3) + B * axis_skew + C_term * np.outer(axis, axis)

        # Compute translation
        t = J @ rho

        # Create transformation matrix
        T = np.eye(4)
        T[:3, :3] = R
        T[:3, 3] = t

        return T

    def _should_add_keyframe(self) -> bool:
        """Determine if a new keyframe should be added"""
        if len(self.keyframes) == 0:
            return True

        # Check distance to last keyframe
        last_kf_pose = self.keyframes[-1]['pose']
        current_translation = np.linalg.norm(
            self.current_pose[:3, 3] - last_kf_pose[:3, 3]
        )

        # Add keyframe if translation is large enough
        return current_translation > 0.5  # 50 cm threshold

    def _add_current_as_keyframe(self):
        """Add current frame as a new keyframe"""
        keyframe = {
            'image': self.current_frame.copy(),
            'pose': self.current_pose.copy(),
            'id': len(self.keyframes),
            'depth_map': self._initialize_depth_map(self.current_frame),
            'valid_pixels': np.ones_like(self.current_frame, dtype=bool)
        }

        self.keyframes.append(keyframe)

    def _initialize_depth_map(self, image: np.ndarray) -> np.ndarray:
        """Initialize depth map for a new keyframe"""
        # In LSD-SLAM, depth is estimated using the semi-dense approach
        # For simplicity, we'll initialize with a flat depth
        return np.ones_like(image, dtype=np.float32) * 2.0  # 2 meters
```

## Performance Considerations for Humanoid Robotics

### Real-time Optimization

```python
class RealTimeVSLAMOptimizer:
    """Real-time optimization for VSLAM in humanoid robotics"""

    def __init__(self, max_processing_time: float = 0.033):  # ~30 FPS
        self.max_processing_time = max_processing_time
        self.processing_times = []
        self.frame_times = []

    def optimize_processing(self, frame_func, *args, **kwargs):
        """Optimize frame processing to meet real-time constraints"""
        import time
        start_time = time.time()

        result = frame_func(*args, **kwargs)

        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)

        # Adjust processing parameters if needed
        if processing_time > self.max_processing_time:
            # Reduce feature count, simplify algorithms, etc.
            self._adjust_for_performance()

        return result

    def _adjust_for_performance(self):
        """Adjust VSLAM parameters for better performance"""
        # This would modify parameters like:
        # - Feature count reduction
        # - Lower resolution processing
        # - Simplified optimization
        # - Reduced keyframe frequency
        pass

    def get_performance_stats(self) -> dict:
        """Get current performance statistics"""
        if len(self.processing_times) == 0:
            return {'avg_processing_time': 0, 'fps': 0, 'utilization': 0}

        avg_time = np.mean(self.processing_times[-100:])  # Last 100 frames
        avg_fps = 1.0 / avg_time if avg_time > 0 else 0
        utilization = avg_time / self.max_processing_time

        return {
            'avg_processing_time': avg_time,
            'target_processing_time': self.max_processing_time,
            'avg_fps': avg_fps,
            'target_fps': 1.0 / self.max_processing_time,
            'utilization': utilization,
            'frames_processed': len(self.processing_times)
        }
```

## Algorithm Selection for Humanoid Robotics

### Adaptive Algorithm Selection

```python
class AdaptiveVSLAMSelector:
    """Select the most appropriate VSLAM algorithm based on conditions"""

    def __init__(self):
        self.algorithms = {
            'feature_based': None,  # ORB-SLAM style
            'direct': None,         # LSD-SLAM style
            'semi_direct': None,    # SVO style
            'filter_based': None    # EKF/Particle filter
        }

        self.performance_monitor = {
            'feature_based': {'success_rate': 0.9, 'processing_time': 0.04},
            'direct': {'success_rate': 0.8, 'processing_time': 0.02},
            'semi_direct': {'success_rate': 0.85, 'processing_time': 0.03},
            'filter_based': {'success_rate': 0.7, 'processing_time': 0.05}
        }

    def select_algorithm(self, scene_features: dict,
                        computational_budget: float,
                        accuracy_requirements: float) -> str:
        """
        Select the most appropriate VSLAM algorithm

        Args:
            scene_features: Dictionary with scene characteristics
            computational_budget: Maximum allowed processing time
            accuracy_requirements: Required accuracy level (0-1)

        Returns:
            Algorithm name to use
        """
        candidates = []

        for alg_name, perf in self.performance_monitor.items():
            # Check computational constraint
            if perf['processing_time'] > computational_budget:
                continue

            # Check if algorithm is suitable for scene
            suitability_score = self._assess_suitability(alg_name, scene_features)

            # Combine suitability with performance
            score = (suitability_score * 0.6 +
                    perf['success_rate'] * 0.4)

            candidates.append((alg_name, score))

        if not candidates:
            return 'feature_based'  # Default fallback

        # Return algorithm with highest score
        best_alg, _ = max(candidates, key=lambda x: x[1])
        return best_alg

    def _assess_suitability(self, alg_name: str, scene_features: dict) -> float:
        """Assess how suitable an algorithm is for the current scene"""
        texture_level = scene_features.get('texture_level', 0.5)
        lighting_condition = scene_features.get('lighting_condition', 'normal')
        motion_level = scene_features.get('motion_level', 0.5)

        if alg_name == 'direct':
            # Direct methods work better with high texture
            if texture_level < 0.3:
                return 0.3  # Poor for low texture
            elif lighting_condition == 'changing':
                return 0.4  # Sensitive to lighting
            else:
                return 0.9  # Good for high texture, stable lighting

        elif alg_name == 'feature_based':
            # Feature-based methods are more robust to lighting changes
            if lighting_condition == 'changing':
                return 0.8  # Good for varying lighting
            elif texture_level < 0.2:
                return 0.5  # Needs some texture
            else:
                return 0.9  # Generally good

        elif alg_name == 'semi_direct':
            # Compromise between feature and direct methods
            return 0.7  # Generally robust

        else:  # filter_based
            return 0.6  # Moderate performance

    def update_performance(self, alg_name: str, success: bool,
                          processing_time: float):
        """Update performance statistics for an algorithm"""
        if alg_name in self.performance_monitor:
            # Update success rate with exponential moving average
            current_rate = self.performance_monitor[alg_name]['success_rate']
            self.performance_monitor[alg_name]['success_rate'] = 0.9 * current_rate + 0.1 * success

            # Update processing time
            current_time = self.performance_monitor[alg_name]['processing_time']
            self.performance_monitor[alg_name]['processing_time'] = 0.9 * current_time + 0.1 * processing_time
```

## Evaluation and Comparison

### VSLAM Algorithm Benchmarking

```python
class VSLAMEvaluator:
    """Evaluate and compare different VSLAM algorithms"""

    def __init__(self):
        self.metrics = {
            'accuracy': [],      # Pose accuracy
            'efficiency': [],    # Processing time
            'robustness': [],    # Success rate
            'consistency': []    # Map consistency
        }

    def evaluate_algorithm(self, algorithm, dataset: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],
                          ground_truth_poses: List[np.ndarray]) -> dict:
        """
        Evaluate a VSLAM algorithm on a dataset

        Args:
            algorithm: VSLAM algorithm instance
            dataset: List of (image, depth, camera_matrix) tuples
            ground_truth_poses: Ground truth poses for evaluation

        Returns:
            Dictionary with evaluation metrics
        """
        estimated_poses = []
        processing_times = []

        for i, (image, depth, camera_matrix) in enumerate(dataset):
            import time
            start_time = time.time()

            # Process frame
            if hasattr(algorithm, 'process_frame'):
                result = algorithm.process_frame(image)
                pose = result.get('pose', np.eye(4))
            else:
                pose = np.eye(4)  # Default

            processing_time = time.time() - start_time
            processing_times.append(processing_time)

            estimated_poses.append(pose)

        # Calculate metrics
        metrics = self._calculate_metrics(
            estimated_poses, ground_truth_poses, processing_times
        )

        return metrics

    def _calculate_metrics(self, estimated_poses: List[np.ndarray],
                          ground_truth_poses: List[np.ndarray],
                          processing_times: List[float]) -> dict:
        """Calculate evaluation metrics"""
        if len(estimated_poses) != len(ground_truth_poses):
            raise ValueError("Pose sequences must have same length")

        # Calculate pose errors
        translation_errors = []
        rotation_errors = []

        for est_pose, gt_pose in zip(estimated_poses, ground_truth_poses):
            # Translation error
            trans_error = np.linalg.norm(est_pose[:3, 3] - gt_pose[:3, 3])
            translation_errors.append(trans_error)

            # Rotation error
            R_error = est_pose[:3, :3] @ gt_pose[:3, :3].T
            rotation_error = np.arccos(
                np.clip((np.trace(R_error) - 1) / 2, -1, 1)
            )
            rotation_errors.append(rotation_error)

        # Calculate ATE (Absolute Trajectory Error)
        ate_trans = np.sqrt(np.mean(np.array(translation_errors)**2))
        ate_rot = np.sqrt(np.mean(np.array(rotation_errors)**2))

        # Calculate processing statistics
        avg_processing_time = np.mean(processing_times) if processing_times else 0
        avg_fps = 1.0 / avg_processing_time if avg_processing_time > 0 else 0

        return {
            'ate_translation': ate_trans,
            'ate_rotation': np.degrees(ate_rot),
            'rmse_translation': np.sqrt(np.mean(np.array(translation_errors)**2)),
            'rmse_rotation': np.degrees(np.sqrt(np.mean(np.array(rotation_errors)**2))),
            'median_translation': np.median(translation_errors),
            'median_rotation': np.degrees(np.median(rotation_errors)),
            'avg_processing_time': avg_processing_time,
            'avg_fps': avg_fps,
            'num_frames': len(estimated_poses)
        }

    def compare_algorithms(self, algorithms: dict, dataset: List,
                          ground_truth: List) -> dict:
        """
        Compare multiple VSLAM algorithms

        Args:
            algorithms: Dictionary mapping algorithm names to instances
            dataset: Evaluation dataset
            ground_truth: Ground truth poses

        Returns:
            Comparison results
        """
        results = {}

        for name, algorithm in algorithms.items():
            print(f"Evaluating {name}...")
            results[name] = self.evaluate_algorithm(algorithm, dataset, ground_truth)

        return results
```

## Summary

VSLAM algorithms can be categorized into several main approaches:

1. **Filter-based Methods**: Use recursive Bayesian estimation (EKF, particle filters) to maintain state estimates. Good for real-time performance but may suffer from linearization errors.

2. **Keyframe-based Methods**: Store key frames and optimize pose graphs. ORB-SLAM is a prime example, offering high accuracy and robust loop closure.

3. **Direct Methods**: Work directly with pixel intensities, enabling dense reconstruction. LSD-SLAM and DSO are examples, working well in textureless environments.

4. **Semi-Direct Methods**: Combine feature tracking with direct alignment. SVO is an example, offering a balance between robustness and efficiency.

For humanoid robotics applications, consider:

- **Computational Efficiency**: Humanoid robots have limited computational resources
- **Robustness**: Legged locomotion introduces vibrations and motion blur
- **Accuracy**: Precise localization is crucial for manipulation tasks
- **Real-time Performance**: Need to operate at sufficient frame rates
- **Adaptability**: Different environments may require different approaches

The choice of VSLAM algorithm depends on the specific requirements of the humanoid robotics application, including the environment characteristics, accuracy requirements, and computational constraints.