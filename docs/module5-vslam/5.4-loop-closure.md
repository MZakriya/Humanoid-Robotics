---
title: Loop Closure
sidebar_label: Loop Closure
description: Comprehensive guide to loop closure detection and optimization for visual SLAM in humanoid robotics
---

# 5.4 Loop Closure

## Overview

Loop closure is a critical component of Visual SLAM (VSLAM) systems that enables global consistency in the map by recognizing previously visited locations. For humanoid robotics applications, robust loop closure detection is essential for long-term navigation, map optimization, and preventing drift accumulation. This chapter covers the theoretical foundations, algorithms, and implementation techniques for effective loop closure in VSLAM systems.

## The Loop Closure Problem

### Definition and Importance

Loop closure occurs when a robot recognizes that it has returned to a previously visited location in the environment. This recognition allows the SLAM system to:

1. Correct accumulated drift in the estimated trajectory
2. Optimize the global map consistency
3. Reduce the uncertainty in the robot's pose estimates
4. Maintain a globally consistent map over long-term operation

### Challenges in Loop Closure

```python
#!/usr/bin/env python3
"""
Loop closure analysis and challenges
"""
import numpy as np
from typing import List, Tuple, Optional
import cv2

class LoopClosureChallenges:
    """Analysis of common loop closure challenges"""

    def __init__(self):
        self.challenges = {
            'appearance_change': {
                'description': 'Changes in lighting, weather, or seasonal conditions',
                'impact': 'High false negative rate',
                'mitigation': 'Use robust features and normalization techniques'
            },
            'dynamic_objects': {
                'description': 'Moving objects that change between visits',
                'impact': 'False positives and incorrect associations',
                'mitigation': 'Object removal or temporal consistency checks'
            },
            'partial_view': {
                'description': 'Different viewpoints of the same location',
                'impact': 'High false negative rate',
                'mitigation': 'Viewpoint invariant features and geometric verification'
            },
            'scale_variations': {
                'description': 'Different scales due to camera height or zoom',
                'impact': 'Feature mismatch',
                'mitigation': 'Scale-invariant features and multi-scale analysis'
            }
        }

    def analyze_challenge_impact(self, scenario: str) -> dict:
        """Analyze impact of specific challenges in humanoid robotics scenarios"""
        if scenario in self.challenges:
            return self.challenges[scenario]
        else:
            return {'description': 'Unknown challenge', 'impact': 'Unknown', 'mitigation': 'Unknown'}
```

## Feature-Based Loop Closure

### Bag of Words (BoW) Approach

The Bag of Words approach represents images as histograms of visual words, enabling efficient similarity computation:

```python
class BagOfWordsLoopClosure:
    """Bag of Words approach for loop closure detection"""

    def __init__(self, vocabulary_size: int = 1000, feature_detector: str = 'ORB'):
        self.vocabulary_size = vocabulary_size
        self.feature_detector = feature_detector

        # Initialize feature detector
        if feature_detector == 'ORB':
            self.detector = cv2.ORB_create()
        elif feature_detector == 'SIFT':
            self.detector = cv2.SIFT_create()
        else:
            raise ValueError(f"Unsupported feature detector: {feature_detector}")

        # Initialize vocabulary (will be built during training)
        self.vocabulary = None
        self.bow_extractor = None

        # Database of image descriptors
        self.image_descriptors = []
        self.image_histograms = []
        self.image_poses = []

        # Parameters for loop closure detection
        self.min_matches = 15
        self.min_inliers = 10
        self.max_reprojection_error = 3.0

    def build_vocabulary(self, images: List[np.ndarray], max_features: int = 500) -> np.ndarray:
        """
        Build vocabulary from a set of training images

        Args:
            images: List of training images
            max_features: Maximum features to extract per image

        Returns:
            Vocabulary (cluster centers)
        """
        all_descriptors = []

        for img in images:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
            keypoints = self.detector.detect(gray)
            if len(keypoints) > max_features:
                keypoints = sorted(keypoints, key=lambda x: x.response, reverse=True)[:max_features]

            _, descriptors = self.detector.compute(gray, keypoints)

            if descriptors is not None:
                all_descriptors.append(descriptors)

        if len(all_descriptors) == 0:
            raise ValueError("No descriptors found in training images")

        # Concatenate all descriptors
        all_descriptors = np.vstack(all_descriptors)

        # Perform K-means clustering to build vocabulary
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.1)
        _, labels, centers = cv2.kmeans(
            all_descriptors.astype(np.float32),
            self.vocabulary_size,
            None,
            criteria,
            3,
            cv2.KMEANS_PP_CENTERS
        )

        self.vocabulary = centers
        self._initialize_bow_extractor()

        return centers

    def _initialize_bow_extractor(self):
        """Initialize BoW extractor with vocabulary"""
        bow_extractor = cv2.BOWKMeansTrainer(self.vocabulary_size)
        bow_extractor.add(self.vocabulary)
        self.vocabulary = bow_extractor.cluster()

        # Create FLANN matcher for BoW descriptor extraction
        self.flann = cv2.FlannBasedMatcher()

        # Create BOW image descriptor extractor
        self.bow_extractor = cv2.BOWImgDescriptorExtractor(self.detector, self.flann)
        self.bow_extractor.setVocabulary(self.vocabulary)

    def add_image(self, image: np.ndarray, pose: Optional[np.ndarray] = None) -> int:
        """
        Add an image to the loop closure database

        Args:
            image: Input image
            pose: Robot pose when image was captured (optional)

        Returns:
            Image ID
        """
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        keypoints = self.detector.detect(gray)

        if len(keypoints) == 0:
            return -1  # No features found

        # Extract BoW descriptor
        bow_descriptor = self.bow_extractor.compute(gray, keypoints)

        if bow_descriptor is None or bow_descriptor.shape[0] == 0:
            return -1  # Failed to compute descriptor

        # Store the descriptor and pose
        self.image_descriptors.append(bow_descriptor)
        self.image_poses.append(pose)

        # Create histogram representation
        hist = bow_descriptor.flatten()
        hist = hist / (np.linalg.norm(hist) + 1e-8)  # Normalize
        self.image_histograms.append(hist)

        return len(self.image_descriptors) - 1

    def detect_loop_closure(self, query_image: np.ndarray,
                           min_similarity: float = 0.3,
                           max_candidates: int = 10) -> List[Tuple[int, float]]:
        """
        Detect potential loop closures for the query image

        Args:
            query_image: Query image to check for loop closure
            min_similarity: Minimum similarity threshold
            max_candidates: Maximum number of candidates to return

        Returns:
            List of (image_id, similarity_score) tuples
        """
        if len(self.image_histograms) == 0:
            return []

        gray = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY) if len(query_image.shape) == 3 else query_image
        keypoints = self.detector.detect(gray)

        if len(keypoints) == 0:
            return []

        # Extract BoW descriptor for query image
        query_descriptor = self.bow_extractor.compute(gray, keypoints)

        if query_descriptor is None:
            return []

        query_hist = query_descriptor.flatten()
        query_hist = query_hist / (np.linalg.norm(query_hist) + 1e-8)  # Normalize

        # Compute similarities with all stored images
        similarities = []
        for i, stored_hist in enumerate(self.image_histograms):
            # Compute cosine similarity
            similarity = np.dot(query_hist, stored_hist)
            if similarity >= min_similarity:
                similarities.append((i, similarity))

        # Sort by similarity (descending)
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Return top candidates
        return similarities[:max_candidates]

    def verify_loop_closure(self, query_image: np.ndarray, candidate_id: int,
                           camera_matrix: np.ndarray) -> Tuple[bool, float]:
        """
        Geometrically verify a potential loop closure

        Args:
            query_image: Current query image
            candidate_id: ID of candidate image to verify against
            camera_matrix: Camera intrinsic matrix

        Returns:
            (is_valid, geometric_score)
        """
        if candidate_id >= len(self.image_descriptors):
            return False, 0.0

        # Extract features from both images
        gray_query = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY) if len(query_image.shape) == 3 else query_image
        keypoints_query, descriptors_query = self.detector.detectAndCompute(gray_query, None)

        candidate_img = self.get_image_by_id(candidate_id)  # This would need implementation
        if candidate_img is None:
            return False, 0.0

        gray_candidate = cv2.cvtColor(candidate_img, cv2.COLOR_BGR2GRAY) if len(candidate_img.shape) == 3 else candidate_img
        keypoints_candidate, descriptors_candidate = self.detector.detectAndCompute(gray_candidate, None)

        if descriptors_query is None or descriptors_candidate is None:
            return False, 0.0

        # Match features between images
        matcher = cv2.BFMatcher()
        matches = matcher.knnMatch(descriptors_query, descriptors_candidate, k=2)

        # Apply Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)

        if len(good_matches) < self.min_matches:
            return False, 0.0

        # Extract corresponding points
        src_pts = np.float32([keypoints_query[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([keypoints_candidate[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

        # Find fundamental matrix using RANSAC
        F, mask = cv2.findFundamentalMat(
            src_pts, dst_pts,
            cv2.RANSAC,
            self.max_reprojection_error,
            0.999
        )

        if F is None:
            return False, 0.0

        # Count inliers
        inliers = np.sum(mask) if mask is not None else 0

        if inliers >= self.min_inliers:
            geometric_score = inliers / len(good_matches)
            return True, geometric_score
        else:
            return False, 0.0

    def get_image_by_id(self, image_id: int) -> Optional[np.ndarray]:
        """
        Retrieve image by ID (placeholder - in real implementation,
        you would store actual images or their paths)
        """
        # This is a placeholder implementation
        # In a real system, you would have a way to retrieve images by ID
        return None
```

### DLoopDetector Approach

DLoopDetector is a specialized method for loop closure detection that focuses on distinctive place recognition:

```python
class DLoopDetector:
    """Implementation of DLoopDetector approach for loop closure detection"""

    def __init__(self, vocabulary_size: int = 1000):
        self.vocabulary_size = vocabulary_size
        self.vocabulary = None
        self.weights = None  # Inverse document frequency weights
        self.image_signatures = []

        # Parameters for distinctive place detection
        self.min_word_frequency = 2
        self.max_word_frequency = 0.8  # As fraction of total images
        self.min_visual_words = 5
        self.max_visual_words = 100

    def build_vocabulary(self, images: List[np.ndarray]) -> np.ndarray:
        """Build vocabulary using clustering of local features"""
        # This is a simplified implementation
        # In practice, you would use more sophisticated clustering techniques

        all_descriptors = []
        for img in images:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
            orb = cv2.ORB_create()
            keypoints = orb.detect(gray)
            _, descriptors = orb.compute(gray, keypoints)

            if descriptors is not None:
                all_descriptors.append(descriptors)

        if len(all_descriptors) == 0:
            raise ValueError("No descriptors found in training images")

        all_descriptors = np.vstack(all_descriptors).astype(np.float32)

        # Perform K-means clustering
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.1)
        _, _, centers = cv2.kmeans(
            all_descriptors,
            self.vocabulary_size,
            None,
            criteria,
            3,
            cv2.KMEANS_PP_CENTERS
        )

        self.vocabulary = centers
        return centers

    def compute_image_signature(self, image: np.ndarray) -> dict:
        """
        Compute distinctive image signature for loop closure

        Args:
            image: Input image

        Returns:
            Dictionary with word counts and statistics
        """
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        orb = cv2.ORB_create()
        keypoints = orb.detect(gray)
        _, descriptors = orb.compute(gray, keypoints)

        if descriptors is None or len(descriptors) == 0:
            return {'words': [], 'counts': [], 'total_words': 0}

        # Assign descriptors to vocabulary words
        matcher = cv2.BFMatcher()
        matches = matcher.knnMatch(descriptors, self.vocabulary, k=1)

        word_counts = {}
        for match in matches:
            if len(match) > 0:
                word_id = match[0].trainIdx
                if word_id in word_counts:
                    word_counts[word_id] += 1
                else:
                    word_counts[word_id] = 1

        # Filter words based on frequency criteria
        filtered_words = {}
        for word_id, count in word_counts.items():
            if self.min_word_frequency <= count:
                filtered_words[word_id] = count

        return {
            'words': list(filtered_words.keys()),
            'counts': list(filtered_words.values()),
            'total_words': sum(filtered_words.values())
        }

    def detect_loop_closure(self, query_signature: dict,
                           database_signatures: List[dict],
                           min_similarity: float = 0.3) -> List[Tuple[int, float]]:
        """
        Detect loop closure using image signatures

        Args:
            query_signature: Signature of query image
            database_signatures: Signatures of database images
            min_similarity: Minimum similarity threshold

        Returns:
            List of (image_id, similarity_score) tuples
        """
        if query_signature['total_words'] == 0:
            return []

        similarities = []

        for i, db_signature in enumerate(database_signatures):
            if db_signature['total_words'] == 0:
                continue

            # Compute TF-IDF similarity
            similarity = self._compute_tfidf_similarity(query_signature, db_signature)

            if similarity >= min_similarity:
                similarities.append((i, similarity))

        # Sort by similarity (descending)
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities

    def _compute_tfidf_similarity(self, sig1: dict, sig2: dict) -> float:
        """Compute TF-IDF based similarity between two signatures"""
        # Create word sets
        words1 = set(sig1['words'])
        words2 = set(sig2['words'])

        # Find common words
        common_words = words1.intersection(words2)

        if len(common_words) == 0:
            return 0.0

        # Compute TF-IDF vectors
        all_words = words1.union(words2)
        vec1 = np.zeros(len(all_words))
        vec2 = np.zeros(len(all_words))

        word_to_idx = {word: i for i, word in enumerate(all_words)}

        # Fill vectors with TF-IDF values
        for word, count in zip(sig1['words'], sig1['counts']):
            idx = word_to_idx[word]
            tf = count / sig1['total_words']  # Term frequency
            # IDF would normally use global statistics
            vec1[idx] = tf

        for word, count in zip(sig2['words'], sig2['counts']):
            idx = word_to_idx[word]
            tf = count / sig2['total_words']  # Term frequency
            vec2[idx] = tf

        # Compute cosine similarity
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        similarity = np.dot(vec1, vec2) / (norm1 * norm2)
        return similarity
```

## Direct Appearance-Based Methods

### Deep Learning Approaches

Modern loop closure detection increasingly uses deep learning methods for more robust place recognition:

```python
class DeepLoopClosure:
    """Deep learning-based loop closure detection"""

    def __init__(self, model_type: str = 'NetVLAD'):
        self.model_type = model_type
        self.feature_extractor = None
        self.global_descriptor_dim = 4096  # Typical for NetVLAD

        # Database of global descriptors
        self.global_descriptors = []
        self.image_poses = []
        self.image_timestamps = []

        # Similarity threshold parameters
        self.similarity_threshold = 0.7
        self.min_temporal_distance = 10  # Minimum frames between candidates

    def extract_global_descriptor(self, image: np.ndarray) -> np.ndarray:
        """
        Extract global descriptor from image using deep learning

        Args:
            image: Input image

        Returns:
            Global descriptor vector
        """
        # This is a simplified implementation
        # In practice, you would use a pre-trained CNN model like NetVLAD

        # For demonstration, we'll use a simple approach
        # In real implementation, you would load a pre-trained model
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

        # Resize to standard size for consistency
        resized = cv2.resize(gray, (224, 224))

        # Extract simple features (in real implementation, use CNN features)
        # This is just a placeholder for the actual deep learning model
        descriptor = self._simple_global_descriptor(resized)

        # Normalize descriptor
        descriptor = descriptor / (np.linalg.norm(descriptor) + 1e-8)

        return descriptor

    def _simple_global_descriptor(self, image: np.ndarray) -> np.ndarray:
        """Simple global descriptor (placeholder for deep features)"""
        # In real implementation, this would be replaced with CNN features
        # For example, features from a pre-trained ResNet or NetVLAD

        # Calculate histogram of oriented gradients as a simple global descriptor
        # This is just a placeholder
        hist = cv2.calcHist([image], [0], None, [32], [0, 256])
        hist = hist.flatten()

        # Calculate basic statistics as additional features
        mean_val = np.mean(image)
        std_val = np.std(image)
        energy = np.sum(image**2) / image.size

        # Combine features
        features = np.concatenate([hist, [mean_val, std_val, energy]])

        # Pad to desired dimension (in real implementation, this would match the CNN output)
        if len(features) < self.global_descriptor_dim:
            features = np.pad(features, (0, self.global_descriptor_dim - len(features)), 'constant')
        else:
            features = features[:self.global_descriptor_dim]

        return features.astype(np.float32)

    def add_image(self, image: np.ndarray, pose: Optional[np.ndarray] = None,
                  timestamp: float = None) -> int:
        """
        Add image to the loop closure database

        Args:
            image: Input image
            pose: Robot pose when image was captured (optional)
            timestamp: Timestamp of image capture (optional)

        Returns:
            Image ID
        """
        descriptor = self.extract_global_descriptor(image)

        self.global_descriptors.append(descriptor)
        self.image_poses.append(pose)
        self.image_timestamps.append(timestamp or len(self.global_descriptors))

        return len(self.global_descriptors) - 1

    def detect_loop_closure(self, query_image: np.ndarray,
                           current_frame_id: int = None,
                           min_similarity: float = 0.6,
                           max_candidates: int = 5) -> List[Tuple[int, float]]:
        """
        Detect potential loop closures using global descriptors

        Args:
            query_image: Query image to check for loop closure
            current_frame_id: Current frame ID (to avoid matching with recent frames)
            min_similarity: Minimum similarity threshold
            max_candidates: Maximum number of candidates to return

        Returns:
            List of (image_id, similarity_score) tuples
        """
        if len(self.global_descriptors) == 0:
            return []

        query_descriptor = self.extract_global_descriptor(query_image)

        similarities = []

        for i, stored_descriptor in enumerate(self.global_descriptors):
            # Apply temporal constraint to avoid matching with recent frames
            if (current_frame_id is not None and
                abs(current_frame_id - i) < self.min_temporal_distance):
                continue

            # Compute cosine similarity
            similarity = np.dot(query_descriptor, stored_descriptor)

            if similarity >= min_similarity:
                similarities.append((i, similarity))

        # Sort by similarity (descending)
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Return top candidates
        return similarities[:max_candidates]

    def compute_similarity_matrix(self) -> np.ndarray:
        """
        Compute similarity matrix between all stored descriptors

        Returns:
            Similarity matrix of shape (n_images, n_images)
        """
        n_descriptors = len(self.global_descriptors)
        if n_descriptors == 0:
            return np.array([])

        similarity_matrix = np.zeros((n_descriptors, n_descriptors))

        for i in range(n_descriptors):
            for j in range(i, n_descriptors):  # Only compute upper triangle
                similarity = np.dot(self.global_descriptors[i], self.global_descriptors[j])
                similarity_matrix[i, j] = similarity
                similarity_matrix[j, i] = similarity  # Matrix is symmetric

        return similarity_matrix
```

## Geometric Verification

### Essential Matrix Verification

Geometric verification ensures that potential loop closures are geometrically consistent:

```python
class GeometricVerification:
    """Geometric verification for loop closure candidates"""

    def __init__(self,
                 min_matches: int = 15,
                 min_inliers: int = 10,
                 max_reprojection_error: float = 3.0,
                 min_inlier_ratio: float = 0.5):
        self.min_matches = min_matches
        self.min_inliers = min_inliers
        self.max_reprojection_error = max_reprojection_error
        self.min_inlier_ratio = min_inlier_ratio

    def verify_loop_closure_geometric(self,
                                    img1: np.ndarray,
                                    img2: np.ndarray,
                                    camera_matrix: np.ndarray,
                                    detector_type: str = 'ORB') -> Tuple[bool, dict]:
        """
        Perform geometric verification of potential loop closure

        Args:
            img1: First image (candidate)
            img2: Second image (query)
            camera_matrix: Camera intrinsic matrix
            detector_type: Feature detector type ('ORB', 'SIFT', etc.)

        Returns:
            (is_valid, verification_stats)
        """
        # Initialize feature detector
        if detector_type == 'ORB':
            detector = cv2.ORB_create()
        elif detector_type == 'SIFT':
            detector = cv2.SIFT_create()
        else:
            raise ValueError(f"Unsupported detector type: {detector_type}")

        # Convert images to grayscale
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape) == 3 else img1
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape) == 3 else img2

        # Extract features
        kp1, desc1 = detector.detectAndCompute(gray1, None)
        kp2, desc2 = detector.detectAndCompute(gray2, None)

        if desc1 is None or desc2 is None:
            return False, {'matches': 0, 'inliers': 0, 'inlier_ratio': 0.0}

        # Match features
        matcher = cv2.BFMatcher()
        matches = matcher.knnMatch(desc1, desc2, k=2)

        # Apply Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)

        # Check minimum number of matches
        if len(good_matches) < self.min_matches:
            return False, {
                'matches': len(good_matches),
                'inliers': 0,
                'inlier_ratio': 0.0,
                'reason': 'insufficient_matches'
            }

        # Extract corresponding points
        if len(good_matches) >= 8:  # Minimum for fundamental matrix
            src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

            # Find fundamental matrix
            F, mask = cv2.findFundamentalMat(
                src_pts, dst_pts,
                cv2.RANSAC,
                self.max_reprojection_error,
                0.999
            )

            # Count inliers
            inliers = np.sum(mask) if mask is not None else 0
            inlier_ratio = inliers / len(good_matches) if len(good_matches) > 0 else 0.0

            # Check geometric consistency
            is_geometrically_valid = (
                inliers >= self.min_inliers and
                inlier_ratio >= self.min_inlier_ratio
            )

            stats = {
                'matches': len(good_matches),
                'inliers': inliers,
                'inlier_ratio': inlier_ratio,
                'fundamental_matrix': F
            }

            return is_geometrically_valid, stats
        else:
            # Not enough matches for geometric verification
            return False, {
                'matches': len(good_matches),
                'inliers': 0,
                'inlier_ratio': 0.0,
                'reason': 'insufficient_matches_for_geometry'
            }

    def verify_with_essential_matrix(self,
                                   img1: np.ndarray,
                                   img2: np.ndarray,
                                   camera_matrix: np.ndarray,
                                   detector_type: str = 'ORB') -> Tuple[bool, dict]:
        """
        Verify using essential matrix for calibrated cameras
        """
        # Initialize feature detector
        if detector_type == 'ORB':
            detector = cv2.ORB_create()
        elif detector_type == 'SIFT':
            detector = cv2.SIFT_create()
        else:
            raise ValueError(f"Unsupported detector type: {detector_type}")

        # Convert images to grayscale
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape) == 3 else img1
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape) == 3 else img2

        # Extract features
        kp1, desc1 = detector.detectAndCompute(gray1, None)
        kp2, desc2 = detector.detectAndCompute(gray2, None)

        if desc1 is None or desc2 is None:
            return False, {'matches': 0, 'inliers': 0, 'relative_pose': None}

        # Match features
        matcher = cv2.BFMatcher()
        matches = matcher.knnMatch(desc1, desc2, k=2)

        # Apply Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)

        if len(good_matches) < 8:  # Minimum for essential matrix
            return False, {'matches': len(good_matches), 'inliers': 0, 'relative_pose': None}

        # Extract corresponding points
        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 2)

        # Normalize points using camera matrix
        src_norm = cv2.undistortPoints(src_pts.reshape(-1, 1, 2), camera_matrix, None)
        dst_norm = cv2.undistortPoints(dst_pts.reshape(-1, 1, 2), camera_matrix, None)

        # Find essential matrix
        E, mask = cv2.findEssentialMat(
            src_norm, dst_norm,
            focal=1.0, pp=(0, 0),  # Already normalized
            method=cv2.RANSAC,
            threshold=1.0,
            prob=0.999
        )

        if E is None or E.size == 0:
            return False, {'matches': len(good_matches), 'inliers': 0, 'relative_pose': None}

        # Recover pose
        _, R, t, mask_pose = cv2.recoverPose(E, src_norm, dst_norm, mask=mask)

        # Count valid pose inliers
        pose_inliers = np.sum(mask_pose) if mask_pose is not None else 0

        stats = {
            'matches': len(good_matches),
            'inliers': pose_inliers,
            'relative_rotation': R,
            'relative_translation': t,
            'essential_matrix': E
        }

        # Check if sufficient inliers for pose recovery
        is_valid = pose_inliers >= self.min_inliers
        return is_valid, stats
```

## Loop Closure Optimization

### Graph-Based Optimization

Once loop closures are detected, the pose graph needs to be optimized to correct drift:

```python
class PoseGraphOptimizer:
    """Graph-based optimization for loop closure correction"""

    def __init__(self):
        self.nodes = {}  # node_id -> pose
        self.edges = []  # (node_id1, node_id2, relative_pose, information_matrix)
        self.optimized = False

    def add_node(self, node_id: int, pose: np.ndarray):
        """Add a node (robot pose) to the graph"""
        self.nodes[node_id] = pose

    def add_constraint(self, node1_id: int, node2_id: int,
                      relative_pose: np.ndarray,
                      information_matrix: np.ndarray = None):
        """Add a constraint (relative measurement) between nodes"""
        if information_matrix is None:
            # Default information matrix (identity as placeholder)
            information_matrix = np.eye(6)

        self.edges.append((node1_id, node2_id, relative_pose, information_matrix))

    def add_odometry_constraint(self, from_node: int, to_node: int,
                               relative_pose: np.ndarray):
        """Add odometry constraint between consecutive nodes"""
        # Odometry constraints typically have higher uncertainty
        # Create a diagonal information matrix with lower values
        info_matrix = np.diag([10, 10, 10, 100, 100, 100])  # [x, y, z, rx, ry, rz]

        self.add_constraint(from_node, to_node, relative_pose, info_matrix)

    def add_loop_closure_constraint(self, node1_id: int, node2_id: int,
                                  relative_pose: np.ndarray):
        """Add loop closure constraint between distant nodes"""
        # Loop closure constraints typically have higher confidence
        # Create a diagonal information matrix with higher values
        info_matrix = np.diag([100, 100, 100, 1000, 1000, 1000])  # Higher confidence

        self.add_constraint(node1_id, node2_id, relative_pose, info_matrix)

    def optimize(self, max_iterations: int = 50) -> bool:
        """
        Optimize the pose graph using Gauss-Newton method

        Args:
            max_iterations: Maximum optimization iterations

        Returns:
            True if optimization converged
        """
        if len(self.nodes) == 0 or len(self.edges) == 0:
            return False

        # This is a simplified implementation of graph optimization
        # In practice, you would use libraries like g2o, Ceres, or GTSAM

        # Convert to optimization-friendly format
        node_ids = sorted(self.nodes.keys())
        initial_poses = [self.nodes[node_id] for node_id in node_ids]

        # Create node_id to index mapping
        node_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}

        # Perform iterative optimization
        for iteration in range(max_iterations):
            # Compute residuals and Jacobians (simplified)
            delta_x = self._compute_gauss_newton_step(node_to_idx)

            if delta_x is None:
                break

            # Apply update
            self._apply_update(delta_x, node_to_idx)

            # Check convergence
            if np.linalg.norm(delta_x) < 1e-6:
                print(f"Optimization converged after {iteration + 1} iterations")
                self.optimized = True
                return True

        self.optimized = True
        return True

    def _compute_gauss_newton_step(self, node_to_idx: dict) -> Optional[np.ndarray]:
        """Compute Gauss-Newton optimization step"""
        # This is a placeholder implementation
        # A full implementation would compute the actual Jacobians and residuals
        # and solve the normal equations: H * dx = -b

        n_nodes = len(node_to_idx)
        # Each node has 6 DOF (3 translation + 3 rotation)
        n_vars = n_nodes * 6

        # For this simplified version, return a small random update
        # In a real implementation, you would compute the proper Jacobians
        return np.random.randn(n_vars) * 0.001

    def _apply_update(self, delta_x: np.ndarray, node_to_idx: dict):
        """Apply the optimization update to node poses"""
        for node_id, idx in node_to_idx.items():
            # Extract 6 DOF update for this node
            start_idx = idx * 6
            pose_update = delta_x[start_idx:start_idx + 6]

            # Apply update to the pose
            current_pose = self.nodes[node_id]

            # Update translation
            current_pose[:3] += pose_update[:3]

            # Update rotation (simplified - in practice, use proper rotation updates)
            rotation_update = pose_update[3:]
            # This is a simplified rotation update
            # In practice, you'd use proper rotation representations like quaternions
            self.nodes[node_id] = current_pose

    def get_optimized_poses(self) -> dict:
        """Get the optimized poses"""
        if not self.optimized:
            print("Warning: Graph has not been optimized yet")
        return self.nodes.copy()

    def visualize_graph(self):
        """Visualize the pose graph (placeholder)"""
        import matplotlib.pyplot as plt
        from mpl_toolkits.mplot3d import Axes3D

        if len(self.nodes) == 0:
            return

        # Extract positions
        positions = []
        node_ids = sorted(self.nodes.keys())
        for node_id in node_ids:
            pose = self.nodes[node_id]
            positions.append(pose[:3])  # Extract translation

        positions = np.array(positions)

        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')

        ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 'b-', linewidth=1, alpha=0.5)
        ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], c='red', s=20)

        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        ax.set_title('Pose Graph Visualization')

        plt.show()
```

## Real-time Loop Closure System

### Integrated Loop Closure Pipeline

```python
class RealTimeLoopClosureSystem:
    """Real-time loop closure system for humanoid robotics"""

    def __init__(self,
                 vocabulary_size: int = 1000,
                 detector_type: str = 'ORB',
                 min_similarity: float = 0.5,
                 min_temporal_distance: int = 20):
        self.vocabulary_size = vocabulary_size
        self.detector_type = detector_type
        self.min_similarity = min_similarity
        self.min_temporal_distance = min_temporal_distance

        # Initialize components
        self.loop_detector = BagOfWordsLoopClosure(vocabulary_size, detector_type)
        self.geometry_verifier = GeometricVerification()
        self.graph_optimizer = PoseGraphOptimizer()

        # System state
        self.frame_id = 0
        self.poses = []  # Estimated poses
        self.loop_candidates = []  # Potential loop closures
        self.confirmed_loops = []  # Confirmed loop closures

        # Performance statistics
        self.processing_times = []
        self.detection_rate = 0.0
        self.false_positive_rate = 0.0

    def process_frame(self,
                     image: np.ndarray,
                     current_pose: np.ndarray,
                     camera_matrix: np.ndarray) -> dict:
        """
        Process a single frame for loop closure detection

        Args:
            image: Current image
            current_pose: Current estimated robot pose
            camera_matrix: Camera intrinsic matrix

        Returns:
            Dictionary with processing results and statistics
        """
        import time
        start_time = time.time()

        # Add current frame to database
        current_id = self.loop_detector.add_image(image, current_pose)
        self.graph_optimizer.add_node(self.frame_id, current_pose)

        # Store current pose
        self.poses.append(current_pose)

        # Detect potential loop closures
        potential_loops = []
        if self.frame_id >= self.min_temporal_distance:
            potential_loops = self.loop_detector.detect_loop_closure(
                image,
                min_similarity=self.min_similarity,
                max_candidates=5
            )

        # Verify potential loops geometrically
        confirmed_loops = []
        for candidate_id, similarity in potential_loops:
            # Verify geometric consistency
            is_valid, verification_stats = self.geometry_verifier.verify_loop_closure_geometric(
                image,  # Current image
                self._get_image_by_id(candidate_id),  # Retrieve candidate image
                camera_matrix,
                self.detector_type
            )

            if is_valid:
                # Add loop closure constraint to graph
                relative_pose = self._compute_relative_pose(
                    current_pose,
                    self.poses[candidate_id]
                )

                self.graph_optimizer.add_loop_closure_constraint(
                    self.frame_id, candidate_id, relative_pose
                )

                confirmed_loops.append((candidate_id, similarity, verification_stats))

        # Store confirmed loops
        self.confirmed_loops.extend(confirmed_loops)

        # Periodically optimize the graph
        if len(self.confirmed_loops) % 5 == 0 and len(self.confirmed_loops) > 0:
            self.graph_optimizer.optimize()

        # Calculate processing statistics
        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)

        # Calculate average processing time (maintain window of last 100 frames)
        if len(self.processing_times) > 100:
            self.processing_times = self.processing_times[-100:]

        avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0

        results = {
            'frame_id': self.frame_id,
            'potential_loops': potential_loops,
            'confirmed_loops': confirmed_loops,
            'processing_time': processing_time,
            'avg_processing_time': avg_processing_time,
            'current_pose': current_pose
        }

        self.frame_id += 1
        return results

    def _get_image_by_id(self, image_id: int) -> Optional[np.ndarray]:
        """Retrieve image by ID (placeholder implementation)"""
        # In a real system, you would have a mechanism to retrieve images
        # This is a placeholder that returns None
        return None

    def _compute_relative_pose(self, pose1: np.ndarray, pose2: np.ndarray) -> np.ndarray:
        """Compute relative pose between two absolute poses"""
        # Convert poses to transformation matrices if needed
        # This is a simplified implementation
        if pose1.shape == (6,) and pose2.shape == (6,):
            # Assume pose is [x, y, z, rx, ry, rz] format
            relative_translation = pose1[:3] - pose2[:3]
            relative_rotation = pose1[3:] - pose2[3:]
            return np.concatenate([relative_translation, relative_rotation])
        else:
            # More complex transformation computation needed
            return pose1 - pose2  # Simplified

    def get_system_status(self) -> dict:
        """Get current system status and statistics"""
        return {
            'total_frames_processed': self.frame_id,
            'total_confirmed_loops': len(self.confirmed_loops),
            'loop_closure_rate': len(self.confirmed_loops) / max(self.frame_id, 1),
            'avg_processing_time': np.mean(self.processing_times) if self.processing_times else 0,
            'current_poses_count': len(self.poses)
        }

    def reset_system(self):
        """Reset the loop closure system"""
        self.frame_id = 0
        self.poses = []
        self.loop_candidates = []
        self.confirmed_loops = []
        self.processing_times = []

        # Reinitialize components if needed
        self.loop_detector = BagOfWordsLoopClosure(self.vocabulary_size, self.detector_type)
        self.graph_optimizer = PoseGraphOptimizer()
```

## Performance Evaluation

### Loop Closure Quality Metrics

```python
class LoopClosureEvaluator:
    """Evaluate loop closure system performance"""

    def __init__(self):
        pass

    def evaluate_detection_performance(self,
                                     detected_loops: List[Tuple[int, int, float]],
                                     ground_truth_loops: List[Tuple[int, int]],
                                     max_distance: int = 10) -> dict:
        """
        Evaluate loop closure detection performance

        Args:
            detected_loops: List of (frame1, frame2, confidence) detected loops
            ground_truth_loops: List of (frame1, frame2) ground truth loops
            max_distance: Maximum frame distance to consider for ground truth

        Returns:
            Dictionary with performance metrics
        """
        # Convert to sets for easier comparison
        detected_set = set((min(f1, f2), max(f1, f2)) for f1, f2, _ in detected_loops)
        ground_truth_set = set((min(f1, f2), max(f1, f2)) for f1, f2 in ground_truth_loops)

        # Calculate true positives, false positives, false negatives
        true_positives = detected_set.intersection(ground_truth_set)
        false_positives = detected_set.difference(ground_truth_set)
        false_negatives = ground_truth_set.difference(detected_set)

        # Calculate metrics
        tp = len(true_positives)
        fp = len(false_positives)
        fn = len(false_negatives)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        return {
            'true_positives': tp,
            'false_positives': fp,
            'false_negatives': fn,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'accuracy': tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
        }

    def evaluate_geometric_consistency(self,
                                     poses: List[np.ndarray],
                                     confirmed_loops: List[Tuple[int, int, dict]]) -> dict:
        """
        Evaluate geometric consistency of confirmed loop closures

        Args:
            poses: List of robot poses
            confirmed_loops: List of confirmed loops with geometric information

        Returns:
            Dictionary with geometric consistency metrics
        """
        geometric_errors = []

        for frame1, frame2, geom_info in confirmed_loops:
            if frame1 >= len(poses) or frame2 >= len(poses):
                continue

            # Compute expected relative pose from odometry
            pose1 = poses[frame1]
            pose2 = poses[frame2]

            # Calculate geometric error
            if 'relative_pose' in geom_info:
                expected_transform = self._compute_expected_transform(pose1, pose2)
                measured_transform = geom_info['relative_pose']

                # Calculate transformation error
                error = self._calculate_transform_error(expected_transform, measured_transform)
                geometric_errors.append(error)

        if geometric_errors:
            return {
                'mean_error': np.mean(geometric_errors),
                'median_error': np.median(geometric_errors),
                'std_error': np.std(geometric_errors),
                'max_error': np.max(geometric_errors),
                'min_error': np.min(geometric_errors),
                'rmse': np.sqrt(np.mean(np.array(geometric_errors)**2))
            }
        else:
            return {
                'mean_error': float('inf'),
                'median_error': float('inf'),
                'std_error': 0,
                'max_error': float('inf'),
                'min_error': float('inf'),
                'rmse': float('inf')
            }

    def _compute_expected_transform(self, pose1: np.ndarray, pose2: np.ndarray) -> np.ndarray:
        """Compute expected relative transformation between two poses"""
        # This is a simplified implementation
        # In practice, you would compute the full SE(3) transformation
        if pose1.shape[0] >= 6 and pose2.shape[0] >= 6:
            # Assume [x, y, z, rx, ry, rz] format
            relative_translation = pose2[:3] - pose1[:3]
            relative_rotation = pose2[3:6] - pose1[3:6]
            return np.concatenate([relative_translation, relative_rotation])
        else:
            return pose2 - pose1

    def _calculate_transform_error(self, expected: np.ndarray, measured: np.ndarray) -> float:
        """Calculate error between expected and measured transformations"""
        # Calculate Euclidean distance between transformations
        return np.linalg.norm(expected - measured)

    def plot_performance_curves(self,
                               detection_results: List[dict],
                               title: str = "Loop Closure Performance") -> None:
        """
        Plot performance curves over time
        """
        import matplotlib.pyplot as plt

        if not detection_results:
            return

        # Extract metrics over time
        precisions = [result['precision'] for result in detection_results]
        recalls = [result['recall'] for result in detection_results]
        f1_scores = [result['f1_score'] for result in detection_results]

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

        # Plot precision and recall
        ax1.plot(precisions, label='Precision', marker='o')
        ax1.plot(recalls, label='Recall', marker='s')
        ax1.set_title('Precision and Recall Over Time')
        ax1.set_xlabel('Time (frames)')
        ax1.set_ylabel('Score')
        ax1.legend()
        ax1.grid(True)

        # Plot F1 score
        ax2.plot(f1_scores, label='F1 Score', marker='^', color='green')
        ax2.set_title('F1 Score Over Time')
        ax2.set_xlabel('Time (frames)')
        ax2.set_ylabel('F1 Score')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()
        plt.suptitle(title)
        plt.show()
```

## Advanced Topics in Loop Closure

### Multi-session Loop Closure

For humanoid robots operating across multiple sessions, loop closure across sessions becomes important:

```python
class MultiSessionLoopClosure:
    """Handle loop closure across multiple sessions"""

    def __init__(self):
        self.session_maps = {}  # session_id -> map data
        self.global_descriptors = {}  # session_id -> descriptors
        self.cross_session_loops = []

    def add_session_data(self, session_id: str, images: List[np.ndarray],
                        poses: List[np.ndarray]):
        """Add data from a new session"""
        self.session_maps[session_id] = {
            'images': images,
            'poses': poses,
            'global_descriptors': []
        }

        # Compute global descriptors for this session
        deep_detector = DeepLoopClosure()
        for img in images:
            desc = deep_detector.extract_global_descriptor(img)
            self.session_maps[session_id]['global_descriptors'].append(desc)

    def detect_cross_session_closure(self, query_session: str,
                                   query_image: np.ndarray,
                                   min_similarity: float = 0.6) -> List[Tuple[str, int, float]]:
        """
        Detect loop closures with other sessions

        Args:
            query_session: Session ID of query
            query_image: Query image
            min_similarity: Minimum similarity threshold

        Returns:
            List of (session_id, image_id, similarity) tuples
        """
        query_desc = DeepLoopClosure().extract_global_descriptor(query_image)
        candidates = []

        for session_id, session_data in self.session_maps.items():
            if session_id == query_session:
                continue  # Don't match within same session

            for img_id, stored_desc in enumerate(session_data['global_descriptors']):
                similarity = np.dot(query_desc, stored_desc)

                if similarity >= min_similarity:
                    candidates.append((session_id, img_id, similarity))

        # Sort by similarity
        candidates.sort(key=lambda x: x[2], reverse=True)
        return candidates
```

## Best Practices for Humanoid Robotics

### Adaptive Loop Closure

For humanoid robots operating in diverse environments, adaptive approaches work best:

```python
class AdaptiveLoopClosure:
    """Adaptive loop closure system that adjusts to environment conditions"""

    def __init__(self):
        self.indoor_detector = BagOfWordsLoopClosure(vocabulary_size=1000)
        self.outdoor_detector = DeepLoopClosure()
        self.dynamic_detector = DLoopDetector(vocabulary_size=1500)

        # Environment classifier
        self.environment_classifier = EnvironmentClassifier()

        # Performance monitors
        self.indoor_success_rate = 0.0
        self.outdoor_success_rate = 0.0
        self.dynamic_success_rate = 0.0

    def select_approach(self, image: np.ndarray) -> str:
        """Select the most appropriate loop closure approach based on environment"""
        env_type = self.environment_classifier.classify_environment(image)

        if env_type == 'indoor':
            return 'indoor'
        elif env_type == 'outdoor':
            return 'outdoor'
        elif env_type == 'dynamic':
            return 'dynamic'
        else:
            # Default to the approach with highest recent success rate
            rates = {
                'indoor': self.indoor_success_rate,
                'outdoor': self.outdoor_success_rate,
                'dynamic': self.dynamic_success_rate
            }
            return max(rates, key=rates.get)

    def detect_loop_closure_adaptive(self, image: np.ndarray,
                                   camera_matrix: np.ndarray) -> Tuple[List, str]:
        """
        Detect loop closure using adaptive approach selection

        Returns:
            (loop_candidates, method_used)
        """
        method = self.select_approach(image)

        if method == 'indoor':
            candidates = self.indoor_detector.detect_loop_closure(image)
        elif method == 'outdoor':
            # For outdoor, we'd need to implement the deep learning approach properly
            candidates = []  # Placeholder
        elif method == 'dynamic':
            # For dynamic environments, use DLoopDetector approach
            candidates = []  # Placeholder
        else:
            candidates = []

        return candidates, method

class EnvironmentClassifier:
    """Classify environment type for adaptive loop closure selection"""

    def classify_environment(self, image: np.ndarray) -> str:
        """Classify environment as indoor, outdoor, or dynamic"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

        # Analyze image characteristics
        # This is a simplified approach - in practice, use ML-based classification

        # Calculate texture level (variance of gradients)
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        texture_level = np.var(gradient_magnitude)

        # Calculate brightness statistics
        mean_brightness = np.mean(gray)

        # Simple heuristic classification
        if texture_level < 1000:  # Low texture might indicate indoor
            if mean_brightness < 100:  # Dark - might be indoor at night
                return 'indoor'
            else:
                return 'indoor'
        elif texture_level > 5000:  # High texture might indicate outdoor
            return 'outdoor'
        else:
            # Moderate texture - could be either, check for dynamic content
            # This would require more sophisticated analysis
            return 'indoor'  # Default assumption
```

## Summary

Loop closure is a fundamental component of VSLAM systems that enables global map consistency and drift correction. For humanoid robotics applications, effective loop closure requires:

1. **Robust Feature Detection**: Use features that are invariant to appearance changes, lighting conditions, and viewpoint variations.

2. **Efficient Retrieval**: Implement fast similarity search methods like Bag of Words or deep learning-based approaches.

3. **Geometric Verification**: Always verify potential loop closures using geometric constraints to avoid false positives.

4. **Graph Optimization**: Use pose graph optimization to correct accumulated drift when loops are confirmed.

5. **Real-time Performance**: Optimize algorithms for real-time operation suitable for humanoid robot platforms.

6. **Adaptive Approaches**: Consider environmental conditions and adjust the loop closure strategy accordingly.

7. **Cross-session Capability**: For long-term operation, consider multi-session loop closure for persistent mapping.

The choice of loop closure approach depends on the specific requirements of the humanoid robotics application, including computational resources, environment characteristics, and accuracy requirements. Modern approaches increasingly combine traditional computer vision methods with deep learning for improved robustness.