---
title: Camera Models
sidebar_label: Camera Models
description: Comprehensive guide to camera models and calibration for visual SLAM in humanoid robotics
---

# 5.2 Camera Models

## Overview

Camera models form the mathematical foundation for understanding how 3D world points are projected onto 2D image planes. In Visual SLAM (VSLAM) systems for humanoid robotics, accurate camera models are essential for reliable pose estimation, mapping, and navigation. This chapter covers the fundamental camera models, calibration techniques, and their specific applications in humanoid robotics.

## Pinhole Camera Model

### Mathematical Foundation

The pinhole camera model is the most fundamental representation of how light travels from a 3D scene through a single point (the pinhole) to form an image on a 2D plane. The model assumes:

1. Light travels in straight lines
2. The aperture is infinitely small
3. No lens distortion
4. Perfect perspective projection

The projection from 3D world coordinates (X, Y, Z) to 2D image coordinates (u, v) is given by:

```
u = fx * (X/Z) + cx
v = fy * (Y/Z) + cy
```

Where:
- (fx, fy) are the focal lengths in pixels
- (cx, cy) is the principal point (image center)

### Implementation

```python
#!/usr/bin/env python3
"""
Pinhole camera model implementation
"""
import numpy as np
from typing import Tuple, Optional
import cv2

class PinholeCameraModel:
    """Implementation of the pinhole camera model"""

    def __init__(self,
                 fx: float,
                 fy: float,
                 cx: float,
                 cy: float,
                 width: int,
                 height: int,
                 k1: float = 0.0,
                 k2: float = 0.0,
                 p1: float = 0.0,
                 p2: float = 0.0):
        """
        Initialize pinhole camera model with optional distortion parameters

        Args:
            fx, fy: Focal lengths in pixels
            cx, cy: Principal point coordinates
            width, height: Image dimensions
            k1, k2: Radial distortion coefficients
            p1, p2: Tangential distortion coefficients
        """
        self.fx = fx
        self.fy = fy
        self.cx = cx
        self.cy = cy
        self.width = width
        self.height = height
        self.k1 = k1  # Radial distortion k1
        self.k2 = k2  # Radial distortion k2
        self.p1 = p1  # Tangential distortion p1
        self.p2 = p2  # Tangential distortion p2

        # Camera intrinsic matrix
        self.K = np.array([
            [fx, 0, cx],
            [0, fy, cy],
            [0, 0, 1]
        ], dtype=np.float32)

        # Distortion coefficients
        self.dist_coeffs = np.array([k1, k2, p1, p2], dtype=np.float32)

    def project_3d_to_2d(self, points_3d: np.ndarray) -> np.ndarray:
        """
        Project 3D points to 2D image coordinates

        Args:
            points_3d: Array of shape (N, 3) with 3D points [X, Y, Z]

        Returns:
            Array of shape (N, 2) with 2D image coordinates [u, v]
        """
        if points_3d.ndim == 1:
            points_3d = points_3d.reshape(1, -1)

        # Normalize by Z coordinate (perspective division)
        points_2d_norm = points_3d[:, :2] / points_3d[:, 2:3]  # [X/Z, Y/Z]

        # Apply distortion
        points_2d_distorted = self._apply_distortion(points_2d_norm)

        # Apply intrinsic parameters
        points_2d = np.column_stack([
            self.fx * points_2d_distorted[:, 0] + self.cx,
            self.fy * points_2d_distorted[:, 1] + self.cy
        ])

        return points_2d

    def _apply_distortion(self, points: np.ndarray) -> np.ndarray:
        """
        Apply lens distortion to normalized points

        Args:
            points: Array of shape (N, 2) with normalized coordinates [x, y]

        Returns:
            Array of shape (N, 2) with distorted coordinates
        """
        x, y = points[:, 0], points[:, 1]

        # Calculate radial distance squared
        r2 = x * x + y * y
        r4 = r2 * r2

        # Radial distortion
        radial_factor = 1 + self.k1 * r2 + self.k2 * r4

        # Tangential distortion
        dx = 2 * self.p1 * x * y + self.p2 * (r2 + 2 * x * x)
        dy = self.p1 * (r2 + 2 * y * y) + 2 * self.p2 * x * y

        # Apply distortion
        x_distorted = x * radial_factor + dx
        y_distorted = y * radial_factor + dy

        return np.column_stack([x_distorted, y_distorted])

    def unproject_2d_to_3d(self, points_2d: np.ndarray, depth: float) -> np.ndarray:
        """
        Unproject 2D points to 3D with given depth

        Args:
            points_2d: Array of shape (N, 2) with 2D image coordinates [u, v]
            depth: Depth value for all points

        Returns:
            Array of shape (N, 3) with 3D world coordinates [X, Y, Z]
        """
        if points_2d.ndim == 1:
            points_2d = points_2d.reshape(1, -1)

        # Remove principal point offset
        x_norm = (points_2d[:, 0] - self.cx) / self.fx
        y_norm = (points_2d[:, 1] - self.cy) / self.fy

        # Remove distortion
        points_norm = np.column_stack([x_norm, y_norm])
        points_norm_undistorted = self._remove_distortion(points_norm)

        # Scale by depth
        x_3d = points_norm_undistorted[:, 0] * depth
        y_3d = points_norm_undistorted[:, 1] * depth
        z_3d = np.full_like(x_3d, depth)

        return np.column_stack([x_3d, y_3d, z_3d])

    def _remove_distortion(self, points: np.ndarray, max_iterations: int = 5) -> np.ndarray:
        """
        Remove lens distortion from normalized points (iterative method)
        """
        # For simplicity, we'll use the first-order approximation
        # In practice, iterative methods are used
        x, y = points[:, 0], points[:, 1]

        # Calculate radial distance squared
        r2 = x * x + y * y
        r4 = r2 * r2

        # Approximate undistortion (first-order)
        factor = 1 - self.k1 * r2 - self.k2 * r4

        # Apply approximate undistortion
        x_undistorted = x * factor - 2 * self.p1 * x * y - self.p2 * (r2 + 2 * x * x)
        y_undistorted = y * factor - self.p1 * (r2 + 2 * y * y) - 2 * self.p2 * x * y

        return np.column_stack([x_undistorted, y_undistorted])

    def get_fov(self) -> Tuple[float, float]:
        """
        Calculate field of view in degrees

        Returns:
            (horizontal_fov, vertical_fov) in degrees
        """
        h_fov = 2 * np.arctan(self.width / (2 * self.fx))
        v_fov = 2 * np.arctan(self.height / (2 * self.fy))
        return np.degrees(h_fov), np.degrees(v_fov)

    def is_point_in_image(self, points_2d: np.ndarray) -> np.ndarray:
        """
        Check if 2D points are within image bounds

        Args:
            points_2d: Array of shape (N, 2) with 2D image coordinates

        Returns:
            Boolean array of shape (N,) indicating if points are in image
        """
        u, v = points_2d[:, 0], points_2d[:, 1]
        return (u >= 0) & (u < self.width) & (v >= 0) & (v < self.height)
```

## Stereo Camera Models

### Stereo Geometry

Stereo vision systems use two cameras to estimate depth through triangulation. The fundamental relationship is based on the disparity between corresponding points in left and right images.

```python
class StereoCameraModel:
    """Stereo camera model implementation"""

    def __init__(self,
                 left_camera: PinholeCameraModel,
                 right_camera: PinholeCameraModel,
                 baseline: float,  # Distance between camera centers
                 rotation: np.ndarray = None,  # 3x3 rotation matrix
                 translation: np.ndarray = None):  # 3x1 translation vector

        self.left_camera = left_camera
        self.right_camera = right_camera
        self.baseline = baseline

        # Extrinsics: right camera pose relative to left camera
        self.R = rotation if rotation is not None else np.eye(3)
        self.T = translation if translation is not None else np.array([-baseline, 0, 0])

        # Stereo calibration parameters
        self.Q = self._compute_disparity_to_depth_matrix()

    def _compute_disparity_to_depth_matrix(self) -> np.ndarray:
        """
        Compute the 4x4 disparity-to-depth mapping matrix Q
        This is used by OpenCV's reprojectImageTo3D
        """
        # Simplified Q matrix computation
        # In practice, this comes from stereo calibration
        Tx = self.T[0]  # Baseline
        f = self.left_camera.fx  # Focal length (assuming same for both cameras)

        Q = np.array([
            [1, 0, 0, -self.left_camera.cx],
            [0, 1, 0, -self.left_camera.cy],
            [0, 0, 0, f],
            [0, 0, -1/Tx, (self.left_camera.cx - self.right_camera.cx)/Tx]
        ])

        return Q

    def compute_depth_from_disparity(self, disparity: float, point_left: np.ndarray) -> float:
        """
        Compute depth from disparity value

        Args:
            disparity: Pixel disparity (u_left - u_right)
            point_left: Left image point coordinates [u, v]

        Returns:
            Depth in camera units
        """
        if disparity <= 0:
            return float('inf')  # Invalid disparity

        # Depth = (focal_length * baseline) / disparity
        depth = (self.left_camera.fx * self.baseline) / disparity
        return depth

    def triangulate_point(self, point_left: np.ndarray, point_right: np.ndarray) -> np.ndarray:
        """
        Triangulate 3D point from stereo correspondences

        Args:
            point_left: Left image point [u, v]
            point_right: Right image point [u, v]

        Returns:
            3D point in left camera coordinate system
        """
        u1, v1 = point_left
        u2, v2 = point_right

        # Formulate the triangulation problem
        # Using the DLT (Direct Linear Transform) method
        A = np.array([
            self.left_camera.fx * (u1 - self.left_camera.cx),
            self.left_camera.fy * (v1 - self.left_camera.cy),
            self.left_camera.fx * self.baseline,
            self.left_camera.fx * (u2 - self.right_camera.cx)
        ])

        # Simplified triangulation (assuming rectified cameras)
        disparity = u1 - u2
        if disparity <= 0:
            return np.array([0, 0, float('inf')])

        depth = (self.left_camera.fx * self.baseline) / disparity
        X = depth * (u1 - self.left_camera.cx) / self.left_camera.fx
        Y = depth * (v1 - self.left_camera.cy) / self.left_camera.fy

        return np.array([X, Y, depth])
```

## Fisheye Camera Models

Fisheye cameras provide wide field of view, which is particularly useful for humanoid robots that need to perceive their environment in a wide area.

```python
class FisheyeCameraModel:
    """Fisheye camera model implementation"""

    def __init__(self,
                 fx: float,
                 fy: float,
                 cx: float,
                 cy: float,
                 width: int,
                 height: int,
                 k1: float = 0.0,
                 k2: float = 0.0,
                 k3: float = 0.0,
                 k4: float = 0.0):
        """
        Initialize fisheye camera model

        Args:
            fx, fy: Focal lengths in pixels
            cx, cy: Principal point coordinates
            width, height: Image dimensions
            k1-k4: Fisheye distortion coefficients
        """
        self.fx = fx
        self.fy = fy
        self.cx = cx
        self.cy = cy
        self.width = width
        self.height = height
        self.k1 = k1
        self.k2 = k2
        self.k3 = k3
        self.k4 = k4

    def project_3d_to_2d(self, points_3d: np.ndarray) -> np.ndarray:
        """
        Project 3D points to 2D using fisheye model
        """
        if points_3d.ndim == 1:
            points_3d = points_3d.reshape(1, -1)

        X, Y, Z = points_3d[:, 0], points_3d[:, 1], points_3d[:, 2]

        # Calculate angles
        theta = np.arctan2(np.sqrt(X*X + Y*Y), Z)
        phi = np.arctan2(Y, X)

        # Apply fisheye distortion
        theta_d = theta * (1 + self.k1*theta**2 + self.k2*theta**4 +
                          self.k3*theta**6 + self.k4*theta**8)

        # Convert to image coordinates
        r_d = theta_d * np.sqrt(X*X + Y*Y) / (Z + 1e-8)  # Avoid division by zero
        u = r_d * np.cos(phi) * self.fx + self.cx
        v = r_d * np.sin(phi) * self.fy + self.cy

        return np.column_stack([u, v])

    def unproject_2d_to_3d(self, points_2d: np.ndarray, depth: float = 1.0) -> np.ndarray:
        """
        Unproject 2D points to 3D using fisheye model
        """
        if points_2d.ndim == 1:
            points_2d = points_2d.reshape(1, -1)

        u, v = points_2d[:, 0], points_2d[:, 1]

        # Normalize by focal length and principal point
        x = (u - self.cx) / self.fx
        y = (v - self.cy) / self.fy

        # Calculate radius
        r = np.sqrt(x*x + y*y)

        # Apply inverse distortion
        theta_d = r
        theta = theta_d / (1 + self.k1*theta_d**2 + self.k2*theta_d**4 +
                          self.k3*theta_d**6 + self.k4*theta_d**8)

        # Convert to 3D coordinates
        sin_theta = np.sin(theta)
        cos_theta = np.cos(theta)

        X = depth * x * sin_theta / (r + 1e-8)
        Y = depth * y * sin_theta / (r + 1e-8)
        Z = depth * cos_theta

        return np.column_stack([X, Y, Z])
```

## Camera Calibration

### Calibration Patterns and Methods

Camera calibration is the process of determining the intrinsic and extrinsic parameters of a camera system. For humanoid robotics, accurate calibration is crucial for reliable VSLAM performance.

```python
#!/usr/bin/env python3
"""
Camera calibration implementation
"""
import numpy as np
import cv2
from typing import Tuple, List, Optional
import matplotlib.pyplot as plt

class CameraCalibrator:
    """Camera calibration class"""

    def __init__(self, pattern_size: Tuple[int, int] = (9, 6), square_size: float = 1.0):
        """
        Initialize calibrator

        Args:
            pattern_size: Number of inner corners (width, height)
            square_size: Size of chessboard squares in world units
        """
        self.pattern_size = pattern_size
        self.square_size = square_size

        # Object points (3D points of chessboard corners in real world)
        self.objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)
        self.objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)
        self.objp *= square_size

        # Arrays to store object points and image points from all images
        self.obj_points = []  # 3D points in real world space
        self.img_points = []  # 2D points in image plane

    def detect_chessboard(self, image: np.ndarray) -> Tuple[bool, Optional[np.ndarray]]:
        """
        Detect chessboard pattern in image

        Args:
            image: Input image

        Returns:
            (success, corners) tuple
        """
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Find chessboard corners
        ret, corners = cv2.findChessboardCorners(
            gray, self.pattern_size,
            cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE
        )

        if ret:
            # Refine corner locations
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
            corners = cv2.cornerSubPix(
                gray, corners, (11, 11), (-1, -1), criteria
            )

        return ret, corners

    def add_calibration_image(self, image: np.ndarray) -> bool:
        """
        Add an image to the calibration dataset

        Args:
            image: Calibration image

        Returns:
            True if chessboard was detected and added
        """
        ret, corners = self.detect_chessboard(image)

        if ret:
            self.obj_points.append(self.objp)
            self.img_points.append(corners)
            return True

        return False

    def calibrate_camera(self) -> Tuple[Optional[np.ndarray], Optional[np.ndarray],
                                       Optional[np.ndarray], Optional[np.ndarray]]:
        """
        Perform camera calibration

        Returns:
            (ret, mtx, dist, rvecs, tvecs) - Calibration results
        """
        if len(self.obj_points) < 10:
            print(f"Warning: Only {len(self.obj_points)} images used for calibration, recommend at least 10")
            if len(self.obj_points) == 0:
                return None, None, None, None

        # Calibrate camera
        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
            self.obj_points, self.img_points,
            (self.img_points[0].shape[1], self.img_points[0].shape[0]),
            None, None
        )

        return ret, mtx, dist, rvecs

    def compute_reprojection_error(self, mtx: np.ndarray, dist: np.ndarray,
                                 rvecs: List[np.ndarray], tvecs: List[np.ndarray]) -> float:
        """
        Compute reprojection error

        Args:
            mtx: Camera matrix
            dist: Distortion coefficients
            rvecs: Rotation vectors
            tvecs: Translation vectors

        Returns:
            Average reprojection error
        """
        total_error = 0
        total_points = 0

        for i in range(len(self.obj_points)):
            # Reproject object points
            img_points2, _ = cv2.projectPoints(
                self.obj_points[i], rvecs[i], tvecs[i], mtx, dist
            )

            # Calculate error
            error = cv2.norm(self.img_points[i], img_points2, cv2.NORM_L2) / len(img_points2)
            total_error += error
            total_points += 1

        return total_error / total_points if total_points > 0 else 0

    def undistort_image(self, image: np.ndarray, mtx: np.ndarray, dist: np.ndarray) -> np.ndarray:
        """
        Undistort an image using calibration parameters

        Args:
            image: Input image
            mtx: Camera matrix
            dist: Distortion coefficients

        Returns:
            Undistorted image
        """
        h, w = image.shape[:2]

        # Get optimal camera matrix
        newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))

        # Undistort
        dst = cv2.undistort(image, mtx, dist, None, newcameramtx)

        # Crop image based on ROI
        x, y, w, h = roi
        dst = dst[y:y+h, x:x+w]

        return dst
```

### Stereo Calibration

```python
class StereoCalibrator:
    """Stereo camera calibration"""

    def __init__(self, pattern_size: Tuple[int, int] = (9, 6), square_size: float = 1.0):
        self.pattern_size = pattern_size
        self.square_size = square_size

        # Object points
        self.objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)
        self.objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)
        self.objp *= square_size

        # Storage for image points
        self.obj_points = []
        self.left_img_points = []
        self.right_img_points = []

    def detect_chessboard_pair(self, left_img: np.ndarray, right_img: np.ndarray) -> Tuple[bool, np.ndarray, np.ndarray]:
        """
        Detect chessboard in both left and right images
        """
        gray_left = cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY)
        gray_right = cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY)

        ret_left, corners_left = cv2.findChessboardCorners(
            gray_left, self.pattern_size,
            cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE
        )

        ret_right, corners_right = cv2.findChessboardCorners(
            gray_right, self.pattern_size,
            cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE
        )

        if ret_left and ret_right:
            # Refine corner locations
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
            corners_left = cv2.cornerSubPix(
                gray_left, corners_left, (11, 11), (-1, -1), criteria
            )
            corners_right = cv2.cornerSubPix(
                gray_right, corners_right, (11, 11), (-1, -1), criteria
            )

        return ret_left and ret_right, corners_left, corners_right

    def calibrate_stereo(self, left_images: List[np.ndarray], right_images: List[np.ndarray]) -> dict:
        """
        Perform stereo calibration
        """
        for left_img, right_img in zip(left_images, right_images):
            ret, corners_left, corners_right = self.detect_chessboard_pair(left_img, right_img)

            if ret:
                self.obj_points.append(self.objp)
                self.left_img_points.append(corners_left)
                self.right_img_points.append(corners_right)

        if len(self.obj_points) < 10:
            raise ValueError(f"Need at least 10 valid image pairs for calibration, got {len(self.obj_points)}")

        # Calibrate individual cameras first
        ret_left, mtx_left, dist_left, rvecs_left, tvecs_left = cv2.calibrateCamera(
            self.obj_points, self.left_img_points,
            (left_images[0].shape[1], left_images[0].shape[0]),
            None, None
        )

        ret_right, mtx_right, dist_right, rvecs_right, tvecs_right = cv2.calibrateCamera(
            self.obj_points, self.right_img_points,
            (right_images[0].shape[1], right_images[0].shape[0]),
            None, None
        )

        # Stereo calibration
        flags = cv2.CALIB_FIX_INTRINSIC
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 1e-5)

        ret, mtx_left, dist_left, mtx_right, dist_right, R, T, E, F = cv2.stereoCalibrate(
            self.obj_points, self.left_img_points, self.right_img_points,
            mtx_left, dist_left, mtx_right, dist_right,
            (left_images[0].shape[1], left_images[0].shape[0]),
            criteria=criteria, flags=flags
        )

        return {
            'ret': ret,
            'mtx_left': mtx_left,
            'dist_left': dist_left,
            'mtx_right': mtx_right,
            'dist_right': dist_right,
            'R': R,  # Rotation between cameras
            'T': T,  # Translation between cameras (baseline)
            'E': E,  # Essential matrix
            'F': F,  # Fundamental matrix
            'baseline': np.linalg.norm(T)  # Baseline distance
        }
```

## Camera Models for Humanoid Robotics

### Multi-Camera Systems

Humanoid robots often use multiple cameras for different purposes (navigation, manipulation, human interaction). Here's how to manage multiple camera models:

```python
class MultiCameraSystem:
    """Management system for multiple cameras on humanoid robot"""

    def __init__(self):
        self.cameras = {}
        self.transforms = {}  # Transform from each camera to robot base frame

    def add_camera(self, name: str, camera_model, transform_to_base: np.ndarray):
        """
        Add a camera to the system

        Args:
            name: Camera identifier
            camera_model: Camera model instance (PinholeCameraModel, etc.)
            transform_to_base: 4x4 transformation matrix from camera to robot base frame
        """
        self.cameras[name] = camera_model
        self.transforms[name] = transform_to_base

    def project_3d_to_camera(self, world_point: np.ndarray, camera_name: str) -> np.ndarray:
        """
        Project a 3D world point to camera coordinates
        """
        if camera_name not in self.cameras:
            raise ValueError(f"Camera {camera_name} not found")

        # Transform point to camera frame
        transform = self.transforms[camera_name]
        cam_point = transform @ np.append(world_point, 1)[:4]  # Apply transformation

        # Project to 2D image
        camera = self.cameras[camera_name]
        img_point = camera.project_3d_to_2d(cam_point[:3].reshape(1, -1))[0]

        return img_point

    def get_camera_pose(self, camera_name: str) -> Tuple[np.ndarray, np.ndarray]:
        """
        Get camera pose (position and orientation) in robot base frame
        """
        if camera_name not in self.transforms:
            raise ValueError(f"Camera {camera_name} not found")

        transform = self.transforms[camera_name]
        position = transform[:3, 3]
        orientation = transform[:3, :3]

        return position, orientation

    def triangulate_from_stereo_pair(self, left_point: np.ndarray, right_point: np.ndarray,
                                   left_camera: str, right_camera: str) -> np.ndarray:
        """
        Triangulate 3D point from stereo camera pair
        """
        # This would involve more complex stereo triangulation
        # considering the relative poses of the cameras
        pass
```

### Depth Camera Integration

Many humanoid robots use RGB-D cameras that provide both color and depth information:

```python
class RGBDCameraModel:
    """Model for RGB-D cameras (e.g., Intel RealSense, Kinect)"""

    def __init__(self, color_camera: PinholeCameraModel, depth_camera: PinholeCameraModel,
                 transform_color_to_depth: np.ndarray):
        """
        Initialize RGB-D camera model

        Args:
            color_camera: Color camera model
            depth_camera: Depth camera model
            transform_color_to_depth: Transform from color camera to depth camera frame
        """
        self.color_camera = color_camera
        self.depth_camera = depth_camera
        self.T_cd = transform_color_to_depth  # Transform color to depth
        self.T_dc = np.linalg.inv(transform_color_to_depth)  # Transform depth to color

    def color_to_depth(self, color_point: np.ndarray) -> np.ndarray:
        """
        Transform point from color camera coordinates to depth camera coordinates
        """
        # Convert 2D color point to 3D ray in color camera frame
        x_c = (color_point[0] - self.color_camera.cx) / self.color_camera.fx
        y_c = (color_point[1] - self.color_camera.cy) / self.color_camera.fy

        # Transform ray to depth camera frame
        ray_c = np.array([x_c, y_c, 1.0])
        ray_d = self.T_cd[:3, :3] @ ray_c  # Only rotation part
        origin_d = self.T_cd[:3, 3]

        # Convert back to 2D coordinates in depth camera
        x_d = ray_d[0] / ray_d[2] * self.depth_camera.fx + self.depth_camera.cx
        y_d = ray_d[1] / ray_d[2] * self.depth_camera.fy + self.depth_camera.cy

        return np.array([x_d, y_d])

    def depth_to_color(self, depth_point: np.ndarray) -> np.ndarray:
        """
        Transform point from depth camera coordinates to color camera coordinates
        """
        # Convert 2D depth point to 3D ray in depth camera frame
        x_d = (depth_point[0] - self.depth_camera.cx) / self.depth_camera.fx
        y_d = (depth_point[1] - self.depth_camera.cy) / self.depth_camera.fy

        # Transform ray to color camera frame
        ray_d = np.array([x_d, y_d, 1.0])
        ray_c = self.T_dc[:3, :3] @ ray_d  # Only rotation part
        origin_c = self.T_dc[:3, 3]

        # Convert back to 2D coordinates in color camera
        x_c = ray_c[0] / ray_c[2] * self.color_camera.fx + self.color_camera.cx
        y_c = ray_c[1] / ray_c[2] * self.color_camera.fy + self.color_camera.cy

        return np.array([x_c, y_c])
```

## Calibration Best Practices for Humanoid Robotics

### Onboard Calibration

For humanoid robots, it's important to have methods for in-field calibration:

```python
class AdaptiveCalibrator:
    """Adaptive calibration for changing conditions"""

    def __init__(self, initial_camera_model: PinholeCameraModel, update_threshold: float = 0.1):
        self.camera_model = initial_camera_model
        self.update_threshold = update_threshold
        self.reference_points = []  # Known 3D-2D correspondences
        self.calibration_history = []

    def add_reference_point(self, world_point: np.ndarray, image_point: np.ndarray):
        """
        Add a reference point for adaptive calibration
        """
        self.reference_points.append((world_point, image_point))

    def update_calibration_if_needed(self, test_image: np.ndarray) -> bool:
        """
        Check if calibration needs updating based on reference points
        """
        if len(self.reference_points) < 3:
            return False

        # Project known 3D points using current model
        world_points = np.array([p[0] for p in self.reference_points])
        projected_points = self.camera_model.project_3d_to_2d(world_points)
        actual_points = np.array([p[1] for p in self.reference_points])

        # Calculate reprojection error
        errors = np.linalg.norm(projected_points - actual_points, axis=1)
        avg_error = np.mean(errors)

        if avg_error > self.update_threshold:
            # Consider recalibrating
            print(f"Calibration drift detected: avg error = {avg_error:.3f}")
            return True

        return False

    def validate_calibration(self, test_image: np.ndarray) -> dict:
        """
        Validate current calibration on test image
        """
        # This would typically involve detecting known patterns
        # and comparing expected vs actual locations
        validation_results = {
            'is_valid': True,
            'error_metrics': {},
            'recommendation': 'Keep current calibration'
        }

        return validation_results
```

## Performance Considerations

### Efficient Camera Model Implementation

For real-time applications in humanoid robotics, efficiency is crucial:

```python
class EfficientCameraModel:
    """Optimized camera model for real-time applications"""

    def __init__(self, fx: float, fy: float, cx: float, cy: float,
                 k1: float = 0.0, k2: float = 0.0, p1: float = 0.0, p2: float = 0.0):
        # Precompute frequently used values
        self.fx = fx
        self.fy = fy
        self.cx = cx
        self.cy = cy
        self.k1 = k1
        self.k2 = k2
        self.p1 = p1
        self.p2 = p2

        # Precompute inverse focal lengths
        self.inv_fx = 1.0 / fx if fx != 0 else 0
        self.inv_fy = 1.0 / fy if fy != 0 else 0

    def project_batch(self, points_3d: np.ndarray) -> np.ndarray:
        """
        Efficiently project multiple 3D points to 2D
        """
        X, Y, Z = points_3d[:, 0], points_3d[:, 1], points_3d[:, 2]

        # Vectorized operations
        X_Z = X / Z
        Y_Z = Y / Z

        # Calculate distortion
        r2 = X_Z * X_Z + Y_Z * Y_Z
        r4 = r2 * r2

        radial_factor = 1 + self.k1 * r2 + self.k2 * r4
        dx = 2 * self.p1 * X_Z * Y_Z + self.p2 * (r2 + 2 * X_Z * X_Z)
        dy = self.p1 * (r2 + 2 * Y_Z * Y_Z) + 2 * self.p2 * X_Z * Y_Z

        x_distorted = X_Z * radial_factor + dx
        y_distorted = Y_Z * radial_factor + dy

        u = self.fx * x_distorted + self.cx
        v = self.fy * y_distorted + self.cy

        return np.column_stack([u, v])
```

## Summary

Camera models are fundamental to VSLAM systems in humanoid robotics. The choice of camera model and calibration approach significantly impacts the performance of the SLAM system:

1. **Pinhole Model**: Most common, good for standard cameras
2. **Fisheye Model**: Essential for wide-angle cameras
3. **Stereo Model**: Required for depth estimation
4. **RGB-D Integration**: Combines color and depth information

For humanoid robotics applications, consider:

- **Real-time performance**: Optimize for the robot's computational capabilities
- **Robustness**: Account for environmental changes and camera movement
- **Accuracy**: Maintain precision for navigation and manipulation tasks
- **Flexibility**: Support multiple camera configurations and types

Proper camera calibration and modeling are essential for reliable VSLAM performance, especially when the robot operates in dynamic environments where precise spatial understanding is critical for navigation and interaction.