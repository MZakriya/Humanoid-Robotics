---
title: Feature Detection
sidebar_label: Feature Detection
description: Comprehensive guide to feature detection algorithms and techniques for visual SLAM in humanoid robotics
---

# 5.1 Feature Detection

## Overview

Feature detection is a fundamental component of Visual Simultaneous Localization and Mapping (VSLAM) systems, particularly critical for humanoid robotics applications. It involves identifying distinctive points, edges, or regions in images that can be reliably tracked across multiple frames to estimate camera motion and build environmental maps. This chapter explores various feature detection algorithms, their implementation, and optimization for humanoid robotics applications.

## Feature Detection Fundamentals

### What are Features?

Features in computer vision are distinctive image locations that can be robustly detected and matched across different views of the same scene. Good features should be:

1. **Repeatably detectable**: Found consistently across different images of the same scene
2. **Distinguishable**: Unique enough to be matched correctly
3. **Localizable**: Precisely localizable in the image
4. **Robust**: Invariant to illumination changes, viewpoint changes, and noise

### Mathematical Foundation

Feature detection often involves finding local extrema of a function that measures "cornerness" or distinctiveness at each image point. For a 2D image I(x,y), we want to find points that are distinctive compared to their neighborhood.

```python
#!/usr/bin/env python3
"""
Feature detection algorithms implementation
"""
import numpy as np
import cv2
from typing import List, Tuple, Optional
import matplotlib.pyplot as plt
from dataclasses import dataclass

@dataclass
class FeaturePoint:
    """Represents a detected feature point"""
    x: float
    y: float
    response: float  # Detection response strength
    octave: int = 0  # Scale space octave
    angle: float = 0.0  # Orientation
    descriptor: Optional[np.ndarray] = None

class FeatureDetector:
    """Base class for feature detectors"""
    def detect(self, image: np.ndarray) -> List[FeaturePoint]:
        """Detect features in an image"""
        raise NotImplementedError

    def compute_descriptors(self, image: np.ndarray, keypoints: List[FeaturePoint]) -> List[FeaturePoint]:
        """Compute descriptors for detected keypoints"""
        raise NotImplementedError

class HarrisCornerDetector(FeatureDetector):
    """Implementation of Harris corner detector"""

    def __init__(self, k: float = 0.04, threshold: float = 1e-6, window_size: int = 3):
        self.k = k  # Harris detector parameter
        self.threshold = threshold
        self.window_size = window_size

    def detect(self, image: np.ndarray) -> List[FeaturePoint]:
        """Detect corners using Harris corner detector"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).astype(np.float32)
        else:
            gray = image.astype(np.float32)

        # Calculate gradients
        Ix = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=3)
        Iy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=3)

        # Calculate products of gradients
        Ixx = Ix * Ix
        Ixy = Ix * Iy
        Iyy = Iy * Iy

        # Apply Gaussian window
        Sxx = cv2.GaussianBlur(Ixx, (self.window_size, self.window_size), 0)
        Sxy = cv2.GaussianBlur(Ixy, (self.window_size, self.window_size), 0)
        Syy = cv2.GaussianBlur(Iyy, (self.window_size, self.window_size), 0)

        # Calculate Harris response
        det = Sxx * Syy - Sxy * Sxy
        trace = Sxx + Syy
        harris_response = det - self.k * trace * trace

        # Find local maxima above threshold
        keypoints = []
        h, w = harris_response.shape

        for y in range(1, h-1):
            for x in range(1, w-1):
                if harris_response[y, x] > self.threshold:
                    # Check if it's a local maximum in 3x3 neighborhood
                    local_max = True
                    for dy in [-1, 0, 1]:
                        for dx in [-1, 0, 1]:
                            if harris_response[y, x] < harris_response[y+dy, x+dx]:
                                local_max = False
                                break
                        if not local_max:
                            break

                    if local_max:
                        keypoints.append(FeaturePoint(
                            x=x, y=y, response=harris_response[y, x]
                        ))

        return keypoints

class FASTDetector(FeatureDetector):
    """Implementation of FAST corner detector"""

    def __init__(self, threshold: int = 20, non_max_suppression: bool = True):
        self.threshold = threshold
        self.non_max_suppression = non_max_suppression

    def detect(self, image: np.ndarray) -> List[FeaturePoint]:
        """Detect corners using FAST algorithm"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # Use OpenCV's FAST implementation for efficiency
        fast = cv2.FastFeatureDetector_create(
            threshold=self.threshold,
            nonmaxSuppression=self.non_max_suppression
        )

        cv_keypoints = fast.detect(gray)
        keypoints = [
            FeaturePoint(
                x=k.pt[0],
                y=k.pt[1],
                response=k.response,
                angle=k.angle if hasattr(k, 'angle') else 0.0
            )
            for k in cv_keypoints
        ]

        return keypoints

class SIFTDetector(FeatureDetector):
    """SIFT feature detector and descriptor"""

    def __init__(self):
        # Use OpenCV's SIFT implementation
        self.sift = cv2.SIFT_create()

    def detect(self, image: np.ndarray) -> List[FeaturePoint]:
        """Detect SIFT keypoints"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        cv_keypoints = self.sift.detect(gray)
        keypoints = [
            FeaturePoint(
                x=k.pt[0],
                y=k.pt[1],
                response=k.response,
                octave=k.octave,
                angle=k.angle
            )
            for k in cv_keypoints
        ]

        return keypoints

    def compute_descriptors(self, image: np.ndarray, keypoints: List[FeaturePoint]) -> List[FeaturePoint]:
        """Compute SIFT descriptors for keypoints"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # Convert FeaturePoint back to cv2.KeyPoint for descriptor computation
        cv_keypoints = [
            cv2.KeyPoint(x=k.x, y=k.y, _size=1, _angle=k.angle, _response=k.response, _octave=k.octave)
            for k in keypoints
        ]

        cv_keypoints, descriptors = self.sift.compute(gray, cv_keypoints)

        # Update keypoints with descriptors
        updated_keypoints = []
        for i, (cv_kp, desc) in enumerate(zip(cv_keypoints, descriptors)):
            kp = keypoints[i]
            kp.descriptor = desc
            updated_keypoints.append(kp)

        return updated_keypoints
```

## Advanced Feature Detection Techniques

### Scale-Invariant Feature Transform (SIFT)

SIFT is one of the most robust feature detection algorithms, providing scale and rotation invariance:

```python
class SIFTDetectorAdvanced(FeatureDetector):
    """Advanced SIFT implementation with custom parameters"""

    def __init__(self,
                 nfeatures: int = 4000,
                 nOctaveLayers: int = 3,
                 contrastThreshold: float = 0.04,
                 edgeThreshold: float = 10,
                 sigma: float = 1.6):

        self.sift = cv2.SIFT_create(
            nfeatures=nfeatures,
            nOctaveLayers=nOctaveLayers,
            contrastThreshold=contrastThreshold,
            edgeThreshold=edgeThreshold,
            sigma=sigma
        )

    def detect_and_compute(self, image: np.ndarray) -> Tuple[List[FeaturePoint], np.ndarray]:
        """Detect and compute SIFT features in one step"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        cv_keypoints, descriptors = self.sift.detectAndCompute(gray, None)

        keypoints = [
            FeaturePoint(
                x=k.pt[0],
                y=k.pt[1],
                response=k.response,
                octave=k.octave,
                angle=k.angle,
                descriptor=desc
            )
            for k, desc in zip(cv_keypoints, descriptors)
        ]

        return keypoints, descriptors

    def match_features(self, desc1: np.ndarray, desc2: np.ndarray, ratio_threshold: float = 0.7) -> List[Tuple[int, int]]:
        """Match features using Lowe's ratio test"""
        # Create BF matcher
        bf = cv2.BFMatcher()
        matches = bf.knnMatch(desc1, desc2, k=2)

        # Apply Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < ratio_threshold * n.distance:
                    good_matches.append((m.queryIdx, m.trainIdx))

        return good_matches
```

### Speeded-Up Robust Features (SURF)

SURF provides faster computation than SIFT while maintaining good performance:

```python
class SURFDetector(FeatureDetector):
    """SURF feature detector implementation"""

    def __init__(self, hessianThreshold: float = 400.0):
        # Note: SURF requires non-free modules in OpenCV
        # This is a conceptual implementation
        self.hessianThreshold = hessianThreshold

    def detect(self, image: np.ndarray) -> List[FeaturePoint]:
        """Detect SURF features (conceptual)"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # SURF uses Hessian matrix determinant to find interest points
        # This is a simplified conceptual implementation
        keypoints = []

        # In practice, use OpenCV's SURF implementation:
        # surf = cv2.xfeatures2d.SURF_create(hessianThreshold=self.hessianThreshold)
        # cv_keypoints, descriptors = surf.detectAndCompute(gray, None)

        # For now, return empty list - actual implementation would use proper SURF
        return keypoints
```

### Oriented FAST and Rotated BRIEF (ORB)

ORB provides a fast, efficient alternative to SIFT:

```python
class ORBDetector(FeatureDetector):
    """ORB feature detector implementation"""

    def __init__(self,
                 nfeatures: int = 500,
                 scaleFactor: float = 1.2,
                 nlevels: int = 8,
                 edgeThreshold: int = 31,
                 patchSize: int = 31,
                 fastThreshold: int = 20):

        self.orb = cv2.ORB_create(
            nfeatures=nfeatures,
            scaleFactor=scaleFactor,
            nlevels=nlevels,
            edgeThreshold=edgeThreshold,
            patchSize=patchSize,
            fastThreshold=fastThreshold
        )

    def detect_and_compute(self, image: np.ndarray) -> Tuple[List[FeaturePoint], np.ndarray]:
        """Detect and compute ORB features"""
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        cv_keypoints, descriptors = self.orb.detectAndCompute(gray, None)

        keypoints = [
            FeaturePoint(
                x=k.pt[0],
                y=k.pt[1],
                response=k.response,
                octave=k.octave,
                angle=k.angle,
                descriptor=desc
            )
            for k, desc in zip(cv_keypoints, descriptors)
        ]

        return keypoints, descriptors
```

## Feature Detection for Humanoid Robotics

### Real-time Feature Detection

For humanoid robots, real-time performance is critical. Here's an optimized approach:

```python
#!/usr/bin/env python3
"""
Real-time feature detection optimized for humanoid robotics
"""
import numpy as np
import cv2
import time
from threading import Thread, Lock
from queue import Queue
from typing import List, Tuple, Optional

class RealTimeFeatureDetector:
    """Real-time optimized feature detector for humanoid robots"""

    def __init__(self,
                 method: str = 'ORB',
                 max_features: int = 1000,
                 detection_rate: float = 30.0,  # Hz
                 use_gpu: bool = False):

        self.method = method
        self.max_features = max_features
        self.detection_rate = detection_rate
        self.use_gpu = use_gpu
        self.running = False
        self.frame_queue = Queue(maxsize=2)  # Only keep latest 2 frames
        self.result_queue = Queue(maxsize=2)
        self.lock = Lock()

        # Initialize detector based on method
        if method == 'ORB':
            self.detector = cv2.ORB_create(nfeatures=max_features)
        elif method == 'SIFT':
            self.detector = cv2.SIFT_create(nfeatures=max_features)
        elif method == 'FAST':
            self.detector = cv2.FastFeatureDetector_create()
        else:
            raise ValueError(f"Unsupported method: {method}")

    def start_detection(self):
        """Start real-time feature detection"""
        self.running = True
        self.detection_thread = Thread(target=self._detection_loop)
        self.detection_thread.start()

    def stop_detection(self):
        """Stop real-time feature detection"""
        self.running = False
        if hasattr(self, 'detection_thread'):
            self.detection_thread.join()

    def _detection_loop(self):
        """Main detection loop running in separate thread"""
        while self.running:
            try:
                # Get latest frame
                if self.frame_queue.empty():
                    time.sleep(0.001)  # Brief sleep to prevent busy waiting
                    continue

                # Get the most recent frame, discarding older ones
                while self.frame_queue.qsize() > 1:
                    self.frame_queue.get()

                frame = self.frame_queue.get()

                # Process frame
                start_time = time.time()
                keypoints, descriptors = self._detect_features(frame)
                processing_time = time.time() - start_time

                # Put results in queue
                result = {
                    'keypoints': keypoints,
                    'descriptors': descriptors,
                    'timestamp': time.time(),
                    'processing_time': processing_time
                }

                # Only keep the latest result
                while not self.result_queue.empty():
                    self.result_queue.get()

                self.result_queue.put(result)

                # Control frame rate
                target_interval = 1.0 / self.detection_rate
                sleep_time = max(0, target_interval - processing_time)
                time.sleep(sleep_time)

            except Exception as e:
                print(f"Error in detection loop: {e}")
                time.sleep(0.01)  # Brief sleep on error

    def _detect_features(self, frame: np.ndarray) -> Tuple[List[cv2.KeyPoint], np.ndarray]:
        """Detect features in a single frame"""
        if len(frame.shape) == 3:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray = frame

        # Detect keypoints and compute descriptors
        keypoints, descriptors = self.detector.detectAndCompute(gray, None)

        # Limit number of features if necessary
        if keypoints and len(keypoints) > self.max_features:
            # Sort by response and keep top features
            keypoints = sorted(keypoints, key=lambda x: x.response, reverse=True)
            keypoints = keypoints[:self.max_features]

            if descriptors is not None:
                # Create new descriptor array with selected keypoints
                indices = [kp.pt for kp in keypoints]
                # This is a simplified approach - in practice, you'd need to map indices properly
                # For now, just return the first max_features descriptors
                descriptors = descriptors[:self.max_features]

        return keypoints or [], descriptors

    def add_frame(self, frame: np.ndarray):
        """Add a frame for processing"""
        try:
            self.frame_queue.put_nowait(frame)
        except:
            # Queue is full, frame is dropped
            pass

    def get_latest_features(self) -> Optional[dict]:
        """Get the latest detected features"""
        if not self.result_queue.empty():
            return self.result_queue.get()
        return None

    def get_processing_stats(self) -> dict:
        """Get processing statistics"""
        if not self.result_queue.empty():
            # Look at the latest result without removing it
            latest = self.result_queue.queue[-1]  # This is implementation-dependent
            return {
                'processing_time': latest.get('processing_time', 0),
                'feature_count': len(latest.get('keypoints', [])),
                'fps': 1.0 / latest.get('processing_time', 0.033) if latest.get('processing_time', 0) > 0 else 0
            }
        return {'processing_time': 0, 'feature_count': 0, 'fps': 0}
```

### Multi-scale Feature Detection

For humanoid robots that need to detect features at different distances:

```python
class MultiScaleFeatureDetector:
    """Feature detector that works at multiple scales"""

    def __init__(self, base_detector: FeatureDetector, scales: List[float] = [0.5, 1.0, 2.0]):
        self.base_detector = base_detector
        self.scales = scales

    def detect_multiscale(self, image: np.ndarray) -> List[FeaturePoint]:
        """Detect features at multiple scales"""
        all_keypoints = []

        for scale in self.scales:
            if scale == 1.0:
                scaled_image = image
            else:
                new_size = (int(image.shape[1] * scale), int(image.shape[0] * scale))
                scaled_image = cv2.resize(image, new_size)

            # Detect features at this scale
            keypoints = self.base_detector.detect(scaled_image)

            # Scale coordinates back to original image size
            for kp in keypoints:
                kp.x /= scale
                kp.y /= scale
                kp.octave = int(np.log2(scale)) if scale > 0 else 0

            all_keypoints.extend(keypoints)

        # Remove duplicate keypoints
        all_keypoints = self._remove_duplicates(all_keypoints)

        return all_keypoints

    def _remove_duplicates(self, keypoints: List[FeaturePoint], min_distance: float = 5.0) -> List[FeaturePoint]:
        """Remove duplicate keypoints that are too close together"""
        if not keypoints:
            return []

        # Sort by response (strength) in descending order
        keypoints.sort(key=lambda x: x.response, reverse=True)

        filtered_keypoints = []
        for kp in keypoints:
            is_duplicate = False
            for existing_kp in filtered_keypoints:
                distance = np.sqrt((kp.x - existing_kp.x)**2 + (kp.y - existing_kp.y)**2)
                if distance < min_distance:
                    is_duplicate = True
                    break

            if not is_duplicate:
                filtered_keypoints.append(kp)

        return filtered_keypoints
```

## Performance Optimization

### GPU Acceleration

```python
class GPUFeatureDetector:
    """GPU-accelerated feature detector using OpenCV's CUDA modules"""

    def __init__(self, use_cuda: bool = True):
        self.use_cuda = use_cuda and cv2.cuda.getCudaEnabledDeviceCount() > 0

        if self.use_cuda:
            print("Using GPU-accelerated feature detection")
            # Initialize GPU feature detectors
            self.gpu_orb = cv2.cuda.ORB_create()
        else:
            print("GPU not available, using CPU feature detection")
            self.cpu_orb = cv2.ORB_create()

    def detect(self, image: np.ndarray) -> Tuple[List[cv2.KeyPoint], np.ndarray]:
        """Detect features using GPU if available"""
        if self.use_cuda:
            # Upload image to GPU
            gpu_image = cv2.cuda_GpuMat()
            gpu_image.upload(image)

            # Convert to grayscale on GPU if needed
            if len(image.shape) == 3:
                gpu_gray = cv2.cuda_GpuMat()
                cv2.cuda.cvtColor(gpu_image, cv2.COLOR_BGR2GRAY, gpu_gray)
            else:
                gpu_gray = gpu_image

            # Detect features on GPU
            keypoints_gpu, descriptors_gpu = self.gpu_orb.detectAndCompute(gpu_gray, None)

            # Download results to CPU
            keypoints = keypoints_gpu.download() if keypoints_gpu is not None else []
            descriptors = descriptors_gpu.download() if descriptors_gpu is not None else np.array([])

            return keypoints, descriptors
        else:
            # Fallback to CPU
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image

            return self.cpu_orb.detectAndCompute(gray, None)
```

### Feature Tracking Integration

```python
class FeatureTracker:
    """Track features across multiple frames"""

    def __init__(self, max_features: int = 1000, window_size: int = 21):
        self.max_features = max_features
        self.window_size = window_size
        self.prev_keypoints = None
        self.prev_gray = None

    def track_features(self,
                      current_frame: np.ndarray,
                      prev_keypoints: List[FeaturePoint]) -> Tuple[List[FeaturePoint], np.ndarray]:
        """Track features from previous frame to current frame"""
        if len(current_frame.shape) == 3:
            current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        else:
            current_gray = current_frame

        if self.prev_gray is None:
            # First frame - just return the input keypoints
            self.prev_gray = current_gray
            self.prev_keypoints = prev_keypoints
            return prev_keypoints, np.ones(len(prev_keypoints), dtype=bool)

        # Convert FeaturePoints to points for optical flow
        if not prev_keypoints:
            return [], np.array([])

        prev_points = np.array([[kp.x, kp.y] for kp in prev_keypoints], dtype=np.float32)
        prev_points = prev_points.reshape(-1, 1, 2)

        # Calculate optical flow
        current_points, status, error = cv2.calcOpticalFlowPyrLK(
            self.prev_gray, current_gray,
            prev_points, None,
            winSize=(self.window_size, self.window_size),
            maxLevel=3,
            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01)
        )

        # Filter out lost features
        valid_indices = [i for i, s in enumerate(status) if s == 1]
        valid_current_points = current_points[valid_indices]
        valid_prev_points = prev_points[valid_indices]
        valid_keypoints = [prev_keypoints[i] for i in valid_indices]

        # Update keypoints with new positions
        tracked_keypoints = []
        for i, kp in enumerate(valid_keypoints):
            new_kp = FeaturePoint(
                x=float(valid_current_points[i][0][0]),
                y=float(valid_current_points[i][0][1]),
                response=kp.response,
                octave=kp.octave,
                angle=kp.angle,
                descriptor=kp.descriptor
            )
            tracked_keypoints.append(new_kp)

        # Update previous frame data
        self.prev_gray = current_gray
        self.prev_keypoints = tracked_keypoints

        return tracked_keypoints, status.flatten().astype(bool)
```

## Evaluation and Benchmarking

### Feature Detection Quality Metrics

```python
class FeatureEvaluation:
    """Evaluate feature detection quality"""

    def __init__(self):
        pass

    def evaluate_repeatability(self,
                             img1: np.ndarray,
                             img2: np.ndarray,
                             transform: np.ndarray) -> float:
        """Evaluate repeatability of feature detection under transformation"""
        # Detect features in both images
        detector = ORBDetector()
        kp1, _ = detector.detect_and_compute(img1)
        kp2, _ = detector.detect_and_compute(img2)

        # Transform keypoints from img1 to img2's coordinate system
        kp1_transformed = []
        for kp in kp1:
            # Apply transformation matrix
            pt = np.array([kp.x, kp.y, 1])
            transformed_pt = transform @ pt
            transformed_pt = transformed_pt / transformed_pt[2]  # Homogeneous normalization
            kp1_transformed.append(FeaturePoint(
                x=transformed_pt[0], y=transformed_pt[1],
                response=kp.response
            ))

        # Match features between transformed keypoints and detected keypoints in img2
        matches = self._match_keypoints(kp1_transformed, kp2, threshold=5.0)

        # Calculate repeatability rate
        if len(kp1) == 0:
            return 0.0

        repeatability = len(matches) / len(kp1)
        return repeatability

    def _match_keypoints(self, kp1: List[FeaturePoint], kp2: List[FeaturePoint], threshold: float) -> List[Tuple[int, int]]:
        """Simple matching based on spatial proximity"""
        matches = []
        for i, pt1 in enumerate(kp1):
            for j, pt2 in enumerate(kp2):
                distance = np.sqrt((pt1.x - pt2.x)**2 + (pt1.y - pt2.y)**2)
                if distance < threshold:
                    matches.append((i, j))
        return matches

    def evaluate_detection_distribution(self, keypoints: List[FeaturePoint], img_shape: Tuple[int, int]) -> dict:
        """Evaluate spatial distribution of detected features"""
        h, w = img_shape[:2]

        # Divide image into grid
        grid_size = 32
        grid_h = h // grid_size
        grid_w = w // grid_size

        grid_counts = np.zeros((grid_h, grid_w))

        for kp in keypoints:
            grid_y = int(kp.y // grid_size)
            grid_x = int(kp.x // grid_size)

            if 0 <= grid_y < grid_h and 0 <= grid_x < grid_w:
                grid_counts[grid_y, grid_x] += 1

        # Calculate statistics
        valid_cells = grid_counts[grid_counts > 0]
        distribution_stats = {
            'total_features': len(keypoints),
            'active_cells': len(valid_cells),
            'grid_coverage': len(valid_cells) / (grid_h * grid_w) if (grid_h * grid_w) > 0 else 0,
            'avg_features_per_active_cell': np.mean(valid_cells) if len(valid_cells) > 0 else 0,
            'std_features_per_active_cell': np.std(valid_cells) if len(valid_cells) > 0 else 0,
            'max_features_per_cell': np.max(valid_cells) if len(valid_cells) > 0 else 0
        }

        return distribution_stats

    def evaluate_feature_quality(self, keypoints: List[FeaturePoint]) -> dict:
        """Evaluate quality metrics for detected features"""
        if not keypoints:
            return {
                'avg_response': 0,
                'std_response': 0,
                'min_response': 0,
                'max_response': 0,
                'feature_count': 0
            }

        responses = [kp.response for kp in keypoints]

        quality_metrics = {
            'avg_response': np.mean(responses),
            'std_response': np.std(responses),
            'min_response': np.min(responses),
            'max_response': np.max(responses),
            'feature_count': len(keypoints)
        }

        return quality_metrics
```

## Best Practices for Humanoid Robotics

### Feature Selection for VSLAM

```python
class VSLAMFeatureSelector:
    """Feature selection optimized for VSLAM applications"""

    def __init__(self,
                 min_features: int = 50,
                 max_features: int = 1000,
                 quality_threshold: float = 0.1,
                 spatial_radius: int = 30):

        self.min_features = min_features
        self.max_features = max_features
        self.quality_threshold = quality_threshold
        self.spatial_radius = spatial_radius

    def select_features(self,
                       keypoints: List[FeaturePoint],
                       image_shape: Tuple[int, int]) -> List[FeaturePoint]:
        """Select best features for VSLAM based on quality and distribution"""

        # First, filter by quality
        high_quality_kps = [kp for kp in keypoints if kp.response > self.quality_threshold]

        # If we don't have enough features, use all available
        if len(high_quality_kps) < self.min_features:
            high_quality_kps = keypoints

        # Apply spatial distribution filtering
        selected_kps = self._apply_spatial_filtering(high_quality_kps, image_shape)

        # If still too many, select top by response
        if len(selected_kps) > self.max_features:
            selected_kps.sort(key=lambda x: x.response, reverse=True)
            selected_kps = selected_kps[:self.max_features]

        # If too few, return what we have
        return selected_kps

    def _apply_spatial_filtering(self,
                                keypoints: List[FeaturePoint],
                                image_shape: Tuple[int, int]) -> List[FeaturePoint]:
        """Apply spatial filtering to ensure good distribution"""
        h, w = image_shape[:2]

        # Create a grid to track feature distribution
        grid_size = self.spatial_radius
        grid_h = (h + grid_size - 1) // grid_size  # Ceiling division
        grid_w = (w + grid_size - 1) // grid_size

        grid = [[[] for _ in range(grid_w)] for _ in range(grid_h)]

        # Assign keypoints to grid cells
        for kp in keypoints:
            grid_y = int(kp.y // grid_size)
            grid_x = int(kp.x // grid_size)

            if 0 <= grid_y < grid_h and 0 <= grid_x < grid_w:
                grid[grid_y][grid_x].append(kp)

        # Select best feature from each non-empty cell
        selected_kps = []
        for row in grid:
            for cell in row:
                if cell:
                    # Select feature with highest response in this cell
                    best_kp = max(cell, key=lambda x: x.response)
                    selected_kps.append(best_kp)

        return selected_kps
```

## Summary

Feature detection forms the foundation of VSLAM systems in humanoid robotics. The choice of feature detection algorithm depends on the specific requirements of the application:

- **SIFT/SURF**: Best for high accuracy and robustness, but computationally expensive
- **ORB**: Good balance of speed and performance, suitable for real-time applications
- **FAST**: Fastest option, good for initial detection with tracking refinement

For humanoid robotics applications, consider the following:

1. **Real-time constraints**: Choose algorithms that meet your frame rate requirements
2. **Robustness**: Consider lighting conditions, motion blur, and environmental factors
3. **Distribution**: Ensure features are well-distributed across the image for good pose estimation
4. **Scale invariance**: Humanoid robots operate at various distances from objects
5. **Computational efficiency**: Balance accuracy with available computational resources

The implementation should include proper evaluation metrics to ensure the feature detection system performs well under the expected operating conditions of the humanoid robot.