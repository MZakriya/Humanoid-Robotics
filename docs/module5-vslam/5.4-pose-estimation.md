---
title: Pose Estimation
sidebar_label: Pose Estimation
description: Comprehensive guide to pose estimation techniques for visual SLAM in humanoid robotics
---

# 5.4 Pose Estimation

## Overview

Pose estimation is a critical component of Visual SLAM (VSLAM) systems, determining the position and orientation of the robot in 3D space. For humanoid robotics applications, accurate and robust pose estimation is essential for navigation, manipulation, and interaction with the environment. This chapter covers the mathematical foundations, algorithms, and implementation techniques for pose estimation in VSLAM systems.

## Mathematical Foundation

### Pose Representation

The pose of a camera (or robot) in 3D space is represented by a rotation matrix R and a translation vector t, forming the transformation matrix T:

```
T = [R  t]
    [0  1]
```

Where R is a 3x3 rotation matrix and t is a 3x1 translation vector.

### Lie Groups and Manifolds

For optimization purposes, poses are often represented using Lie algebra elements:

```python
#!/usr/bin/env python3
"""
Pose representation and operations for VSLAM
"""
import numpy as np
from typing import Tuple, Optional
import scipy.linalg
import cv2

class Pose:
    """SE(3) pose representation"""

    def __init__(self, rotation: np.ndarray = None, translation: np.ndarray = None, matrix: np.ndarray = None):
        """
        Initialize pose

        Args:
            rotation: 3x3 rotation matrix or 3x1 rotation vector (axis-angle)
            translation: 3x1 translation vector
            matrix: 4x4 transformation matrix
        """
        if matrix is not None:
            self.matrix = matrix.copy()
        else:
            if rotation is None:
                rotation = np.eye(3)
            elif rotation.shape == (3,):
                # Convert axis-angle to rotation matrix
                rotation = cv2.Rodrigues(rotation)[0]
            elif rotation.shape == (3, 3):
                # Verify it's a valid rotation matrix
                if not self._is_rotation_matrix(rotation):
                    raise ValueError("Invalid rotation matrix")

            if translation is None:
                translation = np.zeros(3)

            self.matrix = np.eye(4)
            self.matrix[:3, :3] = rotation
            self.matrix[:3, 3] = translation.flatten()

    def _is_rotation_matrix(self, R: np.ndarray) -> bool:
        """Check if matrix is a valid rotation matrix"""
        if R.shape != (3, 3):
            return False
        # Check determinant
        if abs(np.linalg.det(R) - 1.0) > 1e-6:
            return False
        # Check orthogonality
        if not np.allclose(R @ R.T, np.eye(3)):
            return False
        return True

    @property
    def rotation(self) -> np.ndarray:
        """Get rotation matrix"""
        return self.matrix[:3, :3]

    @property
    def translation(self) -> np.ndarray:
        """Get translation vector"""
        return self.matrix[:3, 3]

    @property
    def rotation_vector(self) -> np.ndarray:
        """Get rotation as axis-angle vector"""
        return cv2.Rodrigues(self.rotation)[0].flatten()

    def inverse(self) -> 'Pose':
        """Get inverse pose"""
        R_inv = self.rotation.T
        t_inv = -R_inv @ self.translation
        inv_matrix = np.eye(4)
        inv_matrix[:3, :3] = R_inv
        inv_matrix[:3, 3] = t_inv
        return Pose(matrix=inv_matrix)

    def compose(self, other: 'Pose') -> 'Pose':
        """Compose this pose with another (self * other)"""
        composed_matrix = self.matrix @ other.matrix
        return Pose(matrix=composed_matrix)

    def transform_point(self, point: np.ndarray) -> np.ndarray:
        """Transform a 3D point by this pose"""
        if point.shape[-1] == 3:
            # Apply rotation and translation
            return (self.rotation @ point.T).T + self.translation
        else:
            raise ValueError("Point must be of shape (..., 3)")

    def log(self) -> np.ndarray:
        """Convert to Lie algebra (se(3)) representation"""
        R = self.rotation
        t = self.translation

        # Convert rotation to so(3)
        rot_vec = cv2.Rodrigues(R)[0].flatten()

        # se(3) vector: [rho, phi] where rho is translation-like and phi is rotation
        return np.hstack([t, rot_vec])

    @classmethod
    def exp(cls, xi: np.ndarray) -> 'Pose':
        """Convert from Lie algebra to SE(3)"""
        if len(xi) != 6:
            raise ValueError("xi must be 6-dimensional")

        t = xi[:3]  # Translation part
        rot_vec = xi[3:]  # Rotation part

        R = cv2.Rodrigues(rot_vec)[0]

        matrix = np.eye(4)
        matrix[:3, :3] = R
        matrix[:3, 3] = t
        return cls(matrix=matrix)

    def __str__(self):
        return f"Pose(R={self.rotation}, t={self.translation})"

    def __repr__(self):
        return self.__str__()
```

## Direct Methods for Pose Estimation

### Perspective-n-Point (PnP) Algorithm

The PnP problem involves estimating the pose of a calibrated camera given 3D-2D point correspondences:

```python
class PnPSolver:
    """PnP solver implementation"""

    def __init__(self):
        pass

    def solve_pnp(self,
                  object_points: np.ndarray,
                  image_points: np.ndarray,
                  camera_matrix: np.ndarray,
                  dist_coeffs: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Solve PnP problem

        Args:
            object_points: 3D points in world coordinates (N, 3)
            image_points: 2D points in image coordinates (N, 2)
            camera_matrix: 3x3 camera intrinsic matrix
            dist_coeffs: Distortion coefficients (optional)

        Returns:
            (rotation_vector, translation_vector)
        """
        if dist_coeffs is None:
            dist_coeffs = np.zeros(4)

        success, rvec, tvec = cv2.solvePnP(
            object_points.astype(np.float32),
            image_points.astype(np.float32),
            camera_matrix,
            dist_coeffs
        )

        if not success:
            raise ValueError("PnP solution failed")

        return rvec, tvec

    def solve_pnp_ransac(self,
                        object_points: np.ndarray,
                        image_points: np.ndarray,
                        camera_matrix: np.ndarray,
                        dist_coeffs: np.ndarray = None,
                        reprojection_error: float = 8.0,
                        confidence: float = 0.99) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Solve PnP with RANSAC to handle outliers
        """
        if dist_coeffs is None:
            dist_coeffs = np.zeros(4)

        success, rvec, tvec, inliers = cv2.solvePnPRansac(
            object_points.astype(np.float32),
            image_points.astype(np.float32),
            camera_matrix,
            dist_coeffs,
            reprojectionError=reprojection_error,
            confidence=confidence
        )

        if not success:
            raise ValueError("PnP RANSAC solution failed")

        return rvec, tvec, inliers.flatten() if inliers is not None else np.arange(len(object_points))
```

### Direct Dense Methods

Direct methods use pixel intensities directly rather than feature points:

```python
class DirectPoseEstimator:
    """Direct pose estimation using photometric error minimization"""

    def __init__(self,
                 camera_matrix: np.ndarray,
                 width: int,
                 height: int,
                 min_gradient: float = 10.0):
        self.camera_matrix = camera_matrix
        self.width = width
        self.height = height
        self.min_gradient = min_gradient

        # Precompute camera parameters
        self.fx = camera_matrix[0, 0]
        self.fy = camera_matrix[1, 1]
        self.cx = camera_matrix[0, 2]
        self.cy = camera_matrix[1, 2]

    def estimate_pose_direct(self,
                           current_frame: np.ndarray,
                           reference_frame: np.ndarray,
                           reference_depth: np.ndarray,
                           initial_pose: Pose = None,
                           max_iterations: int = 10) -> Pose:
        """
        Estimate pose using direct image alignment

        Args:
            current_frame: Current grayscale image
            reference_frame: Reference grayscale image
            reference_depth: Depth map for reference frame
            initial_pose: Initial pose estimate
            max_iterations: Maximum optimization iterations

        Returns:
            Estimated pose
        """
        if initial_pose is None:
            current_pose = Pose()
        else:
            current_pose = initial_pose

        # Compute gradients for reference frame
        ref_grad_x = cv2.Sobel(reference_frame, cv2.CV_64F, 1, 0, ksize=3)
        ref_grad_y = cv2.Sobel(reference_frame, cv2.CV_64F, 0, 1, ksize=3)

        for iteration in range(max_iterations):
            # Compute Jacobian and error
            jacobian, error, valid_points = self._compute_jacobian_and_error(
                current_frame, reference_frame, reference_depth, current_pose
            )

            if len(valid_points) < 10:
                print(f"Warning: Too few valid points for optimization: {len(valid_points)}")
                break

            # Solve normal equation: J^T * J * dx = -J^T * error
            JTJ = jacobian.T @ jacobian
            JTe = jacobian.T @ error

            try:
                # Use SVD to solve for pose update
                U, s, Vt = np.linalg.svd(JTJ)
                s_inv = np.where(s > 1e-8, 1.0/s, 0)
                JTJ_inv = Vt.T @ np.diag(s_inv) @ U.T
                dx = JTJ_inv @ JTe

                # Update pose
                update_pose = Pose.exp(dx)
                current_pose = update_pose.compose(current_pose)

                # Check convergence
                if np.linalg.norm(dx) < 1e-6:
                    break

            except np.linalg.LinAlgError:
                print("Singular matrix in pose estimation")
                break

        return current_pose

    def _compute_jacobian_and_error(self,
                                  current_frame: np.ndarray,
                                  reference_frame: np.ndarray,
                                  reference_depth: np.ndarray,
                                  pose: Pose):
        """
        Compute Jacobian matrix and error vector for direct alignment
        """
        height, width = current_frame.shape

        # Generate 3D points from reference depth
        y_coords, x_coords = np.mgrid[0:height, 0:width]

        # Convert pixel coordinates to normalized coordinates
        x_norm = (x_coords - self.cx) / self.fx
        y_norm = (y_coords - self.cy) / self.fy

        # Create 3D points in reference camera frame
        z_ref = reference_depth
        x_ref = x_norm * z_ref
        y_ref = y_norm * z_ref

        # Stack to get 3D points
        points_3d_ref = np.stack([x_ref, y_ref, z_ref], axis=-1).reshape(-1, 3)

        # Transform points to current camera frame
        points_3d_curr = pose.transform_point(points_3d_ref)

        # Project to current image
        valid_z = points_3d_curr[:, 2] > 0  # Only points in front of camera
        points_3d_curr = points_3d_curr[valid_z]

        # Project to 2D
        x_curr = points_3d_curr[:, 0] / points_3d_curr[:, 2]
        y_curr = points_3d_curr[:, 1] / points_3d_curr[:, 2]

        u_curr = x_curr * self.fx + self.cx
        v_curr = y_curr * self.fy + self.cy

        # Check if projected points are within image bounds
        valid_bounds = (u_curr >= 0) & (u_curr < width) & (v_curr >= 0) & (v_curr < height)

        valid_indices = np.where(valid_z)[0][valid_bounds]
        u_curr = u_curr[valid_bounds]
        v_curr = v_curr[valid_bounds]

        if len(u_curr) < 10:
            return np.zeros((1, 6)), np.zeros(1), np.array([])

        # Sample intensities
        u_int = u_curr.astype(int)
        v_int = v_curr.astype(int)

        # Ensure indices are within bounds
        u_int = np.clip(u_int, 0, width - 1)
        v_int = np.clip(v_int, 0, height - 1)

        ref_intensities = reference_frame[v_int, u_int]
        curr_intensities = current_frame[v_int, u_int]

        # Compute error
        error = (curr_intensities - ref_intensities).astype(np.float64)

        # Compute Jacobian
        # For each 3D point, compute derivative of projection w.r.t. pose parameters
        x_ref_valid = points_3d_ref[valid_indices, 0]
        y_ref_valid = points_3d_ref[valid_indices, 1]
        z_ref_valid = points_3d_ref[valid_indices, 2]

        # Jacobian of projection w.r.t. 3D point
        Z_inv = 1.0 / z_ref_valid
        Z_inv2 = Z_inv * Z_inv

        # Jacobian of 3D point transformation w.r.t. se(3)
        # For SE(3) = [I + [phi]_x, rho; 0, 1] where phi is rotation, rho is translation
        J_p_rho = np.zeros((len(valid_indices), 2, 3))  # w.r.t. translation
        J_p_phi = np.zeros((len(valid_indices), 2, 3))  # w.r.t. rotation

        for i in range(len(valid_indices)):
            # Jacobian w.r.t. translation (just projection Jacobian)
            J_p_rho[i, 0, 0] = self.fx * Z_inv[i]  # dx/du
            J_p_rho[i, 0, 2] = -self.fx * x_ref_valid[i] * Z_inv2[i]  # dz/du
            J_p_rho[i, 1, 1] = self.fy * Z_inv[i]  # dy/dv
            J_p_rho[i, 1, 2] = -self.fy * y_ref_valid[i] * Z_inv2[i]  # dz/dv

            # Jacobian w.r.t. rotation (cross product terms)
            # This is more complex and simplified here
            J_p_phi[i, 0, 1] = -self.fx * x_ref_valid[i] * y_ref_valid[i] * Z_inv2[i]  # d/dphi_y
            J_p_phi[i, 0, 2] = self.fx * (1 + (x_ref_valid[i] / z_ref_valid[i])**2)  # d/dphi_z
            J_p_phi[i, 1, 0] = self.fy * x_ref_valid[i] * y_ref_valid[i] * Z_inv2[i]  # d/dphi_x
            J_p_phi[i, 1, 2] = -self.fy * (1 + (y_ref_valid[i] / z_ref_valid[i])**2)  # d/dphi_z

        # Combine into full Jacobian (each point contributes 2 rows: u and v)
        jacobian = np.zeros((len(error), 6))
        for i in range(len(error)):
            jacobian[i, :3] = J_p_rho[i, 0, :]  # Translation part for u
            jacobian[i, 3:] = J_p_phi[i, 0, :]  # Rotation part for u

        return jacobian, error, valid_indices
```

## Indirect Methods for Pose Estimation

### Feature-Based Tracking

```python
class FeatureBasedPoseEstimator:
    """Feature-based pose estimation"""

    def __init__(self,
                 camera_matrix: np.ndarray,
                 min_features: int = 50,
                 max_features: int = 1000):
        self.camera_matrix = camera_matrix
        self.min_features = min_features
        self.max_features = max_features

        # Feature detector and matcher
        self.detector = cv2.ORB_create(nfeatures=max_features)
        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)

        # Previous frame data
        self.prev_keypoints = None
        self.prev_descriptors = None
        self.prev_points_3d = None

    def estimate_pose_from_features(self,
                                  current_frame: np.ndarray,
                                  prev_frame: np.ndarray,
                                  prev_points_3d: np.ndarray,
                                  initial_pose: Pose = None) -> Tuple[Pose, np.ndarray]:
        """
        Estimate pose using tracked features with known 3D positions

        Args:
            current_frame: Current frame
            prev_frame: Previous frame (for tracking)
            prev_points_3d: 3D positions of features in previous frame
            initial_pose: Initial pose estimate

        Returns:
            (estimated_pose, inlier_indices)
        """
        # Convert to grayscale
        if len(current_frame.shape) == 3:
            current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        else:
            current_gray = current_frame

        # Detect features in current frame
        curr_keypoints = self.detector.detect(current_gray)
        if len(curr_keypoints) < self.min_features:
            # Not enough features, return identity pose
            return Pose() if initial_pose is None else initial_pose, np.array([])

        # Compute descriptors
        curr_keypoints, curr_descriptors = self.detector.compute(current_gray, curr_keypoints)

        if self.prev_descriptors is None or len(self.prev_descriptors) == 0:
            # First frame, cannot estimate pose
            self.prev_keypoints = curr_keypoints
            self.prev_descriptors = curr_descriptors
            self.prev_points_3d = prev_points_3d
            return Pose() if initial_pose is None else initial_pose, np.array([])

        # Match features between previous and current frames
        matches = self.matcher.knnMatch(self.prev_descriptors, curr_descriptors, k=2)

        # Apply Lowe's ratio test
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)

        if len(good_matches) < 8:  # Minimum for PnP
            # Not enough matches, return previous pose
            self.prev_keypoints = curr_keypoints
            self.prev_descriptors = curr_descriptors
            self.prev_points_3d = prev_points_3d
            return Pose() if initial_pose is None else initial_pose, np.array([])

        # Extract corresponding points
        prev_indices = [m.queryIdx for m in good_matches]
        curr_indices = [m.trainIdx for m in good_matches]

        # Get 3D points for matched features
        matched_points_3d = prev_points_3d[prev_indices]

        # Get 2D points in current frame
        curr_points_2d = np.array([curr_keypoints[i].pt for i in curr_indices], dtype=np.float32)

        try:
            # Solve PnP
            success, rvec, tvec = cv2.solvePnP(
                matched_points_3d.astype(np.float32),
                curr_points_2d,
                self.camera_matrix,
                np.zeros(4)  # Assuming no distortion
            )

            if success:
                # Create pose from rotation and translation vectors
                rotation_matrix = cv2.Rodrigues(rvec)[0]

                estimated_pose = Pose(
                    rotation=rotation_matrix,
                    translation=tvec.flatten()
                )

                # Update previous frame data
                self.prev_keypoints = curr_keypoints
                self.prev_descriptors = curr_descriptors
                self.prev_points_3d = prev_points_3d  # This would need to be updated with new estimates

                return estimated_pose, np.array(prev_indices)
            else:
                raise ValueError("PnP failed")

        except Exception as e:
            print(f"PnP estimation failed: {e}")
            self.prev_keypoints = curr_keypoints
            self.prev_descriptors = curr_descriptors
            self.prev_points_3d = prev_points_3d
            return Pose() if initial_pose is None else initial_pose, np.array([])
```

## Bundle Adjustment for Pose Refinement

Bundle adjustment jointly optimizes camera poses and 3D point positions:

```python
from scipy.optimize import least_squares
import numpy as np

class BundleAdjustment:
    """Bundle adjustment for pose and structure refinement"""

    def __init__(self, camera_matrix: np.ndarray):
        self.camera_matrix = camera_matrix
        self.fx = camera_matrix[0, 0]
        self.fy = camera_matrix[1, 1]
        self.cx = camera_matrix[0, 2]
        self.cy = camera_matrix[1, 2]

    def bundle_adjustment(self,
                         poses: list,
                         points_3d: np.ndarray,
                         observations: list,
                         camera_indices: np.ndarray,
                         point_indices: np.ndarray,
                         points_2d: np.ndarray) -> Tuple[list, np.ndarray]:
        """
        Perform bundle adjustment

        Args:
            poses: List of initial camera poses
            points_3d: Initial 3D points
            observations: List of observation indices
            camera_indices: Camera index for each observation
            point_indices: Point index for each observation
            points_2d: Observed 2D points

        Returns:
            (optimized_poses, optimized_points_3d)
        """
        # Pack poses and points into a single parameter vector
        n_poses = len(poses)
        n_points = len(points_3d)

        # Each pose is represented by 6 parameters (rotation vector + translation)
        # Each 3D point is represented by 3 parameters
        x0 = np.hstack([
            np.hstack([pose.log() for pose in poses]),
            points_3d.flatten()
        ])

        # Run optimization
        res = least_squares(
            self._bundle_adjustment_cost,
            x0,
            jac=self._bundle_adjustment_jacobian,
            method='trf',
            args=(n_poses, n_points, camera_indices, point_indices, points_2d),
            verbose=0
        )

        # Extract optimized parameters
        optimized_poses = []
        pose_params = res.x[:n_poses*6].reshape(-1, 6)
        for i in range(n_poses):
            pose = Pose.exp(pose_params[i])
            optimized_poses.append(pose)

        optimized_points = res.x[n_poses*6:].reshape(-1, 3)

        return optimized_poses, optimized_points

    def _bundle_adjustment_cost(self, params, n_poses, n_points, camera_indices, point_indices, points_2d):
        """
        Compute cost function for bundle adjustment
        """
        poses = []
        for i in range(n_poses):
            pose = Pose.exp(params[i*6:(i+1)*6])
            poses.append(pose)

        points_3d = params[n_poses*6:].reshape(-1, 3)

        # Project 3D points to 2D
        projected_points = []
        for i in range(len(camera_indices)):
            camera_idx = camera_indices[i]
            point_idx = point_indices[i]

            pose = poses[camera_idx]
            point_3d = points_3d[point_idx]

            # Transform point to camera coordinate system
            point_cam = pose.rotation @ point_3d + pose.translation

            # Project to image plane
            if point_cam[2] > 0:  # Point in front of camera
                u = self.fx * point_cam[0] / point_cam[2] + self.cx
                v = self.fy * point_cam[1] / point_cam[2] + self.cy
                projected_points.append([u, v])
            else:
                # Invalid projection
                projected_points.append([0, 0])

        projected_points = np.array(projected_points)

        # Compute reprojection error
        errors = (projected_points - points_2d).flatten()
        return errors

    def _bundle_adjustment_jacobian(self, params, n_poses, n_points, camera_indices, point_indices, points_2d):
        """
        Compute Jacobian matrix for bundle adjustment
        """
        n_observations = len(camera_indices)

        # This is a simplified Jacobian computation
        # In practice, you'd compute analytical derivatives
        J = np.zeros((2 * n_observations, 6 * n_poses + 3 * n_points))

        # For each observation, compute partial derivatives
        for i in range(n_observations):
            camera_idx = camera_indices[i]
            point_idx = point_indices[i]

            # This would involve computing derivatives of projection function
            # with respect to pose and point parameters
            # Simplified: finite differences would be used in real implementation

            # Jacobian w.r.t. camera pose (6 parameters)
            J[2*i:2*i+2, 6*camera_idx:6*(camera_idx+1)] = self._jacobian_wrt_pose(params, camera_idx, point_idx, n_poses)

            # Jacobian w.r.t. 3D point (3 parameters)
            J[2*i:2*i+2, 6*n_poses + 3*point_idx:6*n_poses + 3*(point_idx+1)] = self._jacobian_wrt_point(params, camera_idx, point_idx, n_poses)

        return J

    def _jacobian_wrt_pose(self, params, camera_idx, point_idx, n_poses):
        """Jacobian of projection w.r.t. camera pose parameters"""
        # Extract pose and point
        pose = Pose.exp(params[6*camera_idx:6*(camera_idx+1)])
        point_3d = params[6*n_poses + 3*point_idx:6*n_poses + 3*(point_idx+1)]

        # Transform point to camera coordinates
        point_cam = pose.rotation @ point_3d + pose.translation

        if point_cam[2] <= 0:
            return np.zeros(6)

        # Derivatives of projection w.r.t. point in camera frame
        z_inv = 1.0 / point_cam[2]
        z_inv2 = z_inv * z_inv

        # Jacobian of 2D projection w.r.t. 3D point in camera frame
        J_proj_point = np.array([
            [self.fx * z_inv, 0, -self.fx * point_cam[0] * z_inv2],
            [0, self.fy * z_inv, -self.fy * point_cam[1] * z_inv2]
        ])

        # Jacobian of camera frame point w.r.t. pose parameters (se(3))
        # This is more complex and would require detailed computation

        # Return simplified version
        return np.zeros((2, 6))

    def _jacobian_wrt_point(self, params, camera_idx, point_idx, n_poses):
        """Jacobian of projection w.r.t. 3D point parameters"""
        # Extract pose and point
        pose = Pose.exp(params[6*camera_idx:6*(camera_idx+1)])
        point_3d = params[6*n_poses + 3*point_idx:6*n_poses + 3*(point_idx+1)]

        # Transform point to camera coordinates
        point_cam = pose.rotation @ point_3d + pose.translation

        if point_cam[2] <= 0:
            return np.zeros((2, 3))

        # Derivatives of projection w.r.t. point in camera frame
        z_inv = 1.0 / point_cam[2]
        z_inv2 = z_inv * z_inv

        # Jacobian of 2D projection w.r.t. 3D point in camera frame
        J = np.array([
            [self.fx * z_inv, 0, -self.fx * point_cam[0] * z_inv2],
            [0, self.fy * z_inv, -self.fy * point_cam[1] * z_inv2]
        ])

        # Transform back to world coordinates using rotation
        J = J @ pose.rotation

        return J
```

## Visual-Inertial Odometry Integration

For humanoid robots, integrating inertial measurements can significantly improve pose estimation:

```python
class VisualInertialEstimator:
    """Visual-inertial pose estimation for humanoid robots"""

    def __init__(self,
                 camera_matrix: np.ndarray,
                 imu_rate: float = 200.0,  # IMU frequency
                 image_rate: float = 30.0):  # Camera frequency
        self.camera_matrix = camera_matrix
        self.imu_rate = imu_rate
        self.image_rate = image_rate

        # IMU state
        self.gravity = np.array([0, 0, -9.81])  # Gravity vector
        self.imu_bias = np.zeros(6)  # [gyro_bias, accel_bias]

        # State: [position, velocity, orientation, bias_gyro, bias_accel]
        self.state = np.zeros(15)
        self.covariance = np.eye(15) * 0.1

        # Feature tracker
        self.feature_tracker = FeatureBasedPoseEstimator(camera_matrix)

    def predict_state(self, dt: float, gyro: np.ndarray, accel: np.ndarray):
        """
        Predict state using IMU measurements

        Args:
            dt: Time step
            gyro: Gyroscope measurement
            accel: Accelerometer measurement
        """
        # Remove bias
        gyro_corrected = gyro - self.imu_bias[:3]
        accel_corrected = accel - self.imu_bias[3:]

        # Extract state variables
        pos = self.state[0:3]
        vel = self.state[3:6]
        q = self.state[6:10]  # Quaternion representation
        bias_gyro = self.state[10:13]
        bias_accel = self.state[13:15]

        # Convert quaternion to rotation matrix
        R = self._quat_to_rot_matrix(q)

        # State propagation
        pos_new = pos + vel * dt + 0.5 * (R @ accel_corrected + self.gravity) * dt**2
        vel_new = vel + (R @ accel_corrected + self.gravity) * dt

        # Update orientation using gyroscope
        omega = gyro_corrected
        q_dot = self._quat_derivative(q, omega)
        q_new = q + q_dot * dt
        q_new = q_new / np.linalg.norm(q_new)  # Normalize

        # Bias evolution (random walk)
        bias_gyro_new = bias_gyro
        bias_accel_new = bias_accel

        # Update state
        self.state = np.hstack([pos_new, vel_new, q_new, bias_gyro_new, bias_accel_new])

    def _quat_to_rot_matrix(self, q: np.ndarray) -> np.ndarray:
        """Convert quaternion to rotation matrix"""
        w, x, y, z = q
        return np.array([
            [1 - 2*(y*y + z*z), 2*(x*y - w*z), 2*(x*z + w*y)],
            [2*(x*y + w*z), 1 - 2*(x*x + z*z), 2*(y*z - w*x)],
            [2*(x*z - w*y), 2*(y*z + w*x), 1 - 2*(x*x + y*y)]
        ])

    def _quat_derivative(self, q: np.ndarray, omega: np.ndarray) -> np.ndarray:
        """Compute quaternion derivative from angular velocity"""
        wx, wy, wz = omega
        Omega = np.array([
            [0, -wx, -wy, -wz],
            [wx, 0, wz, -wy],
            [wy, -wz, 0, wx],
            [wz, wy, -wx, 0]
        ])
        return 0.5 * Omega @ q

    def update_with_visual_features(self, image: np.ndarray, timestamp: float):
        """
        Update pose estimate using visual features
        """
        # This would integrate visual features with the IMU prediction
        # For simplicity, this is a conceptual implementation
        pass
```

## Robust Estimation Techniques

### RANSAC for Outlier Rejection

```python
class RobustPoseEstimator:
    """Robust pose estimation using RANSAC and other techniques"""

    def __init__(self,
                 camera_matrix: np.ndarray,
                 inlier_threshold: float = 5.0):
        self.camera_matrix = camera_matrix
        self.inlier_threshold = inlier_threshold

    def estimate_pose_ransac(self,
                           points_3d: np.ndarray,
                           points_2d: np.ndarray,
                           max_iterations: int = 1000) -> Tuple[Pose, np.ndarray]:
        """
        Estimate pose using RANSAC to handle outliers

        Args:
            points_3d: 3D points (N, 3)
            points_2d: 2D points (N, 2)
            max_iterations: Maximum RANSAC iterations

        Returns:
            (best_pose, inlier_mask)
        """
        if len(points_3d) < 6:  # Minimum for stable PnP
            raise ValueError("Need at least 6 correspondences for PnP")

        best_pose = None
        best_inliers = np.array([])
        max_inliers = 0

        for iteration in range(max_iterations):
            # Randomly sample 6 points (minimum for PnP)
            sample_indices = np.random.choice(len(points_3d), size=6, replace=False)
            sample_3d = points_3d[sample_indices]
            sample_2d = points_2d[sample_indices]

            try:
                # Estimate pose from sample
                rvec, tvec = cv2.solvePnP(
                    sample_3d.astype(np.float32),
                    sample_2d.astype(np.float32),
                    self.camera_matrix,
                    np.zeros(4)
                )

                if not rvec.size or not tvec.size:
                    continue

                # Project all 3D points to 2D using estimated pose
                projected_2d, _ = cv2.projectPoints(
                    points_3d.astype(np.float32),
                    rvec, tvec,
                    self.camera_matrix,
                    np.zeros(4)
                )
                projected_2d = projected_2d.reshape(-1, 2)

                # Calculate reprojection errors
                errors = np.linalg.norm(points_2d - projected_2d, axis=1)

                # Count inliers
                inlier_mask = errors < self.inlier_threshold
                n_inliers = np.sum(inlier_mask)

                if n_inliers > max_inliers:
                    max_inliers = n_inliers
                    best_inliers = inlier_mask

                    # Refine pose using all inliers
                    inlier_3d = points_3d[inlier_mask]
                    inlier_2d = points_2d[inlier_mask]

                    if len(inlier_3d) >= 4:
                        rvec_refined, tvec_refined = cv2.solvePnP(
                            inlier_3d.astype(np.float32),
                            inlier_2d.astype(np.float32),
                            self.camera_matrix,
                            np.zeros(4),
                            flags=cv2.SOLVEPNP_ITERATIVE
                        )

                        if rvec_refined.size and tvec_refined.size:
                            R = cv2.Rodrigues(rvec_refined)[0]
                            best_pose = Pose(rotation=R, translation=tvec_refined.flatten())

            except Exception as e:
                continue  # Skip this iteration if PnP fails

        if best_pose is None:
            # Fallback: return identity pose
            return Pose(), np.array([])

        return best_pose, best_inliers

    def estimate_pose_gnc(self,
                         points_3d: np.ndarray,
                         points_2d: np.ndarray,
                         mu_init: float = 10.0,
                         mu_final: float = 0.1) -> Tuple[Pose, np.ndarray]:
        """
        Estimate pose using Graduated Non-Convexity (GNC)
        Robust to outliers without requiring initial guess
        """
        # This is a simplified implementation
        # Full GNC would involve more sophisticated non-convex optimization

        # Start with high regularization (convex approximation)
        mu = mu_init

        # Initialize with a robust estimate
        pose = Pose()

        # Iteratively refine with decreasing regularization
        while mu > mu_final:
            # Weight the correspondences based on current fit
            projected_2d, _ = cv2.projectPoints(
                points_3d.astype(np.float32),
                pose.rotation_vector.astype(np.float32),
                pose.translation.astype(np.float32),
                self.camera_matrix,
                np.zeros(4)
            )
            projected_2d = projected_2d.reshape(-1, 2)

            errors = np.linalg.norm(points_2d - projected_2d, axis=1)

            # Compute robust weights (Geman-McClure)
            weights = 1.0 / (1.0 + (errors / mu)**2)

            # Solve weighted PnP
            pose = self._weighted_solve_pnp(
                points_3d, points_2d, weights, pose
            )

            mu *= 0.5  # Reduce regularization

        # Final inlier determination
        projected_2d, _ = cv2.projectPoints(
            points_3d.astype(np.float32),
            pose.rotation_vector.astype(np.float32),
            pose.translation.astype(np.float32),
            self.camera_matrix,
            np.zeros(4)
        )
        projected_2d = projected_2d.reshape(-1, 2)

        errors = np.linalg.norm(points_2d - projected_2d, axis=1)
        inlier_mask = errors < self.inlier_threshold

        return pose, inlier_mask

    def _weighted_solve_pnp(self, points_3d, points_2d, weights, initial_pose):
        """
        Solve weighted PnP problem
        """
        # This would implement a weighted least squares solution
        # For now, return the initial pose
        return initial_pose
```

## Performance Optimization for Humanoid Robotics

### Real-time Considerations

```python
class RealTimePoseEstimator:
    """Real-time optimized pose estimator for humanoid robots"""

    def __init__(self,
                 camera_matrix: np.ndarray,
                 image_width: int,
                 image_height: int,
                 target_fps: float = 30.0):
        self.camera_matrix = camera_matrix
        self.image_width = image_width
        self.image_height = image_height
        self.target_fps = target_fps
        self.target_interval = 1.0 / target_fps

        # Feature management
        self.max_features = 500
        self.min_features = 50

        # Multi-threading components
        self.frame_queue = []
        self.pose_queue = []

        # Adaptive feature selection
        self.feature_selector = self._create_feature_selector()

        # Timing statistics
        self.processing_times = []

    def _create_feature_selector(self):
        """Create optimized feature selector"""
        # This would implement efficient feature selection
        # using spatial distribution and quality metrics
        pass

    def estimate_pose_realtime(self,
                             image: np.ndarray,
                             prev_pose: Pose = None) -> Tuple[Pose, dict]:
        """
        Real-time pose estimation with performance monitoring

        Returns:
            (estimated_pose, performance_stats)
        """
        import time
        start_time = time.time()

        # Convert to grayscale if needed
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # Detect features efficiently
        keypoints = self._detect_features_optimized(gray)

        if len(keypoints) < self.min_features:
            # Use motion model prediction if available
            if prev_pose is not None:
                # Simple constant velocity model
                estimated_pose = prev_pose
            else:
                estimated_pose = Pose()
        else:
            # Track features from previous frame if available
            if hasattr(self, 'prev_keypoints') and len(self.prev_keypoints) > 0:
                # Optical flow tracking
                estimated_pose = self._track_and_estimate_pose(
                    gray, self.prev_gray, keypoints, self.prev_keypoints
                )
            else:
                # First frame - cannot estimate motion
                estimated_pose = Pose()

        # Update previous frame data
        self.prev_keypoints = keypoints
        self.prev_gray = gray

        # Calculate performance stats
        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)

        # Maintain window of recent processing times
        if len(self.processing_times) > 100:
            self.processing_times = self.processing_times[-100:]

        avg_processing_time = np.mean(self.processing_times)
        current_fps = 1.0 / processing_time if processing_time > 0 else 0

        stats = {
            'processing_time': processing_time,
            'avg_processing_time': avg_processing_time,
            'current_fps': current_fps,
            'target_fps': self.target_fps,
            'feature_count': len(keypoints),
            'latency_budget_remaining': max(0, self.target_interval - processing_time)
        }

        return estimated_pose, stats

    def _detect_features_optimized(self, image: np.ndarray):
        """Optimized feature detection"""
        # Use ORB which is faster than SIFT/ SURF
        orb = cv2.ORB_create(
            nfeatures=self.max_features,
            scaleFactor=1.2,
            nlevels=4,  # Reduce levels for speed
            edgeThreshold=15,  # Reduce edge threshold
            patchSize=15,      # Smaller patches for speed
            fastThreshold=15   # Lower threshold for more features
        )

        keypoints = orb.detect(image, None)
        return keypoints

    def _track_and_estimate_pose(self, curr_gray, prev_gray, curr_kps, prev_kps):
        """Track features and estimate pose"""
        # Convert keypoints to points
        prev_pts = np.float32([kp.pt for kp in prev_kps]).reshape(-1, 1, 2)
        curr_pts = np.float32([kp.pt for kp in curr_kps]).reshape(-1, 1, 2)

        if len(prev_pts) < 8:
            return Pose()

        # Calculate optical flow
        curr_pts, status, error = cv2.calcOpticalFlowPyrLK(
            prev_gray, curr_gray, prev_pts, None,
            winSize=(15, 15),
            maxLevel=2,
            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
        )

        # Filter valid points
        valid = status.flatten() == 1
        prev_pts_valid = prev_pts[valid]
        curr_pts_valid = curr_pts[valid]

        if len(prev_pts_valid) < 8:
            return Pose()

        # Estimate essential matrix and pose
        E, mask = cv2.findEssentialMat(
            curr_pts_valid, prev_pts_valid,
            self.camera_matrix,
            method=cv2.RANSAC,
            threshold=1.0,
            prob=0.999
        )

        if E is None or len(E) == 0:
            return Pose()

        # Decompose essential matrix
        _, R, t, _ = cv2.recoverPose(E, curr_pts_valid, prev_pts_valid, self.camera_matrix)

        return Pose(rotation=R, translation=t.flatten())
```

## Evaluation and Validation

### Pose Estimation Quality Metrics

```python
class PoseEstimatorEvaluator:
    """Evaluate pose estimation quality"""

    def __init__(self):
        pass

    def evaluate_pose_accuracy(self,
                             estimated_poses: list,
                             ground_truth_poses: list) -> dict:
        """
        Evaluate pose estimation accuracy

        Args:
            estimated_poses: List of estimated Pose objects
            ground_truth_poses: List of ground truth Pose objects

        Returns:
            Dictionary with accuracy metrics
        """
        if len(estimated_poses) != len(ground_truth_poses):
            raise ValueError("Pose sequences must have same length")

        translation_errors = []
        rotation_errors = []

        for est_pose, gt_pose in zip(estimated_poses, ground_truth_poses):
            # Translation error
            trans_error = np.linalg.norm(est_pose.translation - gt_pose.translation)
            translation_errors.append(trans_error)

            # Rotation error (in degrees)
            R_error = est_pose.rotation @ gt_pose.rotation.T
            rotation_error = np.arccos(np.clip((np.trace(R_error) - 1) / 2, -1, 1))
            rotation_error_deg = np.degrees(rotation_error)
            rotation_errors.append(rotation_error_deg)

        # Calculate statistics
        metrics = {
            'translation': {
                'mean_error': np.mean(translation_errors),
                'median_error': np.median(translation_errors),
                'std_error': np.std(translation_errors),
                'rmse': np.sqrt(np.mean(np.array(translation_errors)**2)),
                'max_error': np.max(translation_errors),
                'min_error': np.min(translation_errors),
                'percentiles': {
                    '25%': np.percentile(translation_errors, 25),
                    '75%': np.percentile(translation_errors, 75),
                    '95%': np.percentile(translation_errors, 95)
                }
            },
            'rotation': {
                'mean_error_deg': np.mean(rotation_errors),
                'median_error_deg': np.median(rotation_errors),
                'std_error_deg': np.std(rotation_errors),
                'rmse_deg': np.sqrt(np.mean(np.array(rotation_errors)**2)),
                'max_error_deg': np.max(rotation_errors),
                'min_error_deg': np.min(rotation_errors),
                'percentiles': {
                    '25%': np.percentile(rotation_errors, 25),
                    '75%': np.percentile(rotation_errors, 75),
                    '95%': np.percentile(rotation_errors, 95)
                }
            }
        }

        return metrics

    def evaluate_trajectory_consistency(self, poses: list) -> dict:
        """
        Evaluate trajectory consistency (smoothness, continuity)
        """
        if len(poses) < 2:
            return {'mean_velocity': 0, 'mean_acceleration': 0}

        velocities = []
        accelerations = []

        for i in range(1, len(poses)):
            # Calculate velocity between consecutive poses
            pos1 = poses[i-1].translation
            pos2 = poses[i].translation
            vel = np.linalg.norm(pos2 - pos1)  # Assuming unit time step

            velocities.append(vel)

            if i > 1:
                # Calculate acceleration
                pos0 = poses[i-2].translation
                vel_prev = np.linalg.norm(pos1 - pos0)
                acc = abs(vel - vel_prev)  # Assuming unit time step
                accelerations.append(acc)

        return {
            'mean_velocity': np.mean(velocities) if velocities else 0,
            'std_velocity': np.std(velocities) if velocities else 0,
            'mean_acceleration': np.mean(accelerations) if accelerations else 0,
            'std_acceleration': np.std(accelerations) if accelerations else 0
        }

    def plot_pose_trajectory(self, poses: list, title: str = "Pose Trajectory"):
        """
        Plot 3D trajectory of poses
        """
        import matplotlib.pyplot as plt
        from mpl_toolkits.mplot3d import Axes3D

        positions = np.array([pose.translation for pose in poses])

        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')

        ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 'b-', linewidth=2, label='Trajectory')
        ax.scatter(positions[0, 0], positions[0, 1], positions[0, 2], color='green', s=100, label='Start')
        ax.scatter(positions[-1, 0], positions[-1, 1], positions[-1, 2], color='red', s=100, label='End')

        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        ax.set_title(title)
        ax.legend()

        plt.show()

        return fig
```

## Best Practices for Humanoid Robotics

### Adaptive Pose Estimation

```python
class AdaptivePoseEstimator:
    """Adaptive pose estimation that adjusts to changing conditions"""

    def __init__(self,
                 camera_matrix: np.ndarray,
                 initial_method: str = 'feature_based'):
        self.camera_matrix = camera_matrix
        self.current_method = initial_method
        self.feature_estimator = FeatureBasedPoseEstimator(camera_matrix)
        self.direct_estimator = DirectPoseEstimator(
            camera_matrix, 640, 480  # Assuming 640x480 image
        )

        # Performance monitors
        self.feature_tracking_quality = 1.0
        self.direct_method_quality = 1.0
        self.switching_threshold = 0.5

    def estimate_pose_adaptive(self,
                             current_frame: np.ndarray,
                             prev_frame: np.ndarray,
                             prev_points_3d: np.ndarray = None) -> Tuple[Pose, str]:
        """
        Estimate pose using the most appropriate method based on current conditions

        Returns:
            (estimated_pose, method_used)
        """
        # Evaluate scene conditions
        scene_features = self._analyze_scene_features(current_frame)
        motion_estimate = self._estimate_motion(prev_frame, current_frame)

        # Decide which method to use based on conditions
        if scene_features['texture_level'] < 0.3:
            # Low texture scene - direct methods may fail
            self.current_method = 'feature_based'
        elif motion_estimate['magnitude'] > 0.1:  # Large motion
            # Large motion - feature tracking may fail, use direct method
            self.current_method = 'direct'
        else:
            # Evaluate both methods and use the better one
            if self.feature_tracking_quality > self.direct_method_quality:
                self.current_method = 'feature_based'
            else:
                self.current_method = 'direct'

        # Apply chosen method
        if self.current_method == 'feature_based':
            if prev_points_3d is not None:
                pose, inliers = self.feature_estimator.estimate_pose_from_features(
                    current_frame, prev_frame, prev_points_3d
                )
                # Update quality metric
                if len(prev_points_3d) > 0:
                    self.feature_tracking_quality = len(inliers) / len(prev_points_3d)
                else:
                    self.feature_tracking_quality = 0
            else:
                pose = Pose()
        else:  # direct method
            # For direct method, we need depth information
            # This is a simplified approach
            try:
                # Create a dummy depth map for demonstration
                height, width = current_frame.shape[:2] if len(current_frame.shape) == 2 else current_frame.shape[:2]
                dummy_depth = np.ones((height, width)) * 1.0  # 1 meter depth

                pose = self.direct_estimator.estimate_pose_direct(
                    current_frame, prev_frame, dummy_depth
                )
                # Update quality metric based on convergence
                self.direct_method_quality = 0.8  # Simplified
            except:
                pose = Pose()
                self.direct_method_quality = 0.1

        return pose, self.current_method

    def _analyze_scene_features(self, frame) -> dict:
        """Analyze scene characteristics to choose appropriate method"""
        if len(frame.shape) == 3:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray = frame

        # Calculate texture level using gradient magnitude
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)

        texture_level = np.mean(gradient_magnitude) / 255.0  # Normalize

        # Calculate image variance (another texture measure)
        variance = np.var(gray) / (255.0**2)

        return {
            'texture_level': texture_level,
            'variance': variance,
            'is_low_texture': texture_level < 0.1
        }

    def _estimate_motion(self, prev_frame, curr_frame) -> dict:
        """Estimate motion magnitude between frames"""
        if len(prev_frame.shape) == 3:
            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
        else:
            prev_gray = prev_frame
            curr_gray = curr_frame

        # Use optical flow to estimate motion
        prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=100,
                                          qualityLevel=0.01, minDistance=10)

        if prev_pts is not None:
            curr_pts, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray,
                                                          prev_pts, None)

            valid = status.flatten() == 1
            if np.sum(valid) > 10:  # Enough valid points
                prev_valid = prev_pts[valid].reshape(-1, 2)
                curr_valid = curr_pts[valid].reshape(-1, 2)

                motion_vectors = curr_valid - prev_valid
                motion_magnitude = np.mean(np.linalg.norm(motion_vectors, axis=1))

                return {
                    'magnitude': motion_magnitude,
                    'is_large_motion': motion_magnitude > 5.0  # pixels
                }

        return {'magnitude': 0, 'is_large_motion': False}
```

## Summary

Pose estimation is a fundamental component of VSLAM systems for humanoid robotics. The choice of pose estimation method depends on several factors:

1. **Scene Characteristics**:
   - High-texture scenes: Feature-based methods work well
   - Low-texture scenes: Direct methods may be more appropriate
   - Dynamic scenes: Need robust tracking and outlier rejection

2. **Motion Characteristics**:
   - Small motions: Feature tracking is effective
   - Large motions: Direct methods or wide-baseline matching required
   - Fast motions: Need high frame rates and motion models

3. **Computational Requirements**:
   - Real-time applications: Optimized implementations required
   - Embedded systems: Balance accuracy with efficiency
   - Multi-sensor fusion: Consider IMU integration

4. **Robustness Requirements**:
   - Outlier rejection: RANSAC, GNC, or other robust methods
   - Degenerate configurations: Multiple hypothesis tracking
   - Initialization: Good starting pose estimates

For humanoid robotics applications, consider implementing adaptive systems that can switch between methods based on scene and motion characteristics, and integrate with other sensors like IMUs for improved robustness and accuracy.
